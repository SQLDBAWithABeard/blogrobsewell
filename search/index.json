[{"content":"The Great Job Title Debate I posted a few things on LinkedIn recently about job titles. The discussion was lively ‚Äî as it always is ‚Äî and the general vibe was something along the lines of \u0026ldquo;titles don\u0026rsquo;t define what you do, they\u0026rsquo;re just the label on the drawer.\u0026rdquo; A drawer full of skills, experiences, weird technical rabbit holes at midnight, blog posts, conference talks, and approximately one thousand cups of tea remains the same drawer regardless of what\u0026rsquo;s written on the front.\nThe drawer with cutlery in it is still the same drawer whether it\u0026rsquo;s labelled \u0026ldquo;knives and forks\u0026rdquo; or \u0026ldquo;kitchen tools.\u0026rdquo;\nWell. My drawer has a shiny new label.\nI\u0026rsquo;m the Chief Data Officer at Zure UK I am genuinely, ridiculously excited to announce that I\u0026rsquo;ve joined Zure UK as Chief Data Officer. I announced it on LinkedIn in my own special way with a beard unveiling :-)\nGo on, I\u0026rsquo;ll say it again. Chief Data Officer. I keep saying it to Traci S L and she keeps smiling politely while I grin like an idiot. She\u0026rsquo;s very patient.\nSo, What Is Zure? If you haven\u0026rsquo;t come across Zure before, let me introduce you. They are a Microsoft Azure-focused digital services company ‚Äî exclusively Azure, no hedging, no \u0026ldquo;well we also do a bit of AWS\u0026rdquo; ‚Äî with stupendously clever folk based in Finland, Belgium, Denmark, the Netherlands, and now growing here in the UK. They have several Microsoft RDs and MVPs on staff (I like these people already), an incredible community culture, and a philosophy I find genuinely refreshing: \u0026ldquo;Roar at Challenge.\u0026rdquo;\nThe about page talks about facing challenges without fear, learning from failures openly, and shared responsibility with individual impact. If you\u0026rsquo;ve read this blog for any length of time, you\u0026rsquo;ll know that resonates deeply with me. I\u0026rsquo;ve been banging on about sharing failures as a community for years. These are my kind of people.\nAnd crucially, data and analytics is a core service area for them ‚Äî AI, development, data, integration, cybersecurity, DevOps ‚Äî all built on the Azure platform. So there\u0026rsquo;s a lot of opportunity to do genuinely exciting things and all of it is right in my wheelhouse.\nWhat Am I Actually Going to Be Doing? Two big things, and I\u0026rsquo;m excited about both of them in very different ways.\nFirst: building a data practice for Zure UK.\nThis is the thing that makes me want to bounce around the room. I get to help shape what the data team looks like ‚Äî the people, the practices, the culture, the way we approach client problems. I\u0026rsquo;ve spent years talking about data platforms, Microsoft Fabric, automation, testing, and all the rest of it. Now I get to build something from the ground up that embodies those values. No legacy \u0026ldquo;but we\u0026rsquo;ve always done it this way\u0026rdquo; baggage. Just a blank page and an opportunity to do it properly alongside an excellent team based around Europe.\nIf that sounds like something you\u0026rsquo;d want to be part of ‚Äî get in touch. You can book a no-obligation chat with me through this link and we can talk about what we\u0026rsquo;re building, where we\u0026rsquo;re going, and whether you might want to be part of it.\nOr you can just find me on LinkedIn or Bluesky and send me a message there.\nHere is a link to the Zure careers page where you can see the roles we\u0026rsquo;re hiring for.\nSecond: talking to new clients.\nI love this part too, even though my imposter syndrome is already clearing its throat in the corner. I enjoy the conversations. I enjoy understanding what organisations are trying to achieve with their data, where things have gone sideways for them, and thinking through how we could help. It\u0026rsquo;s the kind of work that keeps you sharp because every client situation is different.\nIf you\u0026rsquo;re looking at your data platform and wondering how to make it actually work for your organisation, how to integrate AI and analytics to your processes ‚Äî or if you\u0026rsquo;re thinking about Microsoft Fabric and want to understand what it can mean for you and your company ‚Äî I\u0026rsquo;d genuinely love to have that conversation. You can find me on LinkedIn or Bluesky, or just turn up at a conference and tap me on the shoulder.\nSame Drawer, New Label So yes. The label has changed. But the drawer is the same one ‚Äî full of the same passion for data, the same community values, the same commitment to sharing what I learn (including the bits that go wrong, which as regular readers know, there are plenty of), and the same terrible jokes.\nI\u0026rsquo;m proud. I\u0026rsquo;m excited. And I\u0026rsquo;m very much looking forward to what comes next.\nHere\u0026rsquo;s to new chapters. ü•Ç\nIf you want to know more about Zure, have a look at zure.com ‚Äî and particularly the community and careers pages if you want to see the kind of company they are. I think you\u0026rsquo;ll like what you find.\n","date":"2026-02-27T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/rob-zure.png","permalink":"https://blog.robsewell.com/blog/the-label-on-my-drawer-has-changed/","title":"The Label on My Drawer Has Changed"},{"content":"Introduction The workspace is the fundamental unit of organisation in Microsoft Fabric. Everything lives inside a workspace ‚Äî your lakehouses, warehouses, notebooks, pipelines, reports. Managing workspaces is therefore the first practical skill to build, and MicrosoftFabricMgmt makes it straightforward.\nWe have already seen Get-FabricWorkspace in the installation post.\nLets explore it in more detail, along with the other workspace management cmdlets: New-FabricWorkspace, Update-FabricWorkspace, and Remove-FabricWorkspace. By the end of this post you will be able to list, get details of, create, update, and remove workspaces in your Fabric tenant using PowerShell.\nGetting Workspaces Get-FabricWorkspace is flexible. With no parameters it returns every workspace you have access to:\n1 Get-FabricWorkspace You can filter by name:\n1 Get-FabricWorkspace -WorkspaceName Fixy Or by ID:\n1 Get-FabricWorkspace -WorkspaceId \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; Thanks to the intelligent output formatting we looked at yesterday, the results show the Capacity Name, Workspace Name, and other properties in a readable table ‚Äî no GUID decoding required.\nCreating a Workspace New-FabricWorkspace creates a workspace and returns the new workspace object so you can immediately use its ID in subsequent commands:\n1 2 New-FabricWorkspace -WorkspaceName \u0026#34;BlogPost\u0026#34; -WorkspaceDescription \u0026#34;A workspace for testing blog post examples\u0026#34; it has been created without being assigned to a capacity.\nAssigning to a Capacity A newly created workspace has no capacity assigned if you do not specify the Capacity ID.\nYou can assign a capacity with Assign-FabricWorkspaceCapacity:\n1 2 3 4 5 # Get the capacity you want $capacity = Get-FabricCapacity -CapacityName \u0026#34;My Fabric Capacity\u0026#34; # Assign the workspace to it Assign-FabricWorkspaceCapacity -WorkspaceId $workspace.id -CapacityId $capacity.id You can also specify the capacity when creating the workspace, and this being PowerShell, you can create many workspaces in a single command\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 $workspaces = @( @{ Name = \u0026#34;Thinky\u0026#34; Description = \u0026#39;where the ‚Äúserious‚Äù thinking allegedly happens\u0026#39; }, @{ Name = \u0026#34;Doey\u0026#34; Description = \u0026#39;for when you might actually get something done\u0026#39; }, @{ Name = \u0026#34;Planney\u0026#34; Description = \u0026#39;full of plans that may or may not survive contact with reality\u0026#39; }, @{ Name = \u0026#34;Breaky\u0026#34; Description = \u0026#39;because sometimes the workspace breaks before you do\u0026#39; }, @{ Name = \u0026#34;Fixy\u0026#34; Description = \u0026#39;the spiritual successor to Breaky\u0026#39; }, @{ Name = \u0026#34;Messy\u0026#34; Description = \u0026#39;the one you swear you will tidy ‚Äúlater‚Äù\u0026#39; }, @{ Name = \u0026#34;Tryey\u0026#34; Description = \u0026#39;where experiments go to be‚Ä¶ attempted\u0026#39; }, @{ Name = \u0026#34;Hacky\u0026#34; Description = \u0026#39;perfect for demos that are 60% genius, 40% duct tape\u0026#39; }, @{ Name = \u0026#34;Showy\u0026#34; Description = \u0026#39;the polished one you pretend you always work in\u0026#39; }, @{ Name = \u0026#34;Sneaky\u0026#34; Description = \u0026#39;for the things you are not ready to talk about yet\u0026#39; } ) $workspaces | ForEach-Object { $workspaceName = $_.Name $workspaceDescription = $_.Description New-FabricWorkspace -WorkspaceName $workspaceName -WorkspaceDescription $workspaceDescription -CapacityId $capacityid } This is how I creaed the workspaces in the screenshots for this post. I find it easier to manage them in code, and it makes it easy to recreate them if I accidentally delete one while testing Remove-FabricWorkspace\nUpdating a Workspace Use Update-FabricWorkspace to rename a workspace or update its description. You can pipe a workspace object from Get-FabricWorkspace into it, or specify the workspace with -WorkspaceId\n1 Get-FabricWorkspace -WorkspaceName Breaky | Update-FabricWorkspace -WorkspaceName SnackTime -Description \u0026#39;Because sometimes snacks take priority\u0026#39; Removing a Workspace Remove-FabricWorkspace removes a workspace. It will ask for confirmation by default:\n1 Remove-FabricWorkspace -WorkspaceId $workspace.id Use -Confirm:$false to skip the prompt in automation scripts:\n1 Remove-FabricWorkspace -WorkspaceId $workspace.id -Confirm:$false Or pipe from Get-FabricWorkspace:\n1 $workspaces | ForEach-Object {Get-FabricWorkspace -WorkspaceName $_.Name | Remove-FabricWorkspace -Confirm:$false} A Practical Example: Dev, Test, Prod Here is a pattern I use for creating a standard set of workspaces for a project:\n1 2 3 4 5 6 $capacityId = (Get-FabricCapacity -CapacityName \u0026#34;My Fabric Capacity\u0026#34;).id $projectName = \u0026#34;DataPlatform\u0026#34; foreach ($env in @(\u0026#34;dev\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;prod\u0026#34;)) { New-FabricWorkspace -WorkspaceName \u0026#34;$projectName-$env\u0026#34; -WorkspaceDescription \u0026#34;$projectName $env environment\u0026#34; -CapacityId $capacityId } Consistent, repeatable, and documented in code. That is the PowerShell way.\nTomorrow Next in this series we look at one of my favourite aspects of MicrosoftFabricMgmt ‚Äî the PowerShell pipeline. How cmdlets are designed to flow together, and how you can write powerful, idiomatic automation with very little code.\nHowever, tomorrow will be a different blog post which I am super excited to share.\nYou can find all of the blog posts about MicrosoftFabricMgmt here - MicrosoftFabricMgmt Blog Posts\n","date":"2026-02-26T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/workspaces.png","permalink":"https://blog.robsewell.com/blog/microsoftfabricmgmt-workspaces-list-get-create-update-and-remove/","title":"MicrosoftFabricMgmt: Workspaces - List, Get, Create, Update, and Remove"},{"content":"Introduction When you work with a REST API that returns GUIDs for everything, human readability goes out the window. You run a query like this in PowerShell to get your Fabric lakehouses and you get something like this returned\nHumans don\u0026rsquo;t work with GUIDs. We want names. Which workspace is 948d3445-54a5-4c2a-85e7-2c3d30933992? Which capacity? Who knows ‚Äî go look it up. Multiply that by fifty items across ten workspaces and you have a frustrating afternoon ahead of you.\nThe PowerShell Module**MicrosoftFabricMgmt** solves some of this frustration.\nToday I want to show you one of the features Jess Pomfret B S L and I are most proud of: intelligent output formatting with smart caching.\nWhat the Module Does When you run a Get-* cmdlet, the module automatically resolves GUIDs to human-readable names and presents the results in a consistent, readable format:\n1 2 3 4 5 Capacity Name Workspace Name Item Name Type ID ------------- -------------- --------- ---- -- Premium Capacity P1 Sales Analytics Sales Lakehouse Lakehouse a1b2c3d4-e5f6... Premium Capacity P1 Sales Analytics ETL Notebook Notebook b2c3d4e5-f6a7... Premium Capacity P2 Marketing Workspace Campaign Data Warehouse c3d4e5f6-a7b8... Capacity Name ‚Üí Workspace Name ‚Üí Item Name ‚Üí Type ‚Üí ID.\nEvery time. Consistent across the module.\nHow It Works Behind the scenes, each formatted cmdlet uses PSTypeName decoration and a custom format file (MicrosoftFabricMgmt.Format.ps1xml) to control what PowerShell displays. The name resolution happens via three helper functions. This will be a post for another day, a little bit more PowerShell nerdery. Today lets focus on the user experience and the caching magic that makes it possible.\nThe Caching Magic Here is where it gets really good. Jess and I are used to working with significant numbers of items and we also remember some of the first feedback from writing dbachecks -\n\u0026ldquo;This is amazing but it takes so long on 10,000 instances\u0026rdquo;\n\u0026ldquo;I love this but on an instance with 2,000 databases it is really slow\u0026rdquo;.\nWe got that feedback in the first weeks of release when we were still glowing with the satisfaction of building something cool!\nWe wanted to make sure that the MicrosoftFabricMgmt module was useful for humans as well as machines so we decided that we would take the same approach as dbatools which provides ComputerName, ServerName and InstanceName properties in the output of all of its functions.\nSMO, the .NET library that communicates with the SQL instances makes it easier to associate resources with their parent resource.\nThe Fabric API is not as helpful. So to make the output human-friendly, we have to make additional API calls to resolve GUIDs to names. This is where caching comes in. We don\u0026rsquo;t want to make an API call every time we need to resolve a name ‚Äî that would be too slow.\nSo we came up with this clever caching mechanism that stores resolved names and reuses them across the session. When you run a function that needs to resolve a GUID, it first checks the cache. If the name is already there, it returns it immediately. If not, it makes the API call, stores the result in the cache, and then returns it.\nThe module uses PSFramework\u0026rsquo;s configuration system as a cache:\nFirst lookup for a single workspace: ~5s (makes 1 or two additional API calls to resolve the GUIDs for friendly output) Cached lookup: ~1s (returns results immediately from the cache without additional API calls) The same thing happens for other resources. Lets take a look at the SQL Endpoint for all the lakehouses in my Strava workspace.\nThis time it takes 10 seconds to get the workspace, resolve the capacity name, get the lakehouses in the workspace, resolve the workspace names and then get all of the SQL Endpoints. The second time it only takes 3 seconds because all of the names are cached.\nSeeing the Cache You can see what is cached with:\n1 Get-PSFConfig -Module MicrosoftFabricMgmt -Name Cache.* Clearing the Cache If you need to remove the cached names because they might be stale, clear it with:\n1 Clear-FabricNameCache -Force 1 Successfully cleared 42 cached name resolution(s) After clearing, the next resolution will make a fresh API call and repopulate the cache.\nSeeing the Raw Data Sometimes you want the raw output without the formatting ‚Äî for instance, when you need the original property names. Jess and I thought of that too and there is a -Raw switch on all of the Get-* cmdlets that returns the unformatted data with the original property names, including the GUIDs and the friendly names.\nIMPORTANT - The properties are always there, we are only changing the default view to be the most useful for humans. If you want to see the GUIDs, they are still in the output, just not in the default view.\n1 2 3 4 5 # Get raw properties Get-FabricWorkspace | Select-Object id, displayName, capacityId # See all properties on an object Get-FabricWorkspace | Get-FabricLakehouse | Get-FabricSQLEndpoint -Raw| Format-List * Why This Matters This feature is the difference between results you can act on immediately and results you have to decode. When you are reviewing thirty lakehouses spread across five workspaces and three capacities, seeing Premium Capacity P1 ‚Üí Sales Analytics ‚Üí Sales Lakehouse is so much more useful than three GUIDs.\nNext we start working with workspaces in depth ‚Äî creating, updating, managing them. See you then.\nYou can find all of the blog posts about MicrosoftFabricMgmt here - MicrosoftFabricMgmt Blog Posts\n","date":"2026-02-25T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/workspaces.png","permalink":"https://blog.robsewell.com/blog/microsoftfabricmgmt-goodbye-guids-intelligent-output-and-smart-caching/","title":"MicrosoftFabricMgmt: Goodbye GUIDs - Intelligent Output and Smart Caching"},{"content":"Introduction Yesterday I introduced the MicrosoftFabricMgmt module and explained what it can do. Today we are getting hands on ‚Äî installing the module, sorting out dependencies, and making your first connection to Microsoft Fabric.\nBy the end of this post you will have the module installed, be authenticated, and have your first list of Fabric workspaces in your terminal.\nPrerequisites MicrosoftFabricMgmt requires PowerShell 7 or later. It will not work on Windows PowerShell 5.1. If you are still on 5.1, now is a great time to upgrade. You can download PowerShell 7 from here.\nYou can check your version with:\n1 $PSVersionTable.PSVersion Installing the Module Installing from the PowerShell Gallery is the recommended approach:\n1 Install-PsResource -Name MicrosoftFabricMgmt The module has dependencies that will be installed automatically:\nModule Minimum Version Purpose PSFramework 5.0.0 Logging and configuration Az.Accounts 5.0.0 Azure authentication Az.Resources 6.15.1 Azure resource management MicrosoftPowerBIMgmt 1.2.1111 Power BI integration If you have already installed the module and want to update to the latest version:\n1 Update-PsResource -Name MicrosoftFabricMgmt Once installed, verify it is available:\n1 Get-PsResource -Name MicrosoftFabricMgmt Authentication Before any cmdlet will work you need to authenticate. This is done with Set-FabricApiHeaders.\nFor interactive use ‚Äî which is what we will cover today ‚Äî this triggers a browser authentication flow using your Microsoft account.\n1 Set-FabricApiHeaders -TenantId \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; You will need your Tenant ID. You can find this in the Azure portal under Azure Active Directory, or by running:\n1 2 # If you have Az.Accounts installed and have Connected-AzAccount, you can get the tenant ID with: (Get-AzContext).Tenant.Id I like to store mine as a secret so I do not have to hardcode it in scripts. I use the Microsoft.PowerShell.SecretManagement module for this:\n1 Set-FabricApiHeaders -TenantId (Get-Secret -Name FabricTenantId -AsPlainText) After running Set-FabricApiHeaders, your browser or a window will open and prompt you to sign in.\nOnce authenticated, the module stores your token securely for use in future commands in this session. It will also show how long the token is valid for, so you know when you will need to re-authenticate.\nWhen the token expires, you will need to re-authenticate, any MicrosoftFabricMgmt commands will inform you with a helpful message.\nBreaking Changes You will also notice when importing that you get some pretty blue text informing about breaking changes. This is because this version of the module contains some breaking changes from the previous version\nWhat About Service Principals and Managed Identity? The module also supports Service Principals (ideal for CI/CD pipelines and scheduled tasks) and Managed Identity (ideal for workloads running on Azure VMs, Azure Functions, or Azure DevOps agents). We will cover those in a dedicated post later in the series. For now, interactive authentication is the way to go.\nYour First Commands With authentication sorted, let us make sure everything is working:\n1 2 # List all workspaces you have access to Get-FabricWorkspace You should see your workspaces listed with their display names. Notice how the output shows the Capacity Name alongside the workspace details ‚Äî not raw GUIDs. That is the intelligent output formatting built into the module, and it is the subject of tomorrow\u0026rsquo;s post.\n1 2 3 4 5 6 # Get details of a specific workspace by name Get-FabricWorkspace -WorkspaceName Strava # Get all the details in a list showing all properties Get-FabricWorkspace -WorkspaceName workyII | Format-List # You only need single quotes for names with spaces. Get-FabricWorkspace -WorkspaceName \u0026#39;Humans read words not GUIDS\u0026#39; Confirming Module Configuration PSFramework powers the module\u0026rsquo;s configuration system. You can inspect all the configuration settings with:\n1 Get-PSFConfig -Module MicrosoftFabricMgmt This shows things like the API base URL, timeout settings, retry attempts, and backoff multiplier. You can adjust these if needed ‚Äî for example, if you want more retry attempts for a flaky network:\n1 Set-PSFConfig -Module MicrosoftFabricMgmt -Name Api.RetryMaxAttempts -Value 5 You Are Ready That is it! The module is installed, you are authenticated, and you have your workspaces. Tomorrow we look at something that makes this module stand out ‚Äî the intelligent output formatting that turns GUIDs into names and makes your results actually readable. See you then.\nYou can find all of the blog posts about MicrosoftFabricMgmt here - MicrosoftFabricMgmt Blog Posts\n","date":"2026-02-24T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/intro.png","permalink":"https://blog.robsewell.com/blog/microsoftfabricmgmt-getting-started-installation-and-authentication/","title":"MicrosoftFabricMgmt: Getting Started - Installation and Authentication"},{"content":"Introduction If you have been following this blog for a while, you will know that I am a huge fan of using PowerShell to manage and automate things. SQL Server, dbatools, dbachecks ‚Äî automating the boring stuff so we can spend time on the interesting stuff.\nI have been introducing the Microsoft fabric-toolbox ‚Äî covering the toolbox itself, FUAM, and FCA. All excellent tools. But there is one item in the toolbox that I have been personally involved in building, and it is the one I am most excited to write about.\nToday I am kicking off a series of posts about MicrosoftFabricMgmt ‚Äî an enterprise-grade PowerShell module that gives you comprehensive, scriptable control over the entire Microsoft Fabric REST API. It is hosted as part of the fabric-toolbox on GitHub.\nWho Built It? This module is a community and Microsoft collaboration. It was started by the talented Tiago Balabuch L and in the last few weeks Jess Pomfret B S L and myself have made major improvements to the module, and we are really proud of it and excited about how it has turned out.\nWhat Does It Do? In short: a lot.\nIt wraps almost the entire Microsoft Fabric REST API in a PowerShell module, with a consistent, intuitive interface that follows PowerShell best practices. It is designed to be used by everyone ‚Äî from the person who just wants to automate a few tasks in their Fabric tenant, to the person who is building an enterprise-grade automation framework for managing hundreds of tenants and thousands of resources. Jess and I have added verification against the official Microsoft Fabric REST API specifications\n295+ cmdlets covering 48 different Microsoft Fabric resource types Lakehouses, Warehouses, Notebooks, Pipelines, Eventstreams, KQL Databases, ML Models, and much more Intelligent output formatting - My second favourite improvement ‚Äî no more squinting at GUIDs. We use our data knowledge and enrich the data before returning it. The module automatically resolves Capacity IDs and Workspace IDs to human-readable names, with smart caching so it stays fast The raw data is still there if you want it.\nFull PowerShell pipeline support - my favourite improvement ‚Äî You can now pipe workspaces to get their lakehouses, pipe to get their SQL Endpoints, pipe to ge the SQL Endpoints connection string all in one line of code. The PowerShell way :-) 1 Get-FabricWorkspace -WorkspaceName Strava | Get-FabricLakehouse | Get-FabricSQLEndpoint | Get-FabricSQLEndpointConnectionString Enterprise-grade resilience ‚Äî built-in retry logic with exponential backoff, automatic rate limit handling, and Long Running Operation support Enterprise-grade logging and error handling ‚Äî consistent, configurable caching, logging, and configuration management throughout the module What is New? The current release represents a major step forward from earlier versions.\nYou can see the full list of changes in the [changelog](https://github.com/microsoft/fabric-toolbox/blob/main/tools/MicrosoftFabricMgmt/output/CHANGELOG.md#104---2026-02-16]\nThe Series Over the next few weeks I am going to take you from installing the module for the first time all the way through to managing your entire Fabric tenant, handling errors gracefully, and even contributing back to the project. We will start with installation and authentication, move through the intelligent output system, explore workspaces and the PowerShell pipeline, dig into PSFramework logging and error handling, cover the major resource types, take a tour of Real-Time Intelligence, spend time with the powerful Admin API, and finish with a complete end-to-end deployment script and a guide to contributing.\nGetting the Module You can install MicrosoftFabricMgmt right now from the PowerShell Gallery:\n1 Install-PsResource -Name MicrosoftFabricMgmt Requires PowerShell 7 or later. The source code lives at github.com/microsoft/fabric-toolbox, and that is also where you can raise issues and submit pull requests.\nSee You Tomorrow The first thing we need to do is get the module installed and prove it works. That is tomorrow\u0026rsquo;s post ‚Äî installation, dependencies, and your first connection to Microsoft Fabric. See you then.\nYou can find all of the blog posts about MicrosoftFabricMgmt here - MicrosoftFabricMgmt Blog Posts\n","date":"2026-02-23T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/breakingchanges.png","permalink":"https://blog.robsewell.com/blog/introducing-microsoftfabricmgmt-managing-microsoft-fabric-with-powershell/","title":"Introducing MicrosoftFabricMgmt: Managing Microsoft Fabric with PowerShell"},{"content":"Introduction This week we have been looking at the fabric-toolbox ecosystem. It is full of fantastic tools for folk that use Microsoft Fabric, and I have been doing a deep dive into the monitoring solutions in particular.\nYesterday we covered FUAM ‚Äî the operational monitoring solution.\nToday we look at another monitoring tool in the collection: FCA, the Fabric Cost Analysis solution.\nWhere FUAM answers \u0026ldquo;how is my Fabric capacity being used?\u0026rdquo;, FCA answers \u0026ldquo;what is it costing us, and who is spending what?\u0026rdquo;\nWhat Is FCA? FCA is a FinOps solution for Microsoft Fabric. It brings cost data from Azure Cost Management into Fabric and breaks it down in ways that are meaningful for Fabric administrators and business stakeholders:\nCost by Fabric capacity Cost by workspace Cost by SKU and billing meter Cost trends over time Showback and chargeback breakdowns The \u0026ldquo;FinOps\u0026rdquo; framing is intentional. FCA is designed around the FinOps framework\u0026rsquo;s principle of making cloud costs visible and understandable to the teams that generate them ‚Äî not just the finance department.\nThis free solution was developed in France by several passionate CSA experts in FinOps and Data: Cedric Dupui, Manel Omani, Antoine Richet, and led by Romain Casteres inspired by his Blog article FinOps for Microsoft Fabric.\nWhere to Find It FCA lives at github.com/microsoft/fabric-toolbox/tree/main/monitoring/fabric-cost-analysis.\nAs with FUAM, the README is the place to start.\nThe FOCUS Standard FCA doesnt reinvent the wheel, it uses cost data in the FOCUS format ‚Äî the Flexible Open Cost and Usage Specification. FOCUS is an emerging open standard for cloud cost data, backed by the FinOps Foundation and increasingly supported by major cloud providers including Microsoft.\nAzure Cost Management can export cost data in FOCUS format, and this is how FCA is built to consume it. The practical benefit is that FOCUS normalises cost data in a consistent way that makes analysis easier ‚Äî categories, meters, and service names are standardised rather than being provider-specific strings.\nIf you are building any kind of multi-cloud cost analysis or want your Fabric costs to integrate cleanly with broader FinOps tooling, FOCUS is worth understanding.\nWhat You Get Once deployed, FCA provides Power BI reports covering:\nA Power Bi report showing cost breakdowns by capacity, workspace, and SKU, with trend analysis over time.\nAn home overview dashboard with total costs A region focused dashboard A Capacity utilisation dashboard - excellent for identifying capacities that are note being used A reservation dashboard - for those with reserved capacity, this is a great way to see how well you are using it A Cost detail dashboard - for showing the year to date and further detailed breakdowns. A Quota dashboard - for showing the quota limits and usage of your capacities (if you have the permissions to get this data) Here is one -\nAnd An Agent for Natural language queries! FCA also includes a data agent so that you can ask questions in English and in French which you can add to Teams :-)\nHow It Works FCA imports cost export files from Azure Cost Management (in FOCUS format) into a Fabric Lakehouse, transforms the data using PySpark notebooks, and surfaces the results through Power BI reports.\nThe data collection is typically run daily or weekly ‚Äî cost data in Azure is available with a day or two of delay, so real-time is not the focus here. Instead, FCA is about trend analysis and retrospective cost review.\nDeployment Overview Configure Azure Cost Management to export cost data in FOCUS format to a storage location your Fabric can access Create a dedicated workspace for FCA in your Fabric tenant Run the setup notebooks from the Create shortcuts in the lakehouse to the storage accounts where the cost data is exported Schedule the data ingestion notebooks This is a most super and wonderful solution that enables larger organisations to eget a great understanding of how their Fabric costs are being generated and by whom. It is a great example of how the fabric-toolbox ecosystem can provide ready-made solutions to common challenges in the Microsoft Fabric world.\nI love it. Hat tip to the team that built it.\nOn Monday On Monday we start the main event ‚Äî a two-week deep dive into the MicrosoftFabricMgmt PowerShell module. 295+ cmdlets, 48 resource types, PSFramework logging, intelligent output formatting, pipeline support, and more. See you then.\n","date":"2026-02-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/fabric-toolbox-fca.png","permalink":"https://blog.robsewell.com/blog/fca-fabric-cost-analysis-for-finops/","title":"FCA - Fabric Cost Analysis for FinOps"},{"content":"Introduction Yesterday I introduced fabric-toolbox ‚Äî Microsoft\u0026rsquo;s community-driven repository of Fabric accelerators. Today we look at one of its flagship solutions: FUAM, the Fabric Unified Admin Monitoring solution.\nWhen you are responsible for a Microsoft Fabric tenant, it will not be very long before you are facing many questions.\nQuestions like:\nHow is my capacity being used? Which workspaces are consuming the most resources? What are my users actually doing? When are my peak usage times? You can scabble around in the Admin portal and try to piece together the answers, but it is a bit like trying to navigate a city with a paper map ‚Äî you can get there eventually, but it is slow and painful, and you will probably miss some things along the way.\nWhat Is FUAM? FUAM is a monitoring solution for Microsoft Fabric administrators. It gives you a holistic, near real-time view of your Fabric platform depending on how frequently you schedule the data collection notebooks to run. It is designed to be a one-stop-shop for all your monitoring needs, providing insights into capacity utilisation, workspace inventory, user activity, and overall health of your Fabric tenant.\nUsing FUAM I am able to answer all of those questions and more, and provide folk with 3 bazillion* reports and dashboards to explore the data themselves. (*Number may be slightly exaggerated but you get the idea)\nIt is all built using Fabric\u0026rsquo;s own native capabilities so can also learn about how to use those capabilities by looking at how FUAM is built. The code is all open source and well documented, so you can see exactly how it works and even contribute if you want to.\nWhere to Find It FUAM lives in the fabric-toolbox repository at github.com/microsoft/fabric-toolbox/tree/main/monitoring/fabric-unified-admin-monitoring.\nThe README is comprehensive and walks you through the prerequisites, deployment steps, and what each dashboard component shows.\nWhat You Get Once deployed, FUAM provides dashboards covering a range of monitoring aspects.\nNo deletion of data older than 14 days like the Capacity Metrics App. This is more useful than you might think, because it allows you to do things like correlate CU consumption with specific reports or activities that you know happened at a certain time, and it allows you to build up a picture of usage patterns over time.\nFUAM extracts the following data from the tenant:\nTenant Settings Delegated Tenant Settings Activities Workspaces Capacities Capacity Metrics Tenant meta data (Scanner API) Capacity Refreshables Git Connections Full list of the reports are here\nHow It Works FUAM collects data using the Fabric REST APIs and the Microsoft 365 audit log, ingests it into a Lakehouse, and transforms it using PySpark notebooks. Power BI reports connect to the Lakehouse\u0026rsquo;s SQL endpoint to provide the dashboards. Simples yeah ? :-) (When somebody else has built it for you!)\nDeployment Overview Deployment involves:\nCreating a dedicated workspace for FUAM in your Fabric tenant Running the setup notebook from the repository, which creates the Lakehouse, tables, semantic models, and reports Configuring authentication (FUAM uses a Service Principal to call the Fabric Admin APIs) Scheduling the data collection notebooks using Fabric\u0026rsquo;s built-in job scheduler The full step-by-step is in the FUAM README. The SPN permissions are the step that tends to catch people out ‚Äî make sure you follow those carefully.\nBest of all You can use it to save time, energy and CUs by using it to feed many other solutions that you may build over time.\nFor example, last week I used it to identify the details of over 40 thousand reports and semantic models to gather information about the RLS on them and I could do it without overloading the Admin API because FUAM had already gathered the data.\nYou can also build queries on top of the FUAM Lakehouse to enable other teams to help themselves to the data they need without having to give them direct access to the Admin API or run their own data collection.\nTrying to build your own Fabric monitoring solutions from scratch is a significant amount of work ‚Äî API pagination, schema management, Power BI data modelling, refresh scheduling. FUAM does all of that for you, and it is maintained by the CAT team and the community, so it improves over time.\nIf you are managing a Fabric tenant of any real size, FUAM is worth the deployment effort. A few hours of setup buys you ongoing visibility that would otherwise take weeks to build yourself.\nTomorrow Tomorrow we look at FCA ‚Äî Fabric Cost Analysis ‚Äî which takes a FinOps approach to understanding what your Fabric capacity is actually costing you. See you then.\n","date":"2026-02-20T00:00:00Z","image":"https://raw.githubusercontent.com/microsoft/fabric-toolbox/refs/heads/main/monitoring/fabric-unified-admin-monitoring/media/general/fuam_cover_flow.png","permalink":"https://blog.robsewell.com/blog/fuam-fabric-unified-admin-monitoring/","title":"FUAM - Fabric Unified Admin Monitoring"},{"content":"Introduction Today I want to talk about fabric-toolbox, the Microsoft-sponsored, community-driven repository of accelerators, tools, and utilities that exists specifically to help Fabric users get further, faster.\nWhat Is fabric-toolbox? fabric-toolbox is a GitHub repository maintained under the Microsoft organisation, led by the Microsoft Customer Acceleration Team (CAT) for Fabric and community contributors. It\u0026rsquo;s a community toolkit, a collection of solutions that go beyond the core product to solve real-world operational, governance, and automation problems.\nThe repository is organised into 5 categories:\nMonitoring ‚Äî tools for understanding what is happening in your Fabric tenant Accelerators ‚Äî end-to-end solutions for common scenarios Tools ‚Äî utilities and PowerShell modules ;-) for managing Fabric programmatically Scripts ‚Äî ready-to-use scripts for specific tasks Samples ‚Äî end-to-end scenario demonstrations Everything in it is open source. Contributions are welcome. Bugs are filed in GitHub Issues. A community is born and grows around it.\nWhat We Will Be Covering Over the next few weeks I am going to write about four things from the fabric-toolbox ecosystem:\n1. FUAM ‚Äî Fabric Unified Admin Monitoring (tomorrow)\nA monitoring solution built entirely with native Fabric capabilities ‚Äî notebooks, Lakehouses, and Power BI dashboards ‚Äî that gives Fabric administrators a holistic view of capacity usage, workspace health, and user activity. If you are responsible for a Fabric tenant, FUAM is worth knowing about.\n2. FCA ‚Äî Fabric Cost Analysis (Friday)\nA FinOps solution that pulls cost data from Azure Cost Management and breaks it down by Fabric capacity, workspace, and SKU. If your organisation is asking \u0026ldquo;how much is Fabric actually costing us?\u0026rdquo;, FCA helps you answer that properly.\n3. MicrosoftFabricMgmt ‚Äî The PowerShell Module (starting Monday the 24th)\nThis is the one I am most invested in, because Jess Pomfret B S L and I have been contributing to it heavily. It is a PowerShell module with over 295 cmdlets covering 48 Fabric resource types ‚Äî workspaces, Lakehouses, Warehouses, Notebooks, Real-Time Intelligence, RBAC, the Admin API, and more. We will spend two weeks going through it in depth.\n##4. Will be no surprise.\nWhy Does This Matter? Microsoft Fabric is a big platform. There is so much to learn about it, and the product team is moving fast, adding new features and capabilities every month. The fabric-toolbox enables you to move quickly forward by giving you expertly written, community-vetted solutions to common problems. It is a place to find tools that save you time, to learn from real-world examples, and to contribute back to the community.\nThe monitoring tools (FUAM, FCA) give you visibility. The PowerShell module gives you control. Together they are a significant part of how serious Fabric teams manage their estates.\nThere is much much more to find there also.\nGetting Started The main repository is at github.com/microsoft/fabric-toolbox. Each tool has its own README with installation and usage instructions. I will cover each one in the posts that follow.\nTomorrow we start with FUAM. See you then.\n","date":"2026-02-19T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/fabric-toolbox-introduction.png","permalink":"https://blog.robsewell.com/blog/introduction-to-fabric-toolbox-microsoft-fabrics-community-accelerator-hub/","title":"Introduction to fabric-toolbox - Microsoft Fabric's Community Accelerator Hub"},{"content":"Introduction Firstly, an apology to my friends (especially Randolph) in the documentation team at Microsoft. I know how hard you work to produce accurate and useful documentation, and I appreciate your efforts. This is not a criticism of your work, but rather an observation about the challenges I faced.\nThis is a story about a recent experience and the lessons learned along the way.\nThe challenge \u0026ldquo;We need to remove all links that have been shared with the whole organisation from our Fabric tenants\u0026rdquo;\nExcellent, Fabric has an API for that - Remove All Sharing Links Just one call and they are all gone.\n\u0026ldquo;Ah, except for these particular ones. They must stay.\u0026rdquo;\nNo problem, I can get all the links, filter out the ones to keep, and then remove the rest.\nThere is an API for that too - Bulk Remove Sharing Links\nIn the documentation it states:\nLimitations Maximum 10 requests per minute. Each request can delete organization sharing links for up to 500 Fabric items.\nThere are about 8,500 links to remove, so I will need to batch them into groups of 500 and make 17 calls. No problem.\n\u0026ldquo;It should take about 20 minutes to complete. Lets say 30 minutes to be safe. I will need to process the entire 100,000 reports as well to be able to match the links that need to be kept, so I will budget a couple of hours for the entire operation.\u0026rdquo; I said confidently.\nThe PowerShell I used the MicrosoftFabricMgmt PowerShell module from Microsoft\u0026rsquo;s fabric-toolbox to do this task.\nNOTE - Not all of the functionality is available today as I have written it to do this task and it has not been released.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # Connect to the Fabric tenant Set-FabricAPIHeaders -TenantId (Get-Secret -Name \u0026#34;FabricTenantId\u0026#34; -AsPlainText) # Get all the sharing links in the organisation $allLinks = Get-FabricWidelySharedLink # get all of the reports as admin $reportsasadmin = Get-FabricAdminReport # Get the list of reports to keep [array]$filteredReports = $reportsasadmin | Where-Object { $_.name -in $reportstokeep } | Select-Object name, id, workspaceId| Sort name | ForEach-Object { Start-Sleep -Seconds 15 $Workspace = Get-FabricAdminWorkspace -WorkspaceId $ _. workspaceId If($workspace.type -eq \u0026#39;Personal\u0026#39;) { $WorkspaceName = \u0026#34;{0} My Workspace\u0026#34; -f $Workspace.name } else{ $WorkspaceName = $Workspace.Name } Write-PSFMessage -Level Important \u0026#34;Exempting report from removal: $($ _. name) in workspace $WorkspaceName ($($ _. workspaceId))\u0026#34; [PSCustomObject]@{ Name = $_.name Id = $_.id WorkspaceId = $_.workspaceId } } $WideSharedReportsToRemove = @() $WideSharedReportsToRemove = $allLinks.ArtifactAccessEntities | Where-Object { $_.artifactId -notin $filteredReports.id} | ForEach-Object { $reportid = $_.artifactId $reportname = $_.displayName $workspaceid = ($reportsasadmin | Where-Object { $_.id -eq $reportid }).workspaceId [PSCustomObject]@{ Reportid = $reportid ReportName = $reportname WorkspaceId = $workspaceid } Write-PSFMessage -Level Important \u0026#34;Report to remove: $reportname from workspace ($workspaceid)\u0026#34; } Write-PSFMessage -Level Important \u0026#34;Found $($WideSharedReportsToRemove.Count) widely shared reports links to delete.\u0026#34; # Create the array of links to remove $RemovingLinkArray = @() $RemovingLinkArray = $WideSharedReportsToRemove | ForEach-Object { [PSCustomObject]@{ Id = $_.Reportid type = \u0026#34;Report\u0026#34; } } I was super careful to double check at each point that each step used the correct data.\nWith the list of reports to remove created and validated, I can then batch them into groups of 500 and call the Bulk Remove Sharing Links API.\n1 2 3 4 5 6 7 8 9 10 11 $totalReports = $RemovingLinkArray.Count $x = 0 $batchSize = 500 While ($x -lt $totalReports) { $batch = $RemovingLinkArray[$x..([Math]::Min($x + $batchSize - 1, $totalReports - 1))] # Call the API to remove the batch of links Remove-FabricSharingLinksBulk -Items $batch -Confirm:$false $x += $batchSize Write-PSFMessage -Level Important \u0026#34;Total attempted so far: $x out of $totalReports.\u0026#34; Start-Sleep -Seconds 10 } Job done. All links removed except the ones we wanted to keep.\nOr so I thought.\nWhen I ran the script, it completed successfully.\nBut when I check the links remaining there were still thousands of links remaining.\nThe output from the script showed the OperationId of the long running operation. I used the Get-FabricLongRunningOperation cmdlet to check the status of some of them.\nwhich produced this output:\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;status\u0026#34;: \u0026#34;Failed\u0026#34;, \u0026#34;createdTimeUtc\u0026#34;: \u0026#34;2026-02-09T13:07:28.951536\u0026#34;, \u0026#34;lastUpdatedTimeUtc\u0026#34;: \u0026#34;2026-02-09T13:07:28.9983967\u0026#34;, \u0026#34;percentComplete\u0026#34;: null, \u0026#34;error\u0026#34;: { \u0026#34;errorCode\u0026#34;: \u0026#34;InternalServerError\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;PowerBISqlOperationException\u0026#34; } } which appeared in no search results but seems to indicate an internal server error.\nI spent some time investigating, trying different batch sizes, I never got the assync operation to give me a status of running, it either immediately succeeded or failed.\nThe solution (well, workaround) ((well, the brute force method)) I worked out that only a batch size of 50 was providing any success and less than that was more reliable.\nI had to repeatedly run the script with a batch size of 40 until almost all of the links were removed. Each time, I ran through the entire list of links to remove. In another session I checked every minute or so the number of remaining links. I could run through the whole list of links and no links would be removed, then the next time through I would see a few links removed, then none, then a few more, then none, and so on.\nI resynced the number of links and removed the ones that needed to remain a couple of times. In hindsight, I should have stored these in a datastore to make this look up simpler and not so time intensive.\nEach time I looped through the links to remove, I hoped that it woud remove all of the links but it was a case of rinse and repeat, and each time I would see some links removed but not all of them. I had to keep running through the list of links to remove until eventually all of the unwanted links were removed.\nThen as the script worked through the last couple of hundred of links a batch size of 10, then 5, then finally 1 was the only way that any links would be removed.\nAn annoying and time consuming process, but eventually all of the unwanted links were removed and luckily the powers that be were understanding of the time taken and the reasons why.\n","date":"2026-02-04T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2026/02/500.png","permalink":"https://blog.robsewell.com/blog/dont-believe-the-documentation-coding-against-the-microsoft-fabric-api-with-powershell/","title":"Don't Believe the Documentation - Coding against the Microsoft Fabric API with PowerShell"},{"content":"Introduction As the year draws to a close, it\u0026rsquo;s a time for reflection and setting new goals. It\u0026rsquo;s also a time to over consume calories!\nFor the last few years in our house we (My wife Traci (S) (L) and I) have been participating in the Festive 500, a cycling challenge that encourages riders to cover 500 kilometers between Christmas Eve and New Year\u0026rsquo;s Eve.\nThere are a couple of reasons that we enjoy this challenge so much. It helps to counterbalance the indulgent eating and drinking that often accompanies the holiday season. (As Jess Pomfret (B) (S) (L) and I often say, we exercise so that we can eat more nice food.) It provides a fun and motivating way to stay active during a time of year when it\u0026rsquo;s easy to become sedentary. Finally, it is a challenge that we can do together, that requires some planning and organising and provides a great sense of achievement.\nThis is a tech blog Rob! Yup, I hear you. So why am I mentioning this here?\nYou can learn a lot about the technology that you work with by creating solutions and solving problems for personal projects and hobbies, just like you do at work.\nThe Festive 500 provides an excellent opportunity to do just that. By tracking our cycling data throughout the challenge, I can create a dashboard that not only helps us to monitor our progress but also to learn and explore things that I dont often get to do in my day job.\nLearning through Hobbies The advantages of using personal projects and hobbies to learn new skills and technologies in this way are many and varied.\nFor me, it is much easier to understand things when I am actually creating something and using it to solve real problems, rather than just reading about it or watching tutorials. This hands-on approach helps to solidify my understanding and makes the learning process more engaging and enjoyable.\nMostly, as I am doing this is for fun, there is much less pressure to deliver something perfect and usually there is no time pressure either. It gives me the freedom to experiment and explore without the constraints often found in work projects. I delete it and restart if I want to, or try something completely different.\nThis also means that I am very aware of backup and recovery :-) I use a ProxMox cluster at home to run my homelab, and I make sure that I have regular backups of all of my LXC\u0026rsquo;s and VMs.\nI make use of the snapshot capabilities of ProxMox to take snapshots before making any major changes or updates to my systems. This way, if something goes wrong, I can easily revert back to a previous state without losing any data.\nI am a heavy user of Git for version control, even for my personal projects. I use GitHub to host my repositories and make sure to commit my changes regularly. This way, I can track my progress and easily revert back to previous versions if needed.\nFinally, working on something that I am passionate about increases my motivation and engagement. I am more likely to stick with it and see it through to completion, which helps me to learn and retain new information. By combining my love of cycling with my passion for technology, I can create a project that is both enjoyable and educational.\nI also learn about the human side of creating something for my tech-adjacent wife to use :-) I have to think about usability, accessibility, and how to present the data in a way that is meaningful and useful to her.\nThe Dashboard To track our progress in the Festive 500, I created a dashboard using Microsoft Fabric. The dashboard pulls in data from Strava and displays it in a visually appealing and easy-to-understand format. (This has last years data so that we could see what it looked like :-) )\nThe dashboard includes several key features:\nProgress Tracking: A visual representation of our progress towards the 500-kilometer goal, including distance covered, remaining distance, and average speed. Number of Days Left: A countdown of the number of days remaining in the challenge, helping us to stay motivated and on track. Hours at Average Speed: A calculation of the total hours required to complete the challenge at our average speed, helping us to plan our rides accordingly. Km required per day: A breakdown of the kilometers needed each day to reach the goal, providing a clear daily target. There is also a Mobile view so that Traci (and I) can check our progress on the go.\nThe Fabric Components I am not going to go into detail on how I built the dashboard here, as that would make this post far too long.\nBut here is a screen shot of my Fabric workspace.\nHappy Holidays! I hope that this post has inspired you to consider using your hobbies and personal projects as a way to learn new skills and technologies. Whether it\u0026rsquo;s cycling, cooking, or any other interest, there are always opportunities to create something meaningful and educational.\n","date":"2025-12-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2025/dashboard-front.png","permalink":"https://blog.robsewell.com/blog/festive-500-learning-through-hobbies/","title":"Festive 500: Learning through Hobbies"},{"content":"Introduction A contained user can create a Windows login as its own account, although as it cannot grant connect permissions it is then is unable to connect at all.\nSo if your vendor application is running as a contained user and during an upgrade it tries to create a login for itself, it will succeed in the creation but then be unable to connect to the SQL Server instance and the upgrade will fail\u0026hellip;\u0026hellip;\u0026hellip;. Sad Trombone.\nGo back to the beginning Rob So this is an odd thing which Kristian asked me about and I thought I would bring to the wider world.\nIt started with a question.\n\u0026ldquo;Can a contained database user create a LOGIN?\u0026rdquo;\nI said No.\nKristian said - Look at this. They had caught a 3rd party vendor running CREATE LOGIN statements which had errored. Fortunately, they had used contained databases for the vendor database and the connecting user because they wanted to reduce the surface area that it was able to affect.\nAlways check your sources So first I tested and I found that I could replicate. I ran it on\nMicrosoft SQL Server 2022 (RTM-CU17) (KB5048038) - 16.0.4175.1 (X64) Dec 13 2024 09:01:53 Copyright (C) 2022 Microsoft Corporation Developer Edition (64-bit) on Windows Server 2022 Datacenter 10.0 \u0026lt;X64\u0026gt; (Build 20348: ) (Hypervisor)\nBecause its what I had available. Kristian, who reported it was running version 16.0.4155.4\nI created a contained database.\n1 2 3 4 5 6 7 ‚Äî Running as sysadmin EXEC sp_configure \u0026#39;contained database authentication\u0026#39;, 1; RECONFIGURE; CREATE DATABASE [containedbeard] CONTAINMENT = PARTIAL Then a contained user jessandrob\\testuser as a Windows User (Yes, I used [Jess Pomfret B and mine test environment. Teamwork makes the dream work! ). I then connected as the contained user and tried to create a SQL login.\n1 2 3 4 5 6 7 USE containedbeard GO CREATE USER [jessandrob\\testuser] --- Connected as jessandrob\\testuser the contained database user CREATE LOGIN IwillFail WITH PASSWORD=\u0026#39;whocares!!0\u0026#39; As expected this fails with\nMsg 15247, Level 16, State 1, Line 5 User does not have permission to perform this action.\nWhich is as expected. The same thing also happens if you try to create a windows login for a different account\n1 CREATE LOGIN [JESSANDROB\\testuser1] FROM WINDOWS Msg 15247, Level 16, State 1, Line 5 User does not have permission to perform this action.\nHowever, if you try to create a login as the same Windows user\n1 CREATE LOGIN [JESSANDROB\\testuser] FROM WINDOWS The Login gets created.\nYAY!!! Oh wait\u0026hellip; NAY!!! Lets take a closer look at the login with some dbatools.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 PS \u0026gt; Get-DbaLogin -SqlInstance sql1 -Login JESSANDROB\\testuser ComputerName : sql1 InstanceName : MSSQLSERVER SqlInstance : sql1 Name : JESSANDROB\\testuser LoginType : WindowsUser CreateDate : 2/11/2025 8:17:24 PM LastLogin : 2/11/2025 8:17:24 PM HasAccess : False IsLocked : IsDisabled : False MustChangePassword : Notice the HasAccess property is set to False. This means that the login cannot connect to the SQL Server instance.\nThis is because the contained user does not have the CONNECT SQL permission on the server. The login is created, but it cannot be used to connect to the SQL Server instance.\n(Get-DbaLogin -SqlInstance sql1 -Login JESSANDROB\\testuser|Remove-DbaLogin -Force will easily remove the annoying login btw )\nKristian found this confusing. As did I.\nIf you look in the documentation it states:\nThe activity of the contained database user is limited to the authenticating database. The database user account must be independently created in each database that the user needs. To change databases, SQL Database users must create a new connection. Contained database users in SQL Server can change databases if an identical user is present in another database.\nBut nowhere does it say that a contained user can create a login for itself. So it is a bit of a gotcha. I think the documentation should be updated to clarify this.\nConclusion So, what‚Äôs the takeaway? If you‚Äôre using contained databases, keep an eye on your vendor applications. They might be trying to do things they shouldn‚Äôt, like creating logins they can‚Äôt even use. And if you run into this issue, now you know what‚Äôs going on‚Äîand how to clean it up.\n","date":"2025-04-23T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2025/contained.png","permalink":"https://blog.robsewell.com/blog/odd-error-with-contained-database-users-look-out-for-your-3rd-party-vendors/","title":"Odd Error with Contained Database Users - look out for your 3rd party vendors"},{"content":"\nT-SQL Tuesday, which was started by Adam Machanic (github is hosted by a different person each month. The host selects the theme, and then the blogging begins. Worldwide, on the second Tuesday of the month (all day, based on GMT time), bloggers attend this party by blogging about the theme.\nThis month it is hosted by Deborah on the theme of mentoring and sponsorship. Thank you Deborah for hosting.\nMentoring I am lucky to have mentored some fantastic people in the community and been able to watch them grow and develop. Chris W, Eivind, Chris J, Justin, Olivier, Umar, Mikey, Thomas, Craig and Paul are some of the official mentees that I have had and even better some of them have gone on to mentor others so I have grand-mentees like Grace and others.\nIt makes me proud to know that I have been able to provide a tiny bit of help to them and that they have gone on to do great things and that that influence is continuing to spread onto others as well.\nWhats in it for me? I have learned so much from my mentees. They have taught me about new technologies, new ways of working, new ways of thinking. They have challenged me to think about things in a different way. They have made me a better person, a better DBA, a better developer, a better speaker, a better writer. They have made me a better mentor. Every person is different and needs new and different things from a mentor. I have learned to adapt and change my style to suit the person I am mentoring.\nThey have made me a better person and I thank each and every one for that. Without doubt, one of the greatest benefits of sharing, blogging, speaking is the knowledge that you build to enable you to do so. You learn so much because you are the one doing the teaching and the sharing and as such you know that you need to research and understand the topic in order to be able to explain it to others. You also learn from the questions that you are asked.\nMentoring is exactly the same. You learn more about being a mentor because you are mentoring, you learn more about explaining situations, you learn more about listening, you learn more about understanding. You have to develop new ways of explaining scenarios and concepts as you work alongside new people with different experiences and different ways of thinking.\nThis can only be of benefit to yourself. You can and will take this knowledge on from the mentoring relationship and use it in your day to day work and life. You will be a better team member, a better leader, a better manager because of the things that you will learn.\nWhat can you do? If you are not already mentoring someone, then I would encourage you to do so. You will learn so much from the experience. One way that you can do so is via New Stars of Data. This is a fantastic program that pairs mentors with mentees. You can find out more about it here You can also join the New Stars Track at the PASS Data Community Summit will run for the third time this year and is designed for speakers who have presented before but never at a big international event. Data Scotland and Dataminds Connect are also great events that support new speakers and mentors.\nOr go to your local user group and offer to mentor someone. Or ask someone to mentor you.\nYou can even be a mentor without realising it Just be kind, be helpful, provide support to people. Online and in-person. You never know who you are influencing and who you are helping. You never know who you are mentoring.\n","date":"2025-03-11T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2025/mentor.png","permalink":"https://blog.robsewell.com/blog/why-be-a-mentor/","title":"Why Be a Mentor?"},{"content":"Can you help? Erland said The first I realised about this problem was when Erland Sommarskog b asked me if I could help with this forum post The poster was getting an error when trying to run a Notebook Agent Job. The error was a very generic instance not found error\nA network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider error: 40 - Could not open a connection to SQL Server) The system cannot find the file specified\nReproduce the issue How very odd I thought. I noticed that it was a named instance, so I tried to recreate the issue on my own named instance DAVE. I added a new notebook job and when I tried to run it. It failed\nHuh ? Hmm. But I got a different error.\n\u0026lsquo;Login failed for user \u0026lsquo;NT Service\\SQLAGENT$DAVE\u0026rsquo;\nThat doesn\u0026rsquo;t make a lot of sense. Why is the login failing?\nWhere is the logon ? I checked the error log using\nGet-DbaErrorLog -SqlInstance $SqlInstance -Source logon\nbut there were no results.\nWait a minute.\nBingo I have the default instance running on this machine. Lets check the error log on that instance as well.\nGet-DbaErrorLog -SqlInstance $ENV:COMPUTERNAME -Source logon\nLogin failed for user \u0026lsquo;NT Service\\SQLAgent$DAVE\u0026rsquo;. Reason: Could not find a login matching the name provided. [CLIENT: ]\nSo the Agent service for the named instance is failing to logon to the default instance on the host.\nLet\u0026rsquo;s recreate the error properly this time Stop the default instance\nStop-Service MSSQLSERVER -Force\nand rerun the job, which fails, and get the error message\nGet-DbaAgentJobHistory -SqlInstance $SqlInstance -Job $JobName\nSo I have recreated the error.\nNow to solve it First we need to understand what is happening. If we look at the Job Step for the Agent Job in Azure Data Studio\nThere is some PowerShell running to execute the cells of the notebook and gather the results and place them in the database for later use.\nThe code uses $(ESCAPE_SQUOTE(A THING)) which is passing the Agent Job tokens to the script. you can find the list of Agent Job tokens here\nThen it calls Invoke-SqlCmd without a ServerInstance parameter. If you look at the documentation for the parameter web\nit shows the Default Value as None but this does not explain what it does. With no value set for the ServerInstance parameter, Invoke-SqlCmd will try to connect to the default instance as we can see below.\nSolution So we can get the Agent Job Tokens for the host and the instance name and set them as a variable and pass them to the ServerInstance paramater every time that it is called.\n1 $SqlInstance = \u0026#39;{0}\\{1}\u0026#39; -f \u0026#34;$(ESCAPE_SQUOTE(MACH))\u0026#34;, \u0026#34;$(ESCAPE_SQUOTE(INST))\u0026#34; Better Solution To save a load of copy pasta, the risk of not identifying all of the calls to the cmdlet, and keep the exising coding standards we can instead use PsDefaultParameters\n1 2 3 4 5 $SqlInstance = \u0026#39;{0}\\{1}\u0026#39; -f \u0026#34;$(ESCAPE_SQUOTE(MACH))\u0026#34;, \u0026#34;$(ESCAPE_SQUOTE(INST))\u0026#34; $PSDefaultParameterValues = @{ \u0026#34;Invoke-SqlCmd:ServerInstance\u0026#34; = $SqlInstance } What is this doing? It is creating a variable named SqlInstance that is made up of the SQL Agent Job Tokens for the machine and the instance name\nThen it is setting that variable as the default value for the ServerInstance paramater of the Invoke-SqlCmd cmdlet for this session only\nThis means that all of the times that the Invoke-SqlCmd cmdlet is called it will use the correct value whether it is on a default or a named instance.\nMost best solution As this code is available on GitHub and anyone can create an issue or a Pull Request, I did just that :-)\nIssue - https://github.com/microsoft/sqltoolsservice/issues/2305\nPull request - https://github.com/microsoft/sqltoolsservice/pull/2306\n","date":"2024-01-03T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2024/ads.png","permalink":"https://blog.robsewell.com/blog/making-notebook-agent-jobs-on-named-instances-work/","title":"Making Notebook Agent Jobs on Named Instances work"},{"content":"Now I know this! I was using some code that I had written for Azure Pipelines for a Windows agent which had $(System.AgentToken) as the variable name and all other pre-defined variables were the same PascalCase and separated by . but the Linux agent needed all upper case and separated by _\nIn VS Code CTRL SHIFT P to open the command pallette and then search for uppercase :-)\nSimples\n","date":"2023-08-20T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/uppercase.png","permalink":"https://blog.robsewell.com/blog/changing-to-upper-case-in-vs-code/","title":"Changing to Upper Case in VS Code"},{"content":"So that I will remember! When running WSL in a corporate environment, I sometimes an error like the following, where the cli is not able to resolve the IP address correctly. This error is from terraform but I have seen it elsewhere.\n‚ï∑ ‚îÇ Error: Failed to query available provider packages ‚îÇ ‚îÇ Could not retrieve the list of available versions for provider hashicorp/random: could not connect to registry.terraform.io: failed to request discovery document: Get ‚îÇ \u0026ldquo;https://registry.terraform.io/.well-known/terraform.json\": dial tcp: lookup registry.terraform.io on 123.45.67.89:53: read udp 123.45.67.89:51522-\u0026gt;123.45.67.89:53: i/o timeout\nto fix it\nChange the DNS Server in /etc/resolv.conf 1 sudo nano /etc/resolv.conf I normally just change the nameserver to 1.1.1.1\nYou will need to do this for every tab you open or you can follow the instructions to set it permanently\n1 2 3 # This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: # [network] # generateResolvConf = false Now I know :-)\n","date":"2023-08-20T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/wsl-dns-fail.png","permalink":"https://blog.robsewell.com/blog/when-wsl-dns-fails-you/","title":"When WSL DNS fails you"},{"content":"So that I will remember! It took me too long to find this. The google-fu was failing. How to download terraform providers. How can I download terraform providers. Downloading terraform providers. Getting terraform providers locally. I even checked for my favourite misspelling!\nbut actually the answer to downloading the terraform providers locally for the mirror is in the documentation https://developer.hashicorp.com/terraform/cli/commands/providers/mirror\nThe terraform providers mirror command downloads the providers required for the current configuration and copies them into a directory in the local filesystem.\n1 terraform providers mirror /tmp Simples\n","date":"2023-08-17T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/terraform.png","permalink":"https://blog.robsewell.com/blog/how-to-download-terraform-providers/","title":"How to download terraform providers"},{"content":"dbachecks v3 at Data Saturday Oslo On Saturday September 2nd 2023, will be Data Saturday Oslo in Oslo, Norway.\nFor more information about this free Saturday event, visit the official event page at DataSaturday0035 where you can sign up. Don\u0026rsquo;t wait, spaces are limited!\nI will be presenting a session with Jess Pomfret Blog Mastodon and Cl√°udio Silva Blog Twitter introducing dbachecks v3.\nabstract dbachecks v1 was released back in 2018, bringing together the DBA PowerShell magic of dbatools with pester, a PowerShell testing framework to create a powerful infrastructure validation tool.\ndbachecks is able to answer questions such as:\ncan I connect to all of my instances? was my last full backup with 7 days are my databases owned by the expected login am I using \u0026lsquo;page verify\u0026rsquo; to detect corruption are any of my certificates nearing expiration and many more\u0026hellip; v2 introduced the ability to store these results in a database and create a time series chart showing changes over time.\nBut now, we introduce v3!\ndbachecks v3 takes advantage of Pester v5 - a total rewrite of the tool enabled us to not only increase the performance of the module but position it to be able to grow and expand in a controlled and predictable way in the future.\nIn this session, we\u0026rsquo;ll talk about the changes we\u0026rsquo;ve made to the module, demo the new features and prove the massive performance increase using Profiler.\nCome and join us as we share the biggest release yet of the dbachecks module and learn how you can utilise it in your environments.\nFor more information about the free Saturday event, visit the official event page at DataSaturday0035. Don\u0026rsquo;t wait, spaces are limited!\nYou can also join Jess and I on Friday 1st September 2023 for a power-packed day of learning, networking, and enhancing your DBA skills.\nReady to take your DBA skills to the next level in a hybrid environment? Secure your spot now by registering at the Eventbrite page: Register Here.\n","date":"2023-07-14T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/datasaturdayoslo-session.jpeg","permalink":"https://blog.robsewell.com/blog/dbachecks-v3-at-data-saturday-oslo/","title":"dbachecks v3 at Data Saturday Oslo"},{"content":"Jess and Rob are doing training in Oslo in September at Data Saturday Oslo On Friday September 1st 2023, I am lucky enough to be delivering a full days training in Oslo, Norway with Jess Pomfret Blog Mastodon\nReady to take your DBA skills to the next level in a hybrid environment? Secure your spot now by registering at the Eventbrite page: Register Here.\nDesigned specifically for DBAs working in hybrid environments, this training day will equip you with the knowledge and tools needed to excel in your role. Whether you\u0026rsquo;re managing on-premises databases, Platform as a Service (PaaS) offerings, or Azure virtual machines, this training has you covered.\nAs technology evolves, so does the role of a Database Administrator (DBA). In today\u0026rsquo;s hybrid computing landscape, where applications are deployed across on-premises and cloud platforms, DBAs face new challenges and opportunities. To navigate this changing landscape, it\u0026rsquo;s crucial to stay ahead of the curve and expand your skill set.\nAutomation One of the key focuses of the training is automation. As authors ofdbatools in a Month of Lunches\u0026quot; we will provide hands-on experience using automation tooling, such as PowerShell and community-developed modules like dbatools, dbachecks, and ImportExcel. You\u0026rsquo;ll learn how to automate routine tasks, streamline your operations, and save valuable time and effort. From simple tasks to complex processes, automation will become your best friend in managing your hybrid environment.\nAdditionally, this training we\u0026rsquo;ll introduce you to the concept of infrastructure as code. You\u0026rsquo;ll discover how to automate the provisioning, configuration, and management of your SQL Server instances and PaaS services. The training doesn\u0026rsquo;t stop there. You\u0026rsquo;ll explore the power of GitHub Actions and Azure Functions, unlocking the ability to integrate your workflows with other tools in your environment. These techniques will elevate the quality of your work, reduce manual intervention, and improve overall productivity.\nWhether you\u0026rsquo;re a seasoned DBA, a developer working with SQL Server, an accidental DBA, or someone with DevOps responsibilities, this training is tailored to suit your needs. Jess and Rob will guide you through various scenarios, enabling you to sharpen your skills and empower your team to be more efficient.\nRest assured, you don\u0026rsquo;t need any specific equipment or tools to attend this training. All scripts and slides will be provided, and you\u0026rsquo;ll have the opportunity to follow along with most of the demos. Just bring something to take notes with, as you\u0026rsquo;ll be learning a wealth of valuable solutions to bring back to your workplace. If you bring a laptop then having a container runtime like docker or podman will be beneficial.\nRegister Here Secure your spot now by registering at the Eventbrite page: Register Here.\nWho are we? Jess Pomfret B M is a Data Platform Architect and a Microsoft MVP. With a passion for SQL Server and PowerShell, she has been solving performance tuning challenges and automating processes since 2011. Jess is an active contributor to the open-source PowerShell modules dbatools and dbachecks, which help DBAs streamline SQL Server management. She\u0026rsquo;s also made significant contributions to the SqlServerDsc module, enhancing its configuration capabilities.\nRob Sewell is a former SQL Server DBA turned automation advocate. His areas of expertise include PowerShell, Azure, and SQL (PaaS). As a Cloud and Data Center MVP and Data Platform MVP, Rob actively supports and participates in various Data and PowerShell events worldwide. He shares his knowledge and experiences through his blog and Twitter, constantly engaging with the community. When he\u0026rsquo;s not looking at a screen, Rob enjoys riding his bike and staying active.\nJoin Us Join us for this exciting training day, packed with demos and practical solutions that you can immediately implement in your hybrid SQL Server environment. Regardless of your experience level, this training will help you take your skills to the next level and optimize your workflows.\nReady to take your DBA skills to the next level in a hybrid environment? Don\u0026rsquo;t miss out on this exclusive training day with Jess Pomfret and Rob Sewell. Secure your spot now by registering at the Eventbrite page: Register Here.\nFor more information about the free Saturday event, visit the official event page at DataSaturday0035. Don\u0026rsquo;t wait, spaces are limited!\nJoin us on Friday 1st September 2023 for a power-packed day of learning, networking, and enhancing your DBA skills. We can\u0026rsquo;t wait to see you there!\n","date":"2023-07-12T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/datasaturdayoslo-trainingday.png","permalink":"https://blog.robsewell.com/blog/the-dba-in-a-hybrid-environment-elevate-your-skills-with-jess-pomfret-and-rob-sewell-in-oslo/","title":"The DBA in a hybrid environment: Elevate Your Skills with Jess Pomfret and Rob Sewell in Oslo"},{"content":"Grateful I never take it for granted and again I am humbled and indeed honoured (*) to again receive the MVP Award for my contributions in\nData Platform Cloud and Datacenter Management What did I do last year ? You can see most of it on my public MVP profile Here is a summary\n18 sessions at conferences including SQLBits, PSCOnfEU, PASS Summit, Data Scotland, Data Grillen, PSConfEU MiniCon, PSDayUK, South Coast Summit 6 sessions for User Groups - in person and remote Conference Organising Committee member for SQLBits, PSConfEU and PSDayUK Mentoring of new speakers Open Source Projects Contributions - Microsoft projects, dbatools, dbachecks, Data Saturdays, Call For Data Speakers also some other things that I cannot talk about, normally related to feedback about new products or functionality to Microsoft Product Groups\nWhat does it change? Nothing.\nIt will make absolutely no difference to the things that I do to help and assist the technical communities that I am a part of. I will continue to help to organise events, to speak at events, to blog, to be a part of the open-source community. I will carry on mentoring speakers, event organisers, other community members as they request it.\nI will continue to be vocal and supportive of people who are not like me. I will stand up and call out discrimination. I will assist events with code of conducts. I will challenge micro-aggressions that are faced, particularly by women. I will make it normal to talk about these things and help people to understand and acknowledge that they exist and what can be done to reduce them.\nI will continue to not take myself seriously, to wear fun outfits and allow myself to be the focus of comedy.\nI hope to have many more photos taken of me, like this one\nwhere I am giving a keynote at the PowerShell Conference Europe dressed in a rainbow suit and talking about how to communicate with other people appropriately\nThank you to all of the people who make this happen, to those that support me and challenge me and let me work alongside them to do amazing things.\nSorry Aleksandar but it\u0026rsquo;s true :-) ","date":"2023-07-06T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/MVP_Logo_Horizontal.png","permalink":"https://blog.robsewell.com/blog/email-subject-congratulations-on-your-mvp-award-mvpbuzz/","title":"Email Subject - Congratulations On Your MVP Award - #MVPBuzz"},{"content":"Remote Working We have spent a lot of time remote working and I was recently reminded about this troubleshooting tip that I use frequently.\nWhen working with remote clients I am often required to be able to give access to the machine to the client to be able to install the VPN client and log into their domain. I use a Hyper-V machine on my home office desktop PC and use that.\nOf course, all sound hardware is attached to my host PC and sometimes getting that to work with Teams running inside the Hyper-V can be challenging. Sometimes, I have muted the hardware either in Windows or on the device itself.\nThe end result is that either I can\u0026rsquo;t hear people in the meeting or tey can\u0026rsquo;t hear me.\nBut sometimes that is due to there being nobody speaking in the meeting or the issue being at the other end.\nSee what Teams is hearing This is one method that I use to try to narrow down where the issue is. Use live captions to see what Teams is \u0026ldquo;hearing\u0026rdquo;. I like to use live captions, especially when working with teams of people for whom English is a second language as it is often better at cutting through the accent than I am and it also gives me a chance to read back to ensure that I have understood hte sentence correctly.\nYou can turn on Live Captions by clicking the 3 dots (hamburger menu) in the meeting and then Language and speech and Turn on live captions It will ask you which language the meeting is in and you are done. Now you will be able to see if other people are speaking and also if Teams is \u0026ldquo;hearing\u0026rdquo; you and troubleshoot accordingly.\n","date":"2023-07-05T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/teams-live-captions.png","permalink":"https://blog.robsewell.com/blog/troubleshooting-teams-sounds/","title":"Troubleshooting Teams sounds"},{"content":"What is SQLBits? SQLBits is the largest data platform conference in Europe. It has been running every year since 2007 in a different city in the UK providing sessions into all things data platform. I have frequently written about SQLBits, it is close to my heart and has had a significant impact on my life, my career and my circle of friends.\nCommittee The SQLBits committee is responsible for the running of the conference, the selection of general sessions and training days, helpers, sponsors, venue, dates, child care, theme, logo, the party, the website, charity support, hotels, bars, delegate app, helpers app, sponsors app, food, drinks, entertainment, prizes, swag, t-shirts, mugs, code of conduct, badges, printing, freight, stickers, lanyards, bags,on-site therapist, pens, notepads\u0026hellip; and everything else that goes into making SQLBits the best conference it can be!!\nBefore the conference We have been meeting every week as a committee to plan the conference. Every committee member has also spent a lot of time in meetings, dealing with correspondence, answering questions, and then feeding this back to the whole committee. We received some feedback from one of our largest sponsors that one of the things that the liked about SQLBits was that we are all so passionate about the conference and that it really shows. We are all volunteers and we all have full time jobs, families, and other commitments but we are also determined to make SQLBits the best Microsoft Data Platform conference in the world.\nSet up We all arrived in Newport on the Sunday or the Monday before the conference to set up. Set up is a massive operation involving a significant number of people who turn the big empty room into the exhibition hall, build all of the sponsor booths, set up all of the rooms with a stage, screens, signage and projectors, ensure that there is wifi, power, and hard-wired internet access in them all. There was also a team taking all of the output from the speakers desk and ensuring that it will be available for the live streams for the online delegates. Whilst all of that is happening, we were unpacking all of the things that had been in storage (or peoples garages) getting all of the t-shirts for the helpers, for the speakers and for the committee. All of the speaker swag and delegate swag was put into a box or a bag by some of the orange shirted helpers. We also had a \u0026ldquo;Meet the trainer\u0026rdquo; event in the evening which allowed delegates to come and interact with the speakers. This was a great opportunity for delegates to ask questions and get to know the trainers before the training days.\nThat was all expected.\nHowever, there are also things that happens behind the scenes during this time. For example, due to a storm over Northern Europe which impacted flights into and out of Schiphol, London and Bristol Airports as well as the trains running from London to Newport, there was a point on Monday afternoon where we were not sure that over half of our training day speakers for the next day would be able to make it. This lead to a couple of hours of manic phone calls and communication with the speakers as we tried our best to get them to the conference. We were very lucky that with only one exception, everyone made it to the conference. Unfortunately, despite best efforts and a lot of searching, one speaker was unable to get to the UK in time and so we had to switch their rooms to a room with virtual speakers enabled and update all of the schedules and teams involved in providing the content. These are the things that you don\u0026rsquo;t see, but that are part of the magic of ensuring that the conference runs smoothly. I hope all of the delegates and most of the speakers and sponsors have no idea of the things that were happening behind the scenes. I always describe it like a swan, it looks calm and serene on the surface, but underneath the water, there is a lot of paddling going on!\nTuesday For me, Tuesday was a training day. With my fantastic partner in fun and games and sharing - Jess Pomfret B M we spent the whole day with about 75 people both in person and online and talked about how to manage large SQL estates. We had a lot of fun and I hope that everyone who attended enjoyed it as much as we did. We had a lot of questions and a lot of interaction with the delegates in both formats and it showed that the system that we had to enable virtual delegates for SQLBits really managed to work well.\nEquity and Inclusivity We had a lot of positive feedback from the delegates and we are really pleased that we were able to provide this opportunity for even more people to attend the conference. We are passionate about Diversity, Equity and Inclusivity and the feedback that we have received is that allowing virtual attendees enabled more people to attend the conference and to learn from the speakers. People who would not have been able to attend because of their family commitments or because of the cost of travel and accommodation were able to attend and this is one of our goals for the conference.\nConcentrating on Inclusion and Equity will naturally have a positive impact on our Diversity and our goal is to make SQLBits a conference that is welcoming to everyone. The DE and I teams initiatives such as childcare are beginning to bear fruit and we are seeing more and more people attending the conference with their families, as well as more people from underrepresented groups attending the conference and we are working hard to ensure that we are providing a safe and welcoming environment for everyone.\nGetting everyone in Whilst Jess and I were doing the training day, the rest of the team were ensuring that everyone was able to get in, with a badge, registered with the session that they expected, or the one they wished to change to. Some sessions had a limited number of places and so we had to ensure that the people who were on the waiting list were able to get into the session that they wanted to attend if places became available. We had to print the badges for the people who had signed up in the days before we sent the information to the printers. We had people paying for new tickets right up until Friday!\nWednesday There was another \u0026ldquo;Meet the trainer\u0026rdquo; event on the Tuesday evening and another set of speakers and delegates who were arriving as well as sponsor teams and their freight. Wednesday morning did not see as much of a rush as Tuesday morning, but there was still a lot of people arriving and needing processing or having questions that needed answering. At events like SQLBits there is also a lot of communication and conversation with the venue. Ensuring that there is coffee at the times that everyone comes out of the rooms for breaks and that there is lunch at the right time. When you are thinking about lunch, you also need to think about the sponsors. On General sessions days we need to ensure that they can have lunch before all of the delegates come out of their sessions and they need to spend their time talking with them and demonstrating their products. Wednesday evening was also when the sponsors were able to setup their booths. This, of course, generates a large number of questions that need answering and boxes that need finding and then unpacking.\nEvening events Wednesday saw the first of the large evening events. We provided a games night with food. Thursday there was a quiz night with curry and Friday was the Party. As you can imagine, there is a significant amount of organisation required behind the scenes for all of these additional events. Food, drinks, prizes, the folks providing the entertainment and the helpers. The venue, security, transport from the hotels to the venue and back again. All of this needs to be organised and then managed on the day. We had a lot of fun at these events though and find them a great way to end each day and enable delegates who want to, to be able to network with each other and with the sponsors all whilst doing something fun. The party is organised by the wonderful, sparkly Julia and her team and they do a fantastic job of making sure that everyone has a great time.\nGeneral sessions General sessions then add further complexity, more delegates, more speakers, more moving parts that always have the possibility for things to require fixing. I have been a SQLBits helper and runner enough times to know that there were many things requiring attention, this year my responsibility was the agenda and updating the processes behind that when things changed and answering the myriad of questions that come up at the information desk. It is a lot of fun but also a lot of work.\nWrap up This is a totally raw outpouring of my experience this year, pretty much without filter. Just so that I had it down somewhere in words.\n","date":"2023-03-19T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/dragonsqlbitslogo.png","permalink":"https://blog.robsewell.com/blog/sqlbits-2023-in-newport-a-beards-viewpoint/","title":"SQLBits 2023 in Newport - A beards viewpoint"},{"content":"What is SQLBits? SQLBits is the largest data platform conference in Europe. It has been running every year since 2007 in a different city in the UK providing sessions into all things data platform. I have frequently written about SQLBits, it is a conference close to my heart and has had a significant impact on my life, my career and my circle of friends.\nWhere and when? This year it is in Newport, Wales at the International Convention Centre Wales (ICC) Tuesday 14th to Saturday 18th of March, 2023\nYou can register to attend here\nWhat is there at SQLBits? Tuesday and Wednesday are training days with all day sessions provided by subject matter experts including Microsoft Product Group members, Microsoft Valued Professionals, Microsoft Certified Trainers, and other experts. There are 30 options covering all areas of the Data Platform and none-technical sessions as well.\nYou can see the training day agenda here\nThursday and Friday have 50 minute, 20 minute and 5 minute sessions with a wide range of topics and levels.\nSaturday is the FREE to attend day. It also has 50 minute, 20 minute and 5 minute sessions with a wide range of topics and levels.\nThere are about 250 sessions on Thursday, Friday and Saturday\nYou can see the general session agenda here\nWhat else is there outside of technical stuff? Oh My!!\nThe biggest benefit is the people, for networking, for answering questions, building relationships with Microsoft product group or local Microsoft, meeting companies who are sponsoring, finding your new job or your new team members, learning and sharing with your peers.\nThere is also a pub quiz on Thursday evening, the Friday night costume party, the community zone.\nHow do I find the sessions? With so many sessions, its hard to find the ones that you want or to get a good overview in the format that you want. So I built a PowerShell module to get that information for you easily in any format you like. (Editor - thats a fib, he built it so that he could write Pester to ensure that speakers were not scheduled when they were not available)\nThe SQLBitsPS module You can find the SQLBitsPS PowerShell module on the PowerShell Gallery\nAs with all PowerShell modules from the Gallery, you can install it by running\nInstall-Module SQLBitsPS\nI find that a lot of people like to use the ShowWindow parameter to have the help in another searchable window.\nGet-Help Get-SQLBitsSchedule -ShowWindow Use the help to find out how to use any PowerShell command you should use Get-Help and this module is no different. The help for the commands is built in and can be accessed with\nGet-Help Get-SQLBitsSchedule\nGetting the schedule You can just run Get-SQLBitsSchedule and by default it will get the schedule and if you have the ImportExcel module available it will write an Excel Workbook with each days agenda on a different sheet and colour code the service sessions like Registration, lunch and coffee breaks\nTo save you having to click to open Excel I have added a Show parameter which will open it for you!\nI would like a csv instead The output parameter gives you a number of options for the format of the output. If you do not have the ImportExcel module it will default to -output csv which you can also combine with the Show parameter\nGet-SQLBitsSchedule -output csv -show\nI would like a html page instead It\u0026rsquo;s not awesome but you can also create an HTML page. This may be useful if you wish to print the agenda yourself so that you have a hard copy.\nGet-SQLBitsSchedule -output html -show\nLet me decide what format I want You can also output a [pscustomobject] which you may use to PowerShell to your hearts content!!\nGet-SQLBitsSchedule -output object |Format-Table\nMaybe you would like to see the sessions that are on Friday at 16:50\nGet-SQLBitsSchedule -output object |Where-Object {$_.Day -eq 'Friday' -and $_.StartTime -eq '16:50'} They look amazing, I recommend Mladen Prajdic session. That one blew me away at Data Grillen and I may very well attend that again.\nI can\u0026rsquo;t do anything fancy, just let me search If all you want is to search for your favourite speaker then you can use the search parameter which will perform a wildcard search.\nGet-SQLBitsSchedule -search Cathrine -output object\nGet-SQLBitsSchedule -search Monica -output object\nYou can also use it search for topics\nGet-SQLBitsSchedule -search 'Mental Health' -output object\nand even to search for wisdom!!\nGet-SQLBitsSchedule -search wisdom -output object I want to make it better Awesome, thank you.\nThis is all open-source and you can find it on GitHub at\nhttps://github.com/SQLDBAWithABeard/SQLBitsPS\nThere are some brief instructions here\nhttps://github.com/SQLDBAWithABeard/SQLBitsPS/blob/main/DevelopingREADME.md\nJoin us SQLBits is the largest data platform conference in Europe. It has been running every year since 2007 in a different city in the UK providing sessions into all things data platform.\nWhere and when? This year it is in Newport, Wales at the International Convention Centre Wales (ICC) Tuesday 14th to Saturday 18th of March, 2023\nYou can register to attend here\n","date":"2023-01-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2023/dragonsqlbitslogo.png","permalink":"https://blog.robsewell.com/blog/sqlbits-agenda-and-powershell-displaying-and-searching/","title":"SQLBits Agenda and PowerShell, displaying and searching"},{"content":"Thank you to the ever wonderful Ben for the suggestion to blog. Check out #NewStarNovember for more blog posts.\nWho was your mentor? I have never had an official mentor but I am so very very grateful to all of the people who have helped, supported and inspired me throughout my career.\nI feel so privileged to be able to reach out to so many wonderful people and ask questions and receive advice.\nSo Rob, Why do you mentor others? I have always been willing to share everything that I know and have been doing so in various formats for many years now. I have officially mentored many people for presentations, through events such as New Stars of Data, Data (n√©e SQL) Grillen, Data Minds, Data Scotland, as well as unofficially because they have asked me.\nWhy do I do this? because I enjoy it, because I believe that anyone can give a good presentation and because the more voices we have giving presentations the better that it is for us all.\nWhat\u0026rsquo;s the best thing about mentoring? So many things are fantastic about mentoring.\nYou learn so many things. Getting the opinions of different people or watching how others perform similar tasks to you almost always leads to you learning something new. Whether it is the subject they are talking about, or just the way that they use PowerPoint or VS Code, or even the way they have Windows set up there is always something to learn.\nI also get such a warm fuzzy feeling from seeing people blossom as they gain confidence and realise that they can do a fantastic presentation and that it is a mainly a case of learning and practicing new skills.\nI am super proud of the people that I have mentored who have continued to present and have since presented at international conferences.\nI am also equally proud of those who have decided that it was not for them right now and are currently taking a break from presenting.\nI am delighted to see people who deserve it, become MVPs, or get promoted or change jobs. I hope that maybe I had a little impact, although I tell everyone I mentor, they are doing all of the hard work I am just listening and commenting.\nDoesn\u0026rsquo;t it take a lot of time? There is, of course, a time element to being a mentor but I am always happy to give up a few hours each month to help and I am also careful that I don\u0026rsquo;t take on too much and over burden myself.\nFor me, being a mentor means giving guidance and setting goals and with a bit of careful planning you can be efficient about it.\nWhat else have you learnt? I have learnt that everyone is different and that, as a mentor, some of your tactics and procedures will not work for every person. As an example, I have learnt that some people like to have a script that they can use to memorise or to have as an aide whilst others are much happier just with key words.\nI have learnt that time zones make it harder. If it is Sunday evening for me, it is Sunday morning on the west coast of the US. This can make it harder to find times that fit well for both parties.\nWhat should I do? If someone asks or suggests that you do a presentation or become a mentor then I say that you should give it a try. They believe in you and so do I. New Stars Of Data CFP are open and available for mentors as well as new speakers.\nIf this blog post makes you think \u0026ldquo;I would like to give this a go\u0026rdquo; at presenting or mentoring, then you can reach out to your local user group. You are a member of your local user group I hope? If not, you can find yours in the Azure Data Community.\nDon\u0026rsquo;t want to speak in person the first time? You can sign up for New Stars Of Data and you will be presenting virtually on the 2023-05-12. They are also taking mentors for the speakers for this event.\nGOOD LUCK\nI believe in you\nYou can do it.\n","date":"2022-11-27T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/newstars.jpg","permalink":"https://blog.robsewell.com/blog/newstarnovember-being-a-mentor/","title":"#NewStarNovember - Being a mentor"},{"content":"Thank you to the ever wonderful Ben for the suggestion to blog. Check out #NewStarNovember for more blog posts.\nIn the beginning It was around May of 2013 that I had written a blog post about spinning up and shutting down an Azure lab with PowerShell after being inspired at my first SQLBits Conference (On a side note, I have attended, volunteered or spoken at every one since and this year I was delighted to become a Committee Member - If you have comments/questions/advice please feel free to email me at rob at sqlbits.com). That blog post still exists here\nAt the next SQL SouthWest User Group (as it was known then) Jonathan had said and next meeting we will have a session from Rob about PowerShell and Azure VMs. That was pretty much it. I thought why not give it a go. The worst that happens is that I don\u0026rsquo;t like it and don\u0026rsquo;t ever do it again. (Narrator - exactly the opposite happened - He loved it and does it at every opportunity)\nIt was not the most brilliant presentation. I learned a lot and blogged about it too as you can read here. But I enjoyed it and it gave me the confidence to do it again and set me on a path to the wonderful life I have now.\nShould you speak at your User Group? Yes\nOK, you want a bit more?\nYes you definitely should.\nYou should try it once.\nYour user group should be filled with some familiar faces of people who you know will support you and only want the best for you. It will usually be a smaller group than at an event like a Data Saturday or other local to your town/city event. Your user group leader will be delighted to help and to support you. If you don\u0026rsquo;t like it, you don\u0026rsquo;t ever have to do it again.\nWhat is the benefit of speaking? Why should you put yourself through all of that stress and work to put on a wonderful session? I think there are a number of benefits and most won\u0026rsquo;t come to you from doing a single session.\nThe first one does. You will know your subject more thoroughly. By preparing a session to present, you will definitely research, you will read blogs, you will try things out, you will want to make sure that you know all there is to know. This means that you will be improving your knowledge on the subject. Confidence. Speaking in public will give you more confidence. It will improve your communication skills. You will have to be able to convey your message to others Some people say that a job interview is like a presentation and you can improve that skill. (I am not sure but I hate interviews and don\u0026rsquo;t perform well) It will improve your time management skills. You have to do a presentation on Monday, you need to get everything in place in time for that. There are plenty of other reasons too.\nWhat should I do? If someone asks or suggests that you do a presentation or become a mentor then I say that you should give it a try. They believe in you and so do I. New Stars Of Data call for papers are open and available for mentors as well as new speakers.\nIf this blog post makes you think \u0026ldquo;I would like to give this a go\u0026rdquo; at presenting or mentoring, then you can reach out to your local user group. You are a member of your local user group I hope? If not, you can find yours in the Azure Data Community.\nDon\u0026rsquo;t want to speak in person the first time? You can sign up for New Stars Of Data and you will be presenting virtually on the 2023-05-12. They are also taking mentors for the speakers for this event.\nGOOD LUCK\nI believe in you\nYou can do it.\n","date":"2022-11-24T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/newstars1.jpg","permalink":"https://blog.robsewell.com/blog/newstarnovember-getting-into-speaking-it-was-fatherjack-and-sqlbits/","title":"#NewStarNovember - Getting into Speaking - It was fatherjack and SQLBits"},{"content":"There are a number of methods to import PowerShell modules into Azure automation as described in the documentation here\nYou may however miss an important piece of information hidden in that documentation if you are uploading a module from a GitHub release instead of via the PowerShell Gallery. The name that you refer to the module must match the module name and module folder name in the zip file.\nMethod one - from Gallery This is my preferred method for importing modules into Azure Automation accounts, the only bothersome part is remembering to do it twice, once for 5.1 and once for 7.1 as I am sure that if I forget that will be the one module that I will need!\nFind the module Go to the Module page for the automation account and then Add module and browse the gallery and search for dbatools (other modules are available!) and install it\nIt will take a few moments to install but you will see it in the list with a green tick once it has imported.\n#\nThen it is available in all of my PowerShell 7.1 runbooks in my automation account - Here I have just run Get-DbaToolsConfig in a test runbook to prove that the module has imported\nMethod two - using the zip file from a GitHub Release Sometimes you may wish to not use the PowerShell Gallery to import the modules, maybe you have a custom module that you are not ready to upload to the gallery or maybe the module is just internally developed and not available on the PowerShell Gallery. In this scenario, you can still import hte module so that it can be used by your runbooks.\nTo demonstrate, I will remove the dbatools module from the Automation Account\nand download the latest release from GitHub directly\nhttps://github.com/dataplat/dbatools/releases/tag/v1.1.118\nIf you are unable to use the PowerShell Gallery to get the latest dbatools release, I would always use the official signed release.\nYou can then upload the zip from the same Modules page using the Browse for file but here is the important bit You must update the name of the module. By default Azure will set the name to match the name of the zip file as that is what is expected and indeed mentioned in the Microsoft documentation here and once it is imported successfully and I have a green tick\nI can run the test - Again I just ran Get-DbaToolsConfig\nThis method will work with both PowerShell 5.1 and PowerShell 7.1, you will just have to upload the zip (and remember to rename the module entry) twice.\nWhen it goes wrong If you do not rename the module correctly but leave it as the name of file dbatools-signed in this example\n.\nError importing the module dbatools-signed. Import failed with the following error: Orchestrator.Shared.AsyncModuleImport.ModuleImportException: Cannot import the module of name dbatools-signed, as the module structure was invalid. at Orchestrator.Activities.GetModuleMetadataAfterValidationActivity.ExecuteInternal(CodeActivityContext context, Byte[] moduleContent, String moduleName, ModuleLanguage moduleLanguage) at Orchestrator.Activities.GetModuleMetadataAfterValidationActivity.Execute(CodeActivityContext context) at System.Activities.CodeActivity.InternalExecute(ActivityInstance instance, ActivityExecutor executor, BookmarkManager bookmarkManager) at System.Activities.Runtime.ActivityExecutor.ExecuteActivityWorkItem.ExecuteBody(ActivityExecutor executor, BookmarkManager bookmarkManager, Location resultLocation)\nIf you get that, just re-upload the zip file and use the correct name in the form.\nHappy Automating\n","date":"2022-07-28T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/dbatools.jpg","permalink":"https://blog.robsewell.com/blog/how-to-import-dbatools-from-a-zip-file-from-the-github-release-into-azure-automation-modules-without-an-error/","title":"How to import dbatools from a zip file from the GitHub release into Azure Automation Modules without an error"},{"content":"It started with a tweet from Benni De Jagere Blog Twitter about how to show the keystrokes on the screen.\nIt depends The best answer is always \u0026ldquo;it depends\u0026rdquo; and the correct response to this to determine how much to trust this information is \u0026ldquo;what does it depend uponW\u0026rdquo;\nwhat does it depend upon? It depends upon which programme you are using and wish to demonstrate.\nVisual Studio Code or Azure Data Studio If you are using Visual Studio Code or Azure Data Studio then you have it built in.\nYou can press CTRL+SHIFT+P and search for screencast\nif you toggle this on then the keystrokes are displayed on the screen as you type\nIf you CTRL+, or click on the cog and then settings, you can search for screencast and there are a number of options available. In the screenshot below you can see that the mouse click is highlighted as well.\nSomething else If you are wanting to demonstrate in a different application or when switching applications, you can use Carnac - http://carnackeys.com/\nDownload the latest release and then run setup.exe and there will be a beautiful purple icon in the taskbar which if you click it will open the settings screen.\nYou can click in any of the squares to decide where you would like the key presses to be displayed and as you can see it works with multiple monitors (although I had some issues with that). You can use the sliders to offset in any direction from the box as well and change the appearance.\nYou can even change the colour of the text as well.\nTo leave the application simple right click on the icon in the task bar and click Exit.\nHappy demonstrating!\n","date":"2022-07-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/containers.jpg","permalink":"https://blog.robsewell.com/blog/how-do-you-show-keystrokes-on-screen/","title":"How Do You Show Keystrokes On Screen"},{"content":"It won\u0026rsquo;t start! I have a 3 node kubernetes cluster running in my office that I have used for my Azure Arc-enabled data services presentations over the last year (Side note, my presentations are here). A few days ago after a power cut I tried to connect to my cluster with Lens and was not able to.\nI tried to run kubectl get nodes but got no response.\nTry on the master node I used my windows terminal profile that ssh\u0026rsquo;s into the master node and ran\nsystemctl status kubelet\nthis resulted in\nrob@beardlinux:~$ systemctl status kubelet ‚óè kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d ‚îî‚îÄ10-kubeadm.conf Active: active (running) since Thu 2022-07-07 09:29:00 BST; 8min ago Docs: https://kubernetes.io/docs/home/ Main PID: 1201 (kubelet) Tasks: 15 (limit: 38316) Memory: 120.3M CGroup: /system.slice/kubelet.service ‚îî‚îÄ1201 /usr/bin/kubelet \u0026ndash;bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \u0026ndash;kubeconfig=/etc/kub\u0026gt; Jul 07 19:37:47 beardlinux kubelet[1201]: E0707 09:37:47.318044 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:37:47 beardlinux kubelet[1201]: E0707 09:37:47.418240 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found\nHow many logs? So beardlinux is the master node that we are running on so why can it not be found?\njournalctl -u kubelet -n 50\nthat will show me, i thought. It showed\njrob@beardlinux:~$ journalctl -u kubelet -n 50 \u0026ndash; Logs begin at Thu 2022-06-16 14:26:08 BST, end at Thu 2022-07-07 19:38:55 BST. \u0026ndash; Jul 07 19:38:50 beardlinux kubelet[1201]: E0707 19:38:50.710347 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:50 beardlinux kubelet[1201]: E0707 19:38:50.810556 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:50 beardlinux kubelet[1201]: E0707 19:38:50.910804 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.011102 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.111501 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.211840 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.312180 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.412460 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.512751 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.612983 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.713231 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.813398 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:51 beardlinux kubelet[1201]: E0707 19:38:51.913647 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.013891 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.114153 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.214312 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.314439 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.414546 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.514875 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.615009 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.715310 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.815683 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:52 beardlinux kubelet[1201]: E0707 19:38:52.915917 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:53 beardlinux kubelet[1201]: E0707 19:38:53.016190 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found Jul 07 19:38:53 beardlinux kubelet[1201]: E0707 19:38:53.116399 1201 kubelet.go:2243] node \u0026ldquo;beardlinux\u0026rdquo; not found\nAh :-(\nso after some investigation I found\nJul 06 08:03:09 beardlinux kubelet[1021]: I0706 08:03:09.755007 1021 kubelet_node_status.go:71] Attempting to register node beardlinux Jul 06 08:03:09 beardlinux kubelet[1021]: E0706 08:03:09.755338 1021 kubelet_node_status.go:93] Unable to register node \u0026ldquo;beardlinux\u0026rdquo; with API server: Post \u0026ldquo;https://192.168.2.62:6443/api/v1/nodes\u0026rdquo;: dial tcp 192.168.2.62:6443: connect: connection refused\nwhich lead me to an issue on GitHub where there was a comment to check for expired certificates\nDo I have expired certificates? You can check your certificates using\nkubeadm certs check-expiration\nwhich resulted in\nAnd renewing them They are renewed using kubeadm certs renew all\nroot@beardlinux:/home/rob# kubeadm certs renew all [renew] Reading configuration from the cluster\u0026hellip; [renew] FYI: You can look at this config file with \u0026lsquo;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026rsquo; [renew] Error reading configuration from the Cluster. Falling back to default configuration\ncertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed\nDone renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.\nstopped and started the kubelet\nroot@beardlinux:/home/rob# systemctl stop kubelet root@beardlinux:/home/rob# systemctl start kubelet\nand checked the nodes\npwsh 7.2.5\u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION beardlinux Ready control-plane,master 376d v1.20.2 beardlinux2 Ready 376d v1.20.2 beardlinux3 Ready 376d v1.20.2\nI also had to update my config with the new certificate data to make that work as well.\n","date":"2022-07-19T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/containers.jpg","permalink":"https://blog.robsewell.com/blog/kubernetes-lab-certificates-expired/","title":"Kubernetes lab certificates expired"},{"content":"The last post showed how we created an easy process to update a web-page using a GitHub Issue and two GitHub Actions.\nProtecting the repository I opened the repository in the browser and GitHub and was provided with a warning that said\nClicking on the protect this branch button gave the reasoning.\nProtect your most important branches Branch protection rules define whether collaborators can delete or force push to the branch and set requirements for any pushes to the branch, such as passing status checks or a linear commit history.\nSo I changed the settings so that a Pull Request is required and needs to be reviewed.\nBreaks the workflow I had already altered the workflow trigger for the workflow to generate the speaker-list.json so that it would run when changes to the speakers directory were pushed to the main branch by adding\n1 2 3 4 5 6 7 8 on: workflow_call: workflow_dispatch: push: branches: - main paths: - speakers/* I then approved a PR with a change to that directory and saw that the workflow had started.\nThen it failed :-(.\nThe error message could be seen in the codespaces with the extension cschleiden.vscode-github-actions\nThis is the error message\nError: To https://github.com/dataplat/DataSpeakers !\trefs/heads/main:refs/heads/main\t[remote rejected] (protected branch hook declined) Done Pushing to https://github.com/dataplat/DataSpeakers POST git-receive-pack (604 bytes) remote: error: GH006: Protected branch update failed for refs/heads/main. remote: error: At least 1 approving review is required by reviewers with write access. error: failed to push some refs to \u0026lsquo;https://github.com/dataplat/DataSpeakers'\nOf course, because I have now protected my branch, I cannot automatically push changes into the main branch.\nFix it To fix this, I had to create a new PAT token with public_repo scope and save it as a secret for the workflow to access and update the checkout to use this token.\nCreate a new PAT token The instructions to do this are found in the docs here\nIn the upper-right corner of any page, click your profile photo, then click Settings. In the left sidebar, click Developer settings. In the left sidebar, click Personal access tokens. Click Generate new token. Give your token a descriptive name. To give your token an expiration. Select the scopes, or permissions, you\u0026rsquo;d like to grant this token. For this scenario just choose public_repo Click Generate token. Save the generated token somewhere safe like your password manager. ( You do have a password manager? - Our family use 1Password) Save it as a secret in the repository You do not ever ever ever want to store secrets in source control. When using GitHub like this you can store your secrets in the settings of the repository by following this guide\nnavigate to the main page of the repository. Under your repository name, click on the \u0026ldquo;Settings\u0026rdquo; tab. In the left sidebar, click Secrets. On the right bar, click on \u0026ldquo;Add a new secret\u0026rdquo; Type a name for your secret in the \u0026ldquo;Name\u0026rdquo; input box. I used REPO_TOKEN Type the value for your secret. Click Add secret. Use it in your workflow Now that you have saved your secret, you can use it your workflows. To get rid of the protected branch error it is used in the actions/checkout action like this\n1 2 3 4 5 - uses: actions/checkout@v2 with: fetch-depth: 0 ref: main token: ${{ secrets.REPO_TOKEN }} I remembered to do for both workflows!!\nI then created a PR to test it and this time it was able to successfully push changes to the main branch\nand you can see the commit here or the PR if you wish.\nBut thats not all folks This will work correctly for a PR and it will work for the initial workflow that has been called.\nIt will not work for the reusable workflow. When the reusable workflow is called from another workflow it is unable to pick up the token from the secrets. In that scenario we get this error\nInput required and not supplied: token\nfor the actions/checkout@v2 action. This took some tracking down to resolve but finally I found the answer in a forum post\nIn the calling workflow add a secrets entry and pass in the token secret.\n1 2 3 4 5 createSpeakerListJson: needs: addNewSpeaker uses: dataplat/DataSpeakers/.github/workflows/wesbiteFile.yml@main secrets: REPO_TOKEN: ${{ secrets.REPO_TOKEN }} and then at the top of the reusable workflow define the secrets\n1 2 3 4 5 on: workflow_call: secrets: REPO_TOKEN: required: true and finally all is well and Dr Greg Low Blog Twitter can be added ;-)\nHappy Automating!\n","date":"2022-07-15T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/noentry.jpg","permalink":"https://blog.robsewell.com/blog/github-action-workflow-protected-branch-update-failed/","title":"GitHub Action Workflow Protected branch update failed"},{"content":"The last post showed the resource that we created to enable speakers to let events know that they have content for pre-cons/training days. This post will describe how the automation was created using a GitHub Issue and two GitHub Actions.\nWhat do we need? The idea was to have a form for user input that could easily allow a person to add themselves and some information to a web page. The page holds a list of speakers who can present training day sessions for data platform events. The web page can be found here. This page is generated from a JSON file.\nA new repository It was decided to use a GitHub repository to hold this information so that it is available publicly as well as via the website.\nCreate a dev container It\u0026rsquo;s a brand new repository .devcontainer directory was created and the files from the Microsoft VS Code Remote / GitHub Codespaces Container Definitions repository PowerShell containers added. This means that whenever I or anyone else wants to work on the repo the development experience will be the same.\nAdd extensions There are a number of default extensions that I install for PowerShell or generic development\nms-vscode.powershell - because I am working with PowerShell 2gua.rainbow-brackets - because I like to easily see which opening bracket matches which closing bracket oderwat.indent-rainbow - so that I can quickly see the indentations, invaluable with YAML files usernamehw.errorlens - so that linting errors are displayed in the editor alongside the code eamodio.gitlens - to make source control easier TylerLeonhardt.vscode-inline-values-powershell - so that you can see inline values when debugging I also added two more for this repository as we are using GitHub Actions\nme-dutour-mathieu.vscode-github-actions - for intellisense for GitHub Action files cschleiden.vscode-github-action - to be able to start/stop/monitor GitHub Actions from the workspace Gather the Information People can update repositories using Pull Requests but this needed to be a little more guided and it was decided that it was to be done with forms via GitHub Issues\nWhere to put it? You can create custom GitHub Issues using YAML files in the .github/ISSUE_TEMPLATE directory. An Add Speaker issue template file was created. The name and the description will be seen on the new issues page.\n1 2 3 4 5 6 7 8 name: Add Speaker description: Add New Speaker information body: - type: markdown attributes: value: | Please follow the instructions to create a new speaker entry. We wil display this on callfordataspeakers.com There are a number of -type entries. You can find the definitions in the docs or you can use the intellisense from the extensions. The types are checkboxes, dropdown, input, markdown, textarea\nI used the intellisense to build a quick simple form to gather 5 pieces of information\nfull name topics regions sessionize profile URL languages You can find the YAML file here and the issue here\nProcess the information Now that we have a method of gathering the information, the next stage is to process it automagically. For this we are going to be using GitHub Actions\nWorkflow GitHub Actions is a platform that can run automated processes called workflows that are defined as YAML files and triggered by events in the repository. We create another directory called workflows also in the .github directory.\nTriggering the workflow Many people are comfortable with a DevOps process that will build, test and deploy code when a pull request is raised and approved, GitHub Actions are able to do more as they can be triggered by any events in the repository.\nYou can automatically add labels, close stale issues and much more. There are a large number of events open to you as can be seen here . Even looking at just issues there are a number of activities types that can be used\nopened edited deleted transferred pinned unpinned closed reopened assigned unassigned labeled unlabeled locked unlocked milestoned demilestoned (and there are separate ones for issue comments)\nThe beginning of the workflow YAML file has the name and then the trigger. This triggers the workflow when an issue is opened.\n1 2 3 4 5 6 name: Add a new speaker json on: issues: types: - \u0026#34;opened\u0026#34; Getting all the source The workflow consists of one or many jobs that can be run on different runners. The first job is named AddNewSpeaker and runs on the latest ubuntu version. Each job can have a number of steps and the first step in this scenario is to checkout the latest version of the repository.\nWe use a default action to checkout and because we push changes back to the repository (more on that later) we choose a fetch-depth of 0 to get all of the history and the ref main as that is the branch we are working with.\n1 2 3 4 5 6 7 8 jobs: addNewSpeaker: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: fetch-depth: 0 ref: main Being polite costs nothing so this action from Peter Evans can be used to add or update a comment\n1 2 3 4 5 6 7 8 9 - name: Add comment to the issue uses: peter-evans/create-or-update-comment@v2 with: issue-number: ${{ github.event.issue.number }} body: | Hi @${{ github.event.issue.user.login }}, Thank you so much for your Speaker submission. The Action should be running now and adding it to the webpage. It should should update here. If it doesn\u0026#39;t - get in touch with Rob on Twitter https://twitter.com/sqldbawithbeard wait a minute, how did you work that out? The say thank you comment uses github.event.issue.number and github.event.issue.user.login to ensure that the comment goes on the issue that triggered the workflow and thanks the user that created it. To work out what is available, I used this PowerShell step to write out the GitHub context to the logs as JSON\n1 2 3 4 5 6 # You also can print the whole GitHub context to the logs to view more details. - name: View the GitHub context run: Write-Host \u0026#34;$GITHUB_CONTEXT\u0026#34; env: GITHUB_CONTEXT: ${{ toJson(github) }} shell: pwsh Get the info into a file Whilst developing, I first saved the issue body to a file so that I could work with it. As I moved forward I forgot and just left the code in and it works. The issue form creates ### \u0026lt;label\u0026gt; and then a blank line and then the data that was entered. This enabled me to use some regex and capture each label, grab the data and put it in a pscustomobject\nThen I could convert it to Json and save it to a file. I chose to save each speakers information in their own file in case anything else would be needed in the future and also so that if the process failed it only affected this speakers information.\nI also add the speaker file name to a text file that I may make use of at some future point.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 - name: Get Speaker Information to file run: | Write-Host \u0026#34;What do we have?\u0026#34; # gci -recurse = this is for troubleshooting because paths are hard $IssueBody = \u0026#34;${{ github.event.issue.body }}\u0026#34; # Write-Host $IssueBody $IssueBody | Out-File speakers/temp.txt # get the temp file contents - I do this so I don\u0026#39;t lose anything $file = Get-Content ./speakers/temp.txt -Raw # parse the issue $regexResult = [regex]::Matches($file, \u0026#39;(?ms)fullname\\n\\n(?\u0026lt;fullname\u0026gt;.*)\\n\\n### topics\\n\\n(?\u0026lt;topics\u0026gt;.*)\\n\\n### regions\\n\\n(?\u0026lt;regions\u0026gt;.*)\\n\\n### Sessionize\\n\\n(?\u0026lt;Sessionize\u0026gt;.*)\\n\\n### language\\n\\n(?\u0026lt;language\u0026gt;.*)\\n\u0026#39;) # create an object $speakerObject = [PSCustomObject]@{ name = $regexResult[0].Groups[\u0026#39;fullname\u0026#39;].Value topics = $regexResult[0].Groups[\u0026#39;topics\u0026#39;].Value regions = $regexResult[0].Groups[\u0026#39;regions\u0026#39;].Value sessionize = $regexResult[0].Groups[\u0026#39;Sessionize\u0026#39;].Value language = $regexResult[0].Groups[\u0026#39;language\u0026#39;].Value } #save it to a file $speakerFileName = $SpeakerObject.name -replace \u0026#39; \u0026#39;, \u0026#39;-\u0026#39; -replace \u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;/\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\\\\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;:\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\*\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\?\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\u0026#34;\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\|\u0026#39;,\u0026#39;-\u0026#39; $filePath = \u0026#39;./speakers/{0}.json\u0026#39; -f $speakerFileName $SpeakerObject |ConvertTo-Json | Out-FIle -FilePath $filePath $speakerFileName | OUt-File ./speakers/list.txt -Append shell: pwsh Because Ben is a fantastic tester All the best testers will do unexpected but valid actions and my wonderful friend Ben Weissman (Twitter Blog) added some characters into the full name option that made the file save fail. He added his pronouns, which is awesome but not what I expected for a full name option. This is totally my fault for not considering either using pronouns or that as a user input field that is used in code the data should be validated. I used a few replaces to ensure the file name is acceptable.\n1 $speakerFileName = $SpeakerObject.name -replace \u0026#39; \u0026#39;, \u0026#39;-\u0026#39; -replace \u0026#39;\u0026#39;\u0026#39;\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;/\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\\\\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;:\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\*\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\?\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\u0026#34;\u0026#39;,\u0026#39;-\u0026#39; -replace \u0026#39;\\|\u0026#39;,\u0026#39;-\u0026#39; Let the user know and commit the new file Next up is another comment, this time to show some progress but also add a link to the created files directory so that the speaker can see it. They can also edit this file if they wish to make any changes. (yes, maybe I should have thought of a way to do it with issues but this is an iterative process).\nI love the EndBug/add-and-commit action as it enables me to make changes in a workflow and commit those changes safely back to the repository.\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Add another comment to the issue uses: peter-evans/create-or-update-comment@v2 with: issue-number: ${{ github.event.issue.number }} body: | The Speaker Json has been added https://github.com/dataplat/DataSpeakers/tree/main/speakers - name: Add \u0026amp; Commit uses: EndBug/add-and-commit@v8.0.2 with: author_name: Beardy McBeardFace author_email: mrrobsewell@outlook.com message: \u0026#39;The Beard says hooray we have another speaker @${{ github.event.issue.user.login }} - This is an automated message\u0026#39; DRY Don\u0026rsquo;t repeat yourself. The idea is to create the JSON file for the web-page from each of the speakers individual json files. People will want to change what they have entered or they will make mistakes, future functionality might require the same steps. With this in mind I created a separate workflow file to create the speaker-list.json file. This used two different triggers\nworkflow_calls so that it can be called from another workflow workflow_dispatch so that it can be run manually The other workflow cannot be triggered manually as it relies on an issue to create the required file.\n1 2 3 on: workflow_call: workflow_dispatch: Only run if The second workflow file uses a PowerShell action to combine the individual JSONs into a single one and commits that to the repository. It also comments on the issue but it can only do this if the workflow was triggered from the add speaker job and not manually so some conditional logic was required. There were a number of options that I could choose to decide if to run this step but I decided on using the event issue number if: github.event.issue.number != null as if there was no issue, there was nothing to comment and this would leave this step open to be used in future coding if required.\n1 2 3 4 5 6 7 8 - name: Add another comment to the issue uses: peter-evans/create-or-update-comment@v2 if: github.event.issue.number != null with: issue-number: ${{ github.event.issue.number }} body: | The speaker-list.json file has been recreated ready for the website https://github.com/dataplat/DataSpeakers/blob/main/website/speaker-list.json https://callfordataspeakers.com/precon should be updated now Calling another workflow To call another workflow in a job you use the uses: field and the path to the yaml file and the branch. We also added the needs: so that this job will run after the addNewSpeaker has completed.\n1 2 3 createSpeakerListJson: needs: addNewSpeaker uses: dataplat/DataSpeakers/.github/workflows/wesbiteFile.yml@main Close the issue This process needed to be completely automatic and so we use Peter Evans close issue action and tag the speaker and say thankyou as well as closing the issue. We have a needs: property so that this job will only run following the successful run of the previous two jobs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 closeIssue: needs: [addNewSpeaker,createSpeakerListJson] runs-on: ubuntu-latest steps: - name: Close Issue uses: peter-evans/close-issue@v2 with: issue-number: ${{ github.event.issue.number }} comment: | Hey @${{ github.event.issue.user.login }}, Closing this issue now that the Action has run successfully. Thank you so much for adding your information to the list. It will be active on callfordataspeakers.com shortly. Please share on social media. Love and Hugs Rob and Daniel @SqlDbaWithABeard @dhmacher Show me what it looks like You can take a look at the repo there are a number of issues like this one from Monica Rathbun (Twitter - Blog)\nyou can see the workflows running here\nHappy Automating!\n","date":"2022-07-11T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/bench.jpg","permalink":"https://blog.robsewell.com/blog/creating-a-training-day-speakers-list-with-github-action-from-a-github-issue/","title":"Creating A Training Day Speakers List with GitHub Action from a GitHub Issue"},{"content":"How do data platform events find Training Day/Pre-Con speakers?\nSo we built a thing for speakers to add themselves and for events to find them\nI think event organisers know who the big names are and the topics that they can deliver for full day training sessions or pre-cons as they are also known. Finding other speakers and finding speakers who can deliver on different topics is a little more difficult.\nHey New Speakers With all the *waves hand at world for the last 2 years going on, there are a number of new speakers who have taken advantage of virtual events like New Stars Of Data, DatiVerse and other events that have helped to promote and support new speakers. This is truly awesome and I love seeing the pool of speakers growing and all the new voices enriching our learning.\nThere are undoubtedly speakers who have content and can provide full day seesions that events and attendees will gladly have if only the organisers knew about the content and/or the speakers knew about the events.\nEvents want your content This came up on social media and after a quick conversation with Daniel Hutmacher (Twitter Blog) we decided to create a resource page that can be found on Call For Data Speakers.\nCall For Data Speakers enables speakers to sign up to receive an email when a new event is announced. It also enables events to sign up, so that speakers can be notified when there is a call for speakers. So this seemed to be the obvious place to hold a list of speakers that event organisers can contact and show the topics that they can present full day or training day sessions on.\nYES even you. Please join. I have created some automation that will make adding (and removing) yourself from this list easy to do. You can just go straight to the repo and follow the instructions if you dont want to read any more here.\nI see this as a resource for everybody, famouse or not, new or old. I absolutely want you to add yourself, if you have content that can be used to provide a full day of training. Please don\u0026rsquo;t let imposter syndrome get in the way. Right now, all you are doing is listing your idea for people to see. Hopefully soon event organisers will get in touch and say hey I see you present on \u0026hellip; please would you submit to our event for a training day.\nEvent organisers - You do need to reach out to speakers. By adding some effort into finding speakers your event will be more rounded and of interest and benefit to a wider numebr of attendees and sponsors. I am talking about pre-con speakers here bu the same applies to general sessions.\nHow do I do it? This process is all automated and driven by GitHub Issues.\nTo add yourself as a speaker Open the Issues Page and click new issue.\nClick the get started button next to Add Speaker.\nFill in the details, the title can be anything that you like -\nYour full name topics you can provide training days or pre-cons for (dbatools, Index Tuning, DevOps for example) Add as many as you like. Just topics not session titles or descriptions, those will be in your sessionize profile then this resource does not need updating so frequently. It is after all just a \u0026ldquo;Here I am, come find me\u0026rdquo; resource. regions that you would be willing to present training days or pre-cons in (these match the regions on callfordataspeakers.com) Your sessionize profile URl which will show the event organisers the precise sessions that you have and also your contact details/methods Thats it, then press Submit new issue and the automation will do its thing\nWhat does it look like? A GitHub Action will run and the web-page will be updated.\nYou can then search for topics, regions, click on any topic to see all the speakers that are happy to present on that topic.\nClick on a speaker and you will be directed to their Sessionize profile page.\nHere is a quick look at the demo page after I had some test data in there!\nWhat else can I do? Please promote this resource. It will have no benefit if speakers do not add themselves and event organisers do not know about it.\nI would be really happy if you can keep this in mind if you are organising a data platform event, let any speakers know that this exists so that they can add themselves, share it on social media.\nMany Thanks.\n","date":"2022-07-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/magnify.jpg","permalink":"https://blog.robsewell.com/blog/training-day-speakers-list/","title":"Training Day Speakers List"},{"content":"Broken Link It started with a message from Mikey Bronowski ( Blog Twitter )\nNow this means that you get to see my awesome 404 page which makes me laugh every time! It is not a very good look though and does not help people who are reading the blog.\nWhy do something manual when you can automate it This blog is running on GitHub Pages via a repository. Every time a change is pushed to the repo a GitHub Action runs which rebuilds the jekyll site and makes it available.\nSo the easy thing to do is to edit the code to add the corrected link, push the change and have GitHub Pages do its thing. If I wanted to validate it first then I could use docker and containers as discussed in these two blog posts Running GitHub Pages locally or Running GitHub Pages locally with a Remote Theme (this site has a remote theme). Then I could see the changes locally before pushing them to the repository.\nBut my brain didn\u0026rsquo;t work in that way. Instead it thought \u0026ldquo;Hmmm maybe I could do this in the browser in GitHub Codespaces and then it could work locally as it will have a dev container (development container) configuration and VS Code will just open that in Docker itself, no need for running docker commands manually and I can write blog posts anywhere there is a browser or VS Code\u0026rdquo;\nThe most wonderful Jess Pomfret Blog Twitter and I delivered a dbatools Training Day at SQL Bits this year which we developed and ran using dev containers. We also presented a session at the PowerShell Conference Europe about using dev containers so I had a little knowledge of how it can be done.\nHow easy is it ? It\u0026rsquo;s super super easy. Surprisingly easy.\nOpen a codespace for your repository First I went to the repository for my website and opened a codespace by clicking on the green code button and creating a codespace\nAdd the development container configuration Using CTRL SHIFT + P to open the command palette and typing codespaces and choosing the Add Development Container Configuration Files\nand follow the prompts\nShow All Definitions Jekyll bullseye (or buster if you use Apples) lts The config files are created This will create a .devcontainer directory with\ndevcontainer.json Dockerfile post-create.sh Which will do all that you need. You can stop here. You will just need to run jekyll serve to start the website.\nAutomatic regeneration To make it automatically regenerate. I added\nbundle exec jekyll serve --force-polling\nto the end of the post-create.sh file. This will automatically start the website and regenerate it everytime I make a change :-)\nView the logs You can watch the logs of the regeneration with View Creation Log from the command palette - Use CTRL SHIFT + P to open it. Then you can see the log output in real-time.\nOpen the website \u0026ldquo;locally\u0026rdquo; To open the website from inside the devcontainers the ports are exposed via the configuration. In the browser in codepaces there is a port tab and a button to press to open the website and show the updates that you have written.\nIf you click that you get a live view of the website so that you can validate that it works.\nAnd VS Code? This showed it being created in codespaces in the browser, you can have the same effect in VS Code by adding a .devcontainer directory and copying the files from the vs code dev containers repo\nThe rest is pretty much the same except the url!\nRather Have Video ? If you prefer video then you can find one on Youtube showing the same process.\n","date":"2022-07-04T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/containers2.jpg","permalink":"https://blog.robsewell.com/blog/github-pages-in-dev-containers-and-codespaces/","title":"GitHub Pages in Dev Containers and Codespaces"},{"content":"","date":"2022-04-05T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/02/remove-them-all.png","permalink":"https://blog.robsewell.com/blog/quickly-creating-test-users-in-sql-server-using-dbatools/","title":"Quickly Creating Test Users in SQL Server using dbatools"},{"content":"The job name Deploy_Function_App appears more than once This was the error I was notified about in a Azure DevOps pipeline when they tried to run it. The error message continued to say that Job Names must be unique within a pipeline.\nSet Up There is a centralised repository of Azure DevOps Pipeline Template Jobs that call the Bicep modules with the required values in the same repo to deploy Azure Infrastructure.\nThe error was received in the pipeline that was created to make use of these template jobs and deploy a whole projects worth of infrastructure.\nIt looked like this\nWhen I looked at the template job it had\n1 2 3 4 5 6 7 8 9 10 11 12 jobs: - job: Deploy_Function_App ${{ if eq(parameters[\u0026#39;dependsOnLogAnalytics\u0026#39;], true) }}: dependsOn: - Deploy_Resource_Group - Deploy_Log_Analytics ${{ if eq(parameters[\u0026#39;dependsOnLogAnalytics\u0026#39;], false) }}: dependsOn: - Deploy_Resource_Group pool: vmImage: windows-latest So you fixed it? I can see that the job name will always be Deploy_Function_App so I just need to paramatarise it. For this example, I am going to say it was a parameter called suffix, and the code looked like this\n1 2 3 4 5 6 7 8 9 10 11 12 jobs: - job: Deploy_Function_App${{ parameters.suffix }}\u0026#39; ${{ if eq(parameters[\u0026#39;dependsOnLogAnalytics\u0026#39;], true) }}: dependsOn: - Deploy_Resource_Group - Deploy_Log_Analytics ${{ if eq(parameters[\u0026#39;dependsOnLogAnalytics\u0026#39;], false) }}: dependsOn: - Deploy_Resource_Group pool: vmImage: windows-latest A quick Pull Request, which was approved and then pushed and I said \u0026ldquo;Hey, all fixed, try again\u0026rdquo;. This is the response I got - It failed again\nJob Deploy_Function_App_speechtotext\u0026rsquo; has an invalid name. Valid names may only contain alphanumeric characters and \u0026lsquo;_\u0026rsquo; and may not start with a number.\nI had to look at it for a few minutes before I spotted the error! The job name sure looks like it only has alphanumeric characters and my YAML is perfectly valid so the string must be properly quoted. I mean it must be properly quoted otherwise it would fail right?\nWrong\n1 - job: Deploy_Function_App${{ parameters.suffix }}\u0026#39; There is only one single quote here which we did not notice!\nAltering it to this worked.\n1 - job: \u0026#39;Deploy_Function_App${{ parameters.suffix }}\u0026#39; Hopefully that might help someone. (No doubt I will find this in a search in a few months time when I do it again!!)\nHappy automating\n","date":"2022-01-28T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/Bicep/xavier-von-erlach-ooR1jY2yFr4-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/azure-devops-pipeline-template-job-names-and-single-quotes/","title":"Azure DevOps Pipeline Template Job Names and single quotes"},{"content":"Can't find loc string for key: JS_InvalidFilePath This was the error I received in my Azure DevOps pipeline when I tried to run it.\nWhen I investigated further it said\n1 2 3 4 5 6 7 8 ##[debug]workingDirectory=/home/vsts/work/1/s ##[debug]check path : /home/vsts/work/1/s ##[warning]Can\\\u0026#39;t find loc string for key: JS_InvalidFilePath ##[debug]Processed: ##vso[task.issue type=warning;]Can\\\u0026#39;t find loc string for key: JS_InvalidFilePath ##[debug]task result: Failed ##[error]JS_InvalidFilePath /home/vsts/work/1/s ##[debug]Processed: ##vso[task.issue type=error;]JS_InvalidFilePath /home/vsts/work/1/s ##[debug]Processed: ##vso[task.complete result=Failed;]JS_InvalidFilePath /home/vsts/work/1/s What is going on? I was trying to run a simple Azure PowerShell task and had defined it like this (I used VS Code with the Azure Pipelines extension and made use of the intellisense). I had defined it like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - task: AzurePowerShell@5 name: deploy displayName: Deploy from cache inputs: azureSubscription: \u0026#39;azurePAYGconnection\u0026#39; Inline: | $date = Get-Date -Format yyyyMMddHHmmsss $deploymentname = \u0026#39;deploy_testRg_{0}\u0026#39; -f $date # name of the deployment seen in the activity log $TemplateFile = \u0026#39;BicepFiles\\Deployments\\TheTestResourceGroup.bicep\u0026#39; New-AzDeployment -Name $deploymentname -Location uksouth -TemplateFile $TemplateFile -Verbose # -WhatIf azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; env: SYSTEM_ACCESSTOKEN: $(system.accesstoken) pwsh: true enabled: true which gave me no errors, the YAML is correct (yes, I was suprised too!). The Azure Pipeline definition does not raise an error either in VS Code or in Azure DevOps.\nWhat was missing? I had not put ScriptType: 'InlineScript' and this is what caused that odd error.\nThe correct definition was\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - task: AzurePowerShell@5 name: deploy displayName: Deploy from cache inputs: azureSubscription: \u0026#39;azurePAYGconnection\u0026#39; ScriptType: \u0026#39;InlineScript\u0026#39; Inline: | $date = Get-Date -Format yyyyMMddHHmmsss $deploymentname = \u0026#39;deploy_testRg_{0}\u0026#39; -f $date # name of the deployment seen in the activity log $TemplateFile = \u0026#39;BicepFiles\\Deployments\\TheTestResourceGroup.bicep\u0026#39; New-AzDeployment -Name $deploymentname -Location uksouth -TemplateFile $TemplateFile -Verbose # -WhatIf azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; env: SYSTEM_ACCESSTOKEN: $(system.accesstoken) pwsh: true enabled: true Hopefully that might help someone. (No doubt I will find this in a search in a few months time when I do it again!!)\nHappy automating\n","date":"2021-11-04T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/Bicep/xavier-von-erlach-ooR1jY2yFr4-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/what-does-js_invalidfilepath-error-mean-in-azure-devops/","title":"What does JS_InvalidFilePath error mean in Azure DevOps? "},{"content":"Using a private module repository From Bicep version 0.4.1008 you can save and version your Bicep modules in repositories. You can read more about how to do it here. This is really useful for reusing modules and modularising large corporate infrastructure environments.\nYou can control how a single resource (think of a storage account) is deployed across your environment and ensure that all requirements are followed (the storage account must have public access disabled, must have private endpoints and must have the one production network allowed). This is really useful and since it has been available we have used this to deploy infrastructure.\nSo whats the problem ? When you need to use a module from the repository, you refer to the repository when you define the module path.\nmodule storage 'br:bearddemoacr.azurecr.io/bicep/storage/storagev2:0.0.2' = {\nThis says I want to deploy something we will call storage and you can find the definition called bicep/storage/storagev2 in a Bicep Repository (br) at bearddemoacr.azurecr.io and we will use the tag 0.0.2. The rest of the properties will then be written.\nOn the client that you use to do the deployment, Bicep will restore the required information from the Bicep Repository and use that to perform the deployments. By default, it uses the path ~/.bicep on Linux/Mac and $HOME\\.bicep on Windows.\nIf you take a look in that directory, you will see the files that have been restored for use.\n![cachecontents]({{ \u0026ldquo;/assets/uploads/2021/Bicep/cachecontents.png\u0026rdquo; | relative_url }})\nBut this relies on the client that is performing the deployment having connectivity and being able to authenticate to the Azure Container Registry (ACR) that is holding the Bicep Modules.\nWhy would the client not have access? There are a number of situations where the deployment client (a workstation, a devops pipeline agent) may not have access to the ACR. The development and testing of the Bicep Modules may take place in a development Azure subscription which has no connectivity to the production Azure subscription. The production environment may be in Azure Government Cloud or it may be in a customers Azure subscription and opening a connection to an ACR in another subscription in another network may be prohibitively complicated and time consuming due to the process required to gain approvals and perform the actions to open the required paths or (more likely) is simply not allowed.\nBICEP_CACHE_DIRECTORY environment variable to the rescue There is an environment variable called BICEP_CACHE_DIRECTORY that defines the path that is used to hold the restored bicep artifacts. This means that we can do two things to enable us to continue to use a Bicep Repository with all of the benefits but still be able to deploy the infrastructure.\nCache the files Firstly, as part of the build process we can set the BICEP_CACHE_DIRECTORY path and perform a bicep restore on the Bicep Resource file. This will restore all of the referenced modules to the path. We can then package this directory with our deployment bicep file and transfer them to the deployment environment.\nDeploy the Bicep Then when we extract the package we can set the BICEP_CACHE_DIRECTORY to the directory holding the cached files and deploy our bicep as we would normally. Even though the files reference the Bicep Repository by name, the deployment will use the cache. I even tested it by deleting the images from the Bicep Repository completely (after I had run bicep restore of course) and I was able to deploy from the cache without issue.\nHopefully, this wil help someone somewhere as the BICEP_CACHE_DIRECTORY variable is not wildly known or documented.\nHappy automating\n","date":"2021-11-02T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/Bicep/xavier-von-erlach-ooR1jY2yFr4-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/deploying-a-bicep-module-from-a-private-repository-without-a-connection-to-the-repository/","title":"Deploying a Bicep Module from a private repository without a connection to the repository"},{"content":"I REALLY needed to see the values The problem was that I had code in an Azure DevOps PowerShell task which was using a Service Principal to do some things in Azure and it was failing.\nThe pipeline had some things a little like this, it got a number of values from a key vault, set them to variables and used them in a custom function\n1 2 3 4 5 $somevalue = (Get-AzKeyVaultSecret -vaultName $KeyVaultName -name \u0026#39;AGeneratedName\u0026#39;).SecretValue $somecredential = New-Object System.Management.Automation.PSCredential (\u0026#39;dummy\u0026#39;, $somevalue ) $something = $somecredential.GetNetworkCredential().Password Do-SomethingSpecial -MyThing $something I was getting an error saying \u0026ldquo;forbidden - *** does not have access\u0026rdquo; or similar\nThing is, I knew that $something did have access as I could run the same code from my workstation and it did the logging in for $something so the error must be in the values that I was passing into the function. (there were more values than this but that is not important)\nAll I needed to do was to see what values had been passed to the function and I could resolve this little issue. But these were secret variables. Helpfully kept out of the logs by Azure DevOps hence the *** so what to do?\nI thought - I know what I will do, I will write the Parameter values from the function out as Verbose, call the function with -Verbose and then delete the run to clear up the logs.\nI added\nWrite-Verbose ($PSBoundParameters | Out-String)\nto my function, called it with verbose in the pipeline and got\nName‚ÄÉValue\n-‚ÄÉ-‚ÄÉ-‚ÄÉ-\nMyThing‚ÄÉ***\nAwesome.\nWrite it to a file and read it back. This is a tactic that you can read about that works but it puts the secrets on disk on the agent and I did not want to do that.\nI thought I would be even cleverer and this time I added to my function\n1 2 $WhatsMyThing = $MyThing + \u0026#39;-1\u0026#39; Write-Verbose \u0026#34;My thing is $WhatsMyThing\u0026#34; Thats bound to work.\nMy how I laughed when in the logs I had\nMy Thing is‚ÄÉ***-1\nRight. I thought.\nThis IS IT.\nI WILL SHOW YOU AZURE DEVOPS\nI added to my function\n1 2 $WhatsMyThing =[Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($$MyThing )) Write-Verbose \u0026#34;My thing is $WhatsMyThing\u0026#34; This converted the value of MyThing into a base64 encoded value which I could see in the logs.\nMy Thing is VGhlIEJlYXJkIGlzIExhdWdoaW5nIGF0IHlvdS4gWW91IHRoaW5rIEkgd291bGQgcHV0IHNvbWV0aGluZyByZWFsIGluIGhlcmU/IEdvb2QgdHJ5Lg==\nand then I could decode it on my workstation with\n[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String('ValueFromLogs'))\nand learn that despite two people looking at the values we couldnt tell the difference between AGeneratedName and AnotherGeneratedName and they were the wrong way around!!!!\nBut at least I know now a good way to get those secret values.\nIf you do this, dont forget to delete the pipeline run from Azure DevOps so that the encoded value is not left in the logs for anyone to read.\nEvery day is a learning day.\n","date":"2021-08-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/michael-dziedzic-1bjsASjhfkE-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/when-you-really-want-to-see-your-azure-devops-secret-variable-values/","title":"When you REALLY want to see your Azure DevOps Secret Variable Values"},{"content":"Want to play before GA ? Azure SQL enabled by Azure Arc will be generally available at the end of the month following the announcement here\nYou can read more about Azure Arc-enabled Data Services I have been playing with it for a few months, mainly in a Kubernetes cluster running on my NUCs in my office but Azure Arc is available in so many places, all the public clouds, your own data center (or NUCs in your office :-) ) so if you want to try it out and you do not want to build your own Kubernetes cluster then you can use AKS in Azure.\nHow can I do that ? One way is to use the Azure Arc Jumpstart website which has many templates for many scenarios.\nI like playing with Bicep which is a domain-specific language or DSL for deploying Azure resources.\nI have created a repository on GitHub which you can use to create your own AKS cluster with an Azure Arc Enabled directly connected Data Controller and SQL Managed Instance either 1 node replica or 3 node replica.\nThere is even the code to create an Azure Virtual Machine and install the required tooling if you need it.\nAll of the details and instructions are in the read me file so feel free to go and make use of it and you can have a resource group that looks like this\nJust dont forget to delete the Resource Group once you have finished!!\nYou can create it any time you like with the code :-)\nHappy Azure Arc SQL Managed Instance playing!\n","date":"2021-07-03T00:00:00Z","image":"https://raw.githubusercontent.com/SQLDBAWithABeard/Beard-Aks-AEDS/main/images/connecteddc.png","permalink":"https://blog.robsewell.com/blog/how-to-deploy-an-azure-arc-enabled-sql-managed-instance-in-aks/","title":"How to deploy an Azure Arc Enabled SQL Managed Instance in AKS"},{"content":"Reusable code We looked at a simple deployment of an Azure SQL Server and a database in the last blog post. You would like to reuse this code though, you will want to create more SQL Instances and SQL databases in the future. With Bicep, you can use modules and parameters to do this.\nYou can create a module for your SQL Instance. I look up the resource information from the documentation and create a file named SQLInstance.bicep. I put it in a Resources directory.\nParameters At the top of the file you need to define parameters to enabled you to pass in different values for the deployment. You can find information about Bicep parameters in the docs on GitHub.\nYou define a parameter using the keyword param. At a minimum you need a name and a datatype. An obvious one for this usecase would be the name of the SQL Instance which could be defined as\n1 param SqlInstanceName string Perhaps your organisation has a requirement for all of the data to be stored in a particular region. You might want to have a default value for your location parameter. You can define a default parameter by assigning it with an equals sign.\n1 param location string = \u0026#39;northeurope\u0026#39; Some parameters that you would like to use will only allow certain values. You can define those as follows\n1 2 3 4 5 @allowed([ \u0026#39;Enabled\u0026#39; \u0026#39;Disabled\u0026#39; ]) param transparentDataEncryption string = \u0026#39;Enabled\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 targetScope = \u0026#39;resourceGroup\u0026#39; param SqlInstanceName string param location string = \u0026#39;northeurope\u0026#39; param tags object param administratorLogin string param administratorLoginPassword string param minimalTlsVersion string = \u0026#39;1.0\u0026#39; // 1.0,1.1,1.2 param publicNetworkAccess string = \u0026#39;Disabled\u0026#39; // \u0026#39;Disabled\u0026#39;,\u0026#39;Enabled\u0026#39; param ActiveDirectoryAdminUser string param ActiveDirectoryAdminUserSid string param tenantid string param azureADOnlyAuthentication bool = false param ExternalAdministratorPrincipalType string // User Application Group param sqlauditActionsAndGroups array //BATCH_COMPLETED_GROUP,,SUCCESSFUL_DATABASE_AUTHENTICATION_GROUP,FAILED_DATABASE_AUTHENTICATION_GROUP maybe some of these too but the logs will get large,APPLICATION_ROLE_CHANGE_PASSWORD_GROUP,BACKUP_RESTORE_GROUP,DATABASE_LOGOUT_GROUP,DATABASE_OBJECT_CHANGE_GROUP,DATABASE_OBJECT_OWNERSHIP_CHANGE_GROUP,DATABASE_OBJECT_PERMISSION_CHANGE_GROUP,DATABASE_OPERATION_GROUP,DATABASE_PERMISSION_CHANGE_GROUP,DATABASE_PRINCIPAL_CHANGE_GROUP,DATABASE_PRINCIPAL_IMPERSONATION_GROUP,DATABASE_ROLE_MEMBER_CHANGE_GROUP,FAILED_DATABASE_AUTHENTICATION_GROUP,SCHEMA_OBJECT_ACCESS_GROUP,SCHEMA_OBJECT_CHANGE_GROUP,SCHEMA_OBJECT_OWNERSHIP_CHANGE_GROUP,SCHEMA_OBJECT_PERMISSION_CHANGE_GROUP,SUCCESSFUL_DATABASE_AUTHENTICATION_GROUP,USER_CHANGE_PASSWORD_GROUP,BATCH_STARTED_GROUP,BATCH_COMPLETED_GROUP param SqldatabaseNames array param dbSkuName string // for example GP_Gen5_2, BC_Gen5_10, HS_Gen5_8, P5, S0 etc param collation string = \u0026#39;SQL_Latin1_General_CP1_CI_AS\u0026#39; // param maxSizeBytes int // The max size of the database expressed in bytes. param zoneRedundant bool = false // Whether or not this database is zone redundant, which means the replicas of this database will be spread across multiple availability zones. param licenseType string = \u0026#39;LicenseIncluded\u0026#39; //\tThe license type to apply for this database. LicenseIncluded if you need a license, or BasePrice if you have a license and are eligible for the Azure Hybrid Benefit. - LicenseIncluded or BasePrice resource sql \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; = { name: SqlInstanceName location: location tags: tags identity: { type: \u0026#39;SystemAssigned\u0026#39; } properties: { administratorLogin: administratorLogin administratorLoginPassword: administratorLoginPassword version: \u0026#39;12.0\u0026#39; minimalTlsVersion: minimalTlsVersion publicNetworkAccess: publicNetworkAccess administrators: { administratorType: \u0026#39;ActiveDirectory\u0026#39; login: ActiveDirectoryAdminUser sid: ActiveDirectoryAdminUserSid tenantId: tenantid azureADOnlyAuthentication: azureADOnlyAuthentication principalType: ExternalAdministratorPrincipalType } } } // SQL Databases resource symbolicname \u0026#39;Microsoft.Sql/servers/databases@2020-11-01-preview\u0026#39; = [for item in SqldatabaseNames:{ parent: sql name: \u0026#39;${item}\u0026#39; location: location tags: tags sku: { name: dbSkuName } properties: { collation: collation maxSizeBytes: maxSizeBytes zoneRedundant: zoneRedundant licenseType: licenseType } }] ","date":"2021-05-21T00:00:00Z","image":"https://datasaturdays.com/assets/design/twitter/c.twitter%201r.png","permalink":"https://blog.robsewell.com/blog/flexing-my-bicep-reusable-code-with-modules-for-deploying-an-azure-sql-server/","title":"Flexing My Bicep - Reusable code with modules for deploying an Azure SQL Server"},{"content":"An Error Did I tear my bicep? No but I got an error. Whilst trying to deploy a network with Bicep using Azure DevOps I received the error\nError: Code=InvalidTemplateDeployment; Message=The template deployment \u0026lsquo;deploy_bicep003_20210505094331\u0026rsquo; is not valid according to the validation procedure. The tracking id is \u0026lsquo;4bdec1fe-915d-4735-a1c1-7b56fbba0dc2\u0026rsquo;. See inner errors for details.\nUnfortunately that was all that I had. I had to find the inner error for details\nTry the deployment log on the Resource Group As I know that the Bicep deployments are logged in Azure under the Resource Groups deployment I looked there first but there were no entries (obviously Rob, there had been no deployment)\nSo I navigated to the home page of the Azure Portal and searched for Activity log.\nI searched for the name of the deployment deploy_bicep003_20210505094331 and saw\nclicking on the link showed me this with the relevant information hidden in the JSON\nResource name {\u0026rsquo;name\u0026rsquo;:\u0026lsquo;subnet1\u0026rsquo;,\u0026lsquo;addressPrefix\u0026rsquo;:\u0026lsquo;10.0.0.0/24\u0026rsquo;}.name is invalid.\nBingo.\nI had made a mistake in my resource definition for the subnets. I had used\n1 2 3 4 5 6 subnets: [for item in subnets:{ name: \u0026#39;${item}.name\u0026#39; properties: { addressPrefix: \u0026#39;${item}.addressPrefix\u0026#39; } }] where I should have used\n1 2 3 4 5 6 subnets: [for item in subnets:{ name: \u0026#39;${item.name}\u0026#39; properties: { addressPrefix: \u0026#39;${item.addressPrefix}\u0026#39; } }] Every day is a learning day.\n","date":"2021-05-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/Bicep/xavier-von-erlach-ooR1jY2yFr4-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/invalid-template-deployment-with-my-bicep/","title":"Invalid Template Deployment with my Bicep"},{"content":"Starting working out? It is important to keep a healthy body and mind, especially when my life is so sedentary these days. Getting exercise is good for both. This blog post has nothing to do with exercise though (apart from maybe exercising the mind)\nProject Bicep Bicep is a language for declaring and deploying Azure Resources. Like Terraform it enables you to define your infrastructure as code.\nWHy use Bicep instead? I really like being able to control infrastructure with code. I have used Terraform to deploy infrastructure and almost exclusively on Azure. I have created and altered many environments for clients over the past couple of years using Terraform. I have also used ARM templates but found them confusing and unwieldly to use.\nExisting State Terraform will deploy the required changes to your infrastructure by comparing the existing state which is stored in a state file with the expected state which is created by running the plan command. If someone alters the Azure resource via the portal, Azure CLI or Azure PowerShell all kinds of mayhem can occur normally failure in deployment and time spent troubleshooting. It is possible to use the import command in Terraform to get the existing resource state into the state file so that the comparison is performed against the existing state of the resource but this requires a lot of manual intervention.\nBicep deploys the changes by comparing the existing state of the Azure resources with the expected state in the code. This, for me, is a super benefit and reduces the complications of those type of errors.\nLatest API support Terraform resources have a lag between features or properties from Azure being made available and those features or properties being incorporated into the Terraform resource. This has lead to me requiring my deployments to have additional Azure CLI, Azure PowerShell or worse both steps to achieve what I need.\nBicep immediately supports all preview and GA versions for Azure Services, I don\u0026rsquo;t have to wait and all the things I can do are available to me.\nAuthoring I love Visual Studio Code and there is a super extension that makes authoring a joy.\nWhat If Support I have written before about the importance of WhatIf for PowerShell functions and how to implement it and Bicep has What If for deployments so that you can validate that the code you have written will perform the tasks that you expect.\nDeployments recorded in Azure The changes that I make with Bicep are recorded in Azure and I can find them in the deployments for the Resource Group\nCost Bicep is free :-)\nDeploy an Azure SQL Database Rob OK, let\u0026rsquo;s see an example. I would like to deploy an Azure SQL Database into a Resource Group. I will need an Azure SQL Server resource and an Azure SQL Database resource. The Azure Templates site has the examples that I need. The Azure SQL Server page shows the Bicep code I need and the explanations of the expected values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 resource symbolicname \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; = { name: \u0026#39;string\u0026#39; location: \u0026#39;string\u0026#39; tags: {} identity: { type: \u0026#39;string\u0026#39; } properties: { administratorLogin: \u0026#39;string\u0026#39; administratorLoginPassword: \u0026#39;string\u0026#39; version: \u0026#39;string\u0026#39; minimalTlsVersion: \u0026#39;string\u0026#39; publicNetworkAccess: \u0026#39;string\u0026#39; encryptionIdentityId: \u0026#39;string\u0026#39; keyId: \u0026#39;string\u0026#39; administrators: { administratorType: \u0026#39;ActiveDirectory\u0026#39; principalType: \u0026#39;string\u0026#39; login: \u0026#39;string\u0026#39; sid: \u0026#39;string\u0026#39; tenantId: \u0026#39;string\u0026#39; azureADOnlyAuthentication: bool } } } I create a file with a .bicep extension in VS Code\nand add only the required values. (NOTE - this is just an example and I would never recommend that you would put the password for anything in a file in plain text, we will cover how to handle secrets later. )\n1 2 3 4 5 6 7 8 9 resource sql \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; = { name: \u0026#39;beardsqlrand01\u0026#39; location: \u0026#39;northeurope\u0026#39; properties: { administratorLogin: \u0026#39;sysadmin\u0026#39; administratorLoginPassword: \u0026#39;dbatools.IO\u0026#39; // DON\u0026#39;T DO THIS - EVER version: \u0026#39;12.0\u0026#39; } } Validate the deployment with WhatIf I created an empty Resource Group for my test\n1 New-AzResourceGroup -Name \u0026#39;BicepTest\u0026#39; -Location \u0026#39;northeurope\u0026#39; Next, I am going to check that the code that I have written will perform the actions that I expect. I am hoping to get\nAn Azure SQL Instance called beardsqlrand01 In the location North Europe With an admin login and password as stated in the file (NO Don\u0026rsquo;t ever do this in Production) I do this using the Azure PowerShell command New-AzResourceGroupDeployment and give it the Resource Group Name and the path to the file\n1 2 3 4 5 6 7 # Validate the deployment with Whatif $DeploymentConfig = @{ ResourceGroupName = \u0026#39;BicepTest\u0026#39; TemplateFile = \u0026#39;.\\SimpleSqlDatabase\\SqlInstance.bicep\u0026#39; WhatIf = $true } New-AzResourceGroupDeployment @DeploymentConfig The first thing this does is check the status of the resources in the resource group\nthen it provides a list of what it will do. In this example there is only one resource.\nThis tells us that there will be a creation of 1 resource and that the values are as I expect them. As I am happy with that I can then deploy the infrastructure by changing the WhatIf value to false\n1 2 3 4 5 6 7 # Deploy the changes $DeploymentConfig = @{ ResourceGroupName = \u0026#39;BicepTest\u0026#39; TemplateFile = \u0026#39;.\\SimpleSqlDatabase\\SqlInstance.bicep\u0026#39; WhatIf = $false } New-AzResourceGroupDeployment @DeploymentConfig Deployment can be seen in the Azure Portal If I look in the Azure Portal, I can see the deployment is happening.\nOnce it has finished I get an output on the screen\nand when I look in the portal at the deployment\nand my resource has been created\nAdd a database I have my Azure SQL Instance, next I need a database. I look up the resource information and add the required information to my bicep file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 resource sql \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; = { name: \u0026#39;beardsqlrand01\u0026#39; location: \u0026#39;northeurope\u0026#39; properties: { administratorLogin: \u0026#39;sysadmin\u0026#39; administratorLoginPassword: \u0026#39;dbatools.IO\u0026#39; // DON\u0026#39;T DO THIS - EVER version: \u0026#39;12.0\u0026#39; publicNetworkAccess: \u0026#39;Disabled\u0026#39; } resource bearddatabase \u0026#39;databases@2020-11-01-preview\u0026#39; = { name: \u0026#39;BicepDatabase\u0026#39; location: \u0026#39;northeurope\u0026#39; sku: { name: \u0026#39;Basic\u0026#39; } properties: {} } } This is a super simple example. The database resource is defined within the SQL Instance resource with a name and a SKU.\nWe validate it in exactly the same way as before. This time we will see that we can incrementally add or change resources to our deployment and validate what will happen.\n1 2 3 4 5 6 7 # Validate the deployment with Whatif $DeploymentConfig = @{ ResourceGroupName = \u0026#39;BicepTest\u0026#39; TemplateFile = \u0026#39;.\\SimpleSqlDatabase\\SqlInstance.bicep\u0026#39; WhatIf = $true } New-AzResourceGroupDeployment @DeploymentConfig This time the result looks a little different as we already have a resource in the Resource Group.\nAt the top it gives you three types of changes\nCreate NoChange Ignore It shows at the bottom that the changes are\nResource changes: 1 to create, 1 no change, 1 to ignore.\nThis tells you that it will create the Azure SQL Database, it will not change the Azure SQL Server and there is no change to the master database.\nI am happy with that validation, so I deploy the changes, again using the same code as before.\n1 2 3 4 5 6 7 # Deploy the changes $DeploymentConfig = @{ ResourceGroupName = \u0026#39;BicepTest\u0026#39; TemplateFile = \u0026#39;.\\SimpleSqlDatabase\\SqlInstance.bicep\u0026#39; WhatIf = $false } New-AzResourceGroupDeployment @DeploymentConfig If I look in the portal I can see the deployment\nand once it has completed I can see the database in the Portal\nThats all there is to Bicep.\nFind the resource information in the docs Define your deployment in code Validate your deployment with WhatIf Deploy your changes Remove the Resource Group Now that my test has finished I will remove the Resource Group. If you are following along, this is how to do that\n1 Remove-AzResourceGroup -Name BicepTest -Force All of the code I have added all of the code for this blog post to my GitHub here https://github.com/SQLDBAWithABeard/BeardBicep/tree/main/SimpleSqlDatabase so that you can follow along.\nNext steps Now that you have an introduction to Bicep and can see how useful and powerful it can be, we will expand on this in the following blog posts.\n","date":"2021-05-20T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/Bicep/xavier-von-erlach-ooR1jY2yFr4-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/flexing-my-bicep-deploy-an-azure-sql-database-intro-to-azure-bicep-iac/","title":"Flexing My Bicep - Deploy an Azure SQL Database -Intro to Azure Bicep IaC"},{"content":"A different method for my own site This blog post is for Mikey Bronowski t - b and Jonathan Allen t - b after a twitter discussion a few weeks ago.\nHow can I see my GitHub Pages site locally when I use a remote theme?\nDo you need to? My first answer is do you need to see them? Once you have your theme set up as you like, you can view your blog in Visual Studio Code using the keyboard shortcut CTRL + K, V and you can see a live preview of your post as you type.\nHowever, I appreciate that at some point you will probably want to see what your site looks like locally, so I decided to look at the blog posts in the theme locally for this blog. My last post showed how I do this with the Data Saturdays web-site but I get an error when running this for my site because it cant find the gem sources. This is because I am using a remote theme for my blog.\nWhat I could do is work out how to get these in the right place, but I am lazy! Whilst researching for the Data Saturdays site, I had found another docker container, the official Jekyll one https://hub.docker.com/r/jekyll/jekyll. I wondered if I could use that.\nWhich version to use? First we need to know which version of Jekyll GitHub Pages is using. You can find all of that information here https://pages.github.com/versions/\nSo we need to use 3.9.0\nso I ran\ndocker pull jekyll/jekyll:3.9\nbut\nso I tried 3.8.6 and it worked for me.\ndocker pull jekyll/jekyll:3.8.6\nSet up Let\u0026rsquo;s back up a bit and set the environment up. I am using Docker on Windows Subsystem for Linux 2 (WSL2) I installed it using this guide. I believe this will work using native Docker, you would just need to replace the $PWD in the example below with a dot .\nOnce that is installed and the image is pulled, I can then run my blog locally using\ndocker run --rm --volume=$PWD:/srv/jekyll -p 4001:4000 jekyll/jekyll:3.8 jekyll serve\nor if not using WSL2\ndocker run --rm --volume=.:/srv/jekyll -p 4001:4000 jekyll/jekyll:3.8 jekyll serve\nThe --rm means that the container will be removed when it is stopped, --volume=\u0026quot;$PWD:/srv/jekyll\u0026quot; maps the current directory locally to the /srv/jekyll directory in the container so I need to change the directory to my local repository for my blog. -p 4001:4000 says map port 4001 on my machine to port 4000 on the container. This means that I can view the blog locally at https://localhost:4001. jekyll serve will build the site and run it for me.\nOf course, there is tweaking We have to make a few changes to make this work easily. When I run the site locally with this command I get the following error and the site would not build.\nLiquid Exception: No repo name found. Specify using PAGES_REPO_NWO environment variables, \u0026lsquo;repository\u0026rsquo; in your configuration, or set up an \u0026lsquo;origin\u0026rsquo; git remote pointing to your github.com repository. in /_layouts/default.html\nERROR: YOUR SITE COULD NOT BE BUILT:\nNo repo name found. Specify using PAGES_REPO_NWO environment variables, \u0026lsquo;repository\u0026rsquo; in your configuration, or set up an \u0026lsquo;origin\u0026rsquo; git remote pointing to yocom repository.\nto fix this add the following to your _config.yml file\nrepository: GITHUBUSERNAME/REPONAME\nmine is\nrepository: SQLDBAWithABeard/robsewell\nThen when I run the container I get another warning\nGitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.\nThis does not really matter as the site still builds but another warning\nAuto-regeneration may not work on some Windows versions. Please see: https://github.com/Microsoft/BashOnWindows/issues/216 If it does not work, please upgrade Bash on Windows or run Jekyll with \u0026ndash;no-watch.\nmeans that the site will not auto-regenerate when you make a change and save the file.\nWe fix these errors by adding\ngithub: [metadata]\nto the _config.yml file\nand running the container with an extra switch for the jekyll command --force_polling\nSo now it works? So with the additional data in the _config.yml file and the new command\ndocker run --rm --volume=\u0026quot;$PWD:/srv/jekyll\u0026quot; -p 4001:4000 jekyll/jekyll:3.8 jekyll serve --force_polling\nthe site will build. You will still get the warning for auto-regeneration but it works. The purple arrow and the yellow box show the file that was changed and that it regenerated.\nIt will only regenerate whilst running for blog post changes and not for configuration changes, such as altering the _config.yml file. If you want to see those, you will have to stop the container and re-run it.\nThere is one last problem however. When you write your blog posts in Jekyll you name the file YYYY-MM-DD-Nameoffile.md this will give the post time of YYYY-MM-DD but the file for this blog post is named with a date in the future and by default it doesn\u0026rsquo;t show. The green box shows the file name but there is no corresponding blog post.\nTo fix this we add another entry to the _config.yml file\nfuture: true\nThis will tell Jekyll to show the posts with a data in the future. Unless you wish to show future posts on your blog when it is live, you will have to remember to change this to\nfuture: false\nwhen you push your changes to GitHub so that your blog behaves as expected but now you can see your current blog post and write away and be able to see how it will look in your theme\nLet\u0026rsquo;s make it even better When you run the container, it will need to download all of the things it needs to run the site. This can take a little time.\nIt would be better if we had our own image that had all of those already downloaded for us. Let\u0026rsquo;s create our own image. We need to run our container without the rm option this time as we need it not to be removed when we stop it.\ndocker run -volume=\u0026quot;$PWD:/srv/jekyll\u0026quot; -p 4001:4000 jekyll/jekyll:3.8 jekyll serve\nOnce it has finished downloading and installing all that it needs and generated the site press CTRL +C to stop the container and run\ndocker ps -a\nwhich will show you all of containers.\nUse the first 3 characters of the container. In my example it is 760. If you have more than one, look for the one with the jekyll/jekyll:3.8.6 as the image.\nThen we can create our own image using\ndocker commit 760 myblogimage\nreplace 760 with your own container.\nOnce you have created the image, you can remove the stopped container with\ndocker rm 760\nAgain, replace 760 with your own container.\nQuicker run Now you can use your own image and the container will not need to download and install all of the things. Replace jekyll/jekyll:3.8 with myblogimage\ndocker run --rm --volume=\u0026quot;$PWD:/srv/jekyll\u0026quot; -p 4001:4000 myblogimage jekyll serve --force_polling\nHappy local blog writing.\n","date":"2021-04-15T00:00:00Z","image":"https://datasaturdays.com/assets/design/twitter/c.twitter%201r.png","permalink":"https://blog.robsewell.com/blog/viewing-github-pages-locally-with-a-remote-theme/","title":"Viewing GitHub Pages Locally With a Remote Theme"},{"content":"Do I use Notebooks? T-SQL Tuesday is the brainchild of Adam Machanic (Blog Twitter). The first T-SQL Tuesday invitation was in December 2009 and it is still going strong. It is a monthly blog party on the second Tuesday of each month. Currently, Steve Jones (Blog Twitter) organises the event and maintains a website with all previous posts. Everyone is welcome to participate in this monthly blog post.\nThis month‚Äôs T-SQL Tuesday is hosted by Steve. Steve says:\nI want you to write about how you have used, or would like to use, a Jupyter notebook. This seemed to be exciting for many people at first, but I haven‚Äôt seen a lot of uptake from users in general. So I‚Äôm curious if you are using them.\nThe original post is here.\nAm I using Notebooks ? Hehe. I LOVE notebooks. I use them all of the time and every day.\nI have written a few posts about them as well. I have a repository on GitHub with many notebooks https://beard.media/Notebooks. I have given presentations about notebooks https://beard.media/presentations I have videos on my youtube channel about notebooks https://beard.media/notebooksyoutube I have written a PowerShell Module to create Notebooks [] I have assisted clients with using notebooks to\nIntegrate new team members Create a repository of incident response notebooks dynamically created with Azure DevOps Create a repository of daily tasks notebooks Create a repository of common large scale changes Off-load DBA requests to Service Desk with notebooks Use notebooks to demonstrate changes to Product Owners and other teams Use notebooks for diagnosis by customers Use notebooks to investigate Azure environments and Azure Data Services and more I use notebooks to validate dbachecks PRs, to demonstrate dbachecks and dbatools with docker that anyone can use.\nI am thoroughly looking forward to seeing what other people do with notebooks. I love how the community helps us all to develop and move forward by sharing.\nAll this and Thank you - https://www.advancinganalytics.co.uk/\n","date":"2021-04-13T00:00:00Z","image":"https://blog.robsewell.com/assets/images/TSQL2sDay150x150.jpg","permalink":"https://blog.robsewell.com/blog/tsql2sday-do-i-use-notebooks/","title":"TSQL2sDay - Do I use Notebooks?"},{"content":"Data Saturdays Has New Clothes! The Data Saturdays Admins asked the community to vote on their favourite logo for the Data Saturdays website. After over 400 votes the results came in.\nDenny Cherry \u0026amp; Associates Consulting https://www.dcac.com/ generously supported Data Saturdays and paid for the artist to design the logo and create the artifacts via 99designs.com. THANK YOU Denny and many thanks to Monica Rathbun twitter - blog for all of the hard work in organising and administering all of the requirements and handling all of the communication with the artists.\nNow we have to update the web-site The next challenge we face is to update the website. As the website is hosted on GitHub Pages using Jekyll, this means that we can easily update the website by updating the code and letting GitHub actions build the new site but we have no way of checking the way that it looks before we push the changes. With such a radical change required, I felt that it would be a good idea to explore how to do this locally.\nInstall everything you need locally I examined the requirements to create a local development environment and this meant installing Jekyll and Ruby and a host of other things, there appeared to be a whole bundle of quirks and strange errors that may or may not need to be handled so I quickly went off that idea!!\nDocker to the rescue This is a fantastic use case for using a Docker container. I can host all of the required bits inside a container, spin it up and down as I need it and I don\u0026rsquo;t have to worry about polluting my machine with software and settings or the pain of having to configure it to work.\nAlso, other people have already done a lot of the work so I dont have to.\nI am running Docker in WSL2. I followed these instructions to set it up. It doesn\u0026rsquo;t take very long.\nWith thanks to Hans Kristian Flaatten GitHub - Twitter who has created this docker image it is as easy as running this from the local directory of the site repository\n1 docker run -it --rm -v \u0026#34;$PWD\u0026#34;:/usr/src/app -p \u0026#34;4000:4000\u0026#34; starefossen/github-pages If you are not using WSL but native Docker on Windows, then the command to run is slightly different\n1 docker run -it --rm -v .:/usr/src/app -p \u0026#34;4000:4000\u0026#34; starefossen/github-pages As soon as the container has started running and built the site I can see my changes locally in my browser at http://localhost:4000/ There are a few warnings as it builds that can be ignored. These are due to the autoomatic dynamic page generation code.\nDevelop and Test Now I can make changes to the code in the website and save the file and the site will update. In the below video, you can see that I have updated the favicon so that the new logo appears.\nI shall go back to editing the site now.\nA little \u0026lsquo;Feature\u0026rsquo; if you are working on your event page If you are following the wiki documentation to create or edit your event, you will find there is a little complication. When you click on yours or any event link on the front page it will take you to a page that starts http://0.0.0.0:4000/ like http://0.0.0.0:4000/2021-04-17-datasaturday0005/. This will not work on a Windows machine so you will have to replace 0.0.0.0 in the address bar with localhost\nand then it will work\nData Saturdays You can find the Data Saturdays web-site here. There is a list of all of the upcoming and past Data Saturdays events available.\n","date":"2021-04-11T00:00:00Z","image":"https://datasaturdays.com/assets/design/twitter/c.twitter%201r.png","permalink":"https://blog.robsewell.com/blog/viewing-github-pages-locally-for-data-saturdays/","title":"Viewing GitHub Pages Locally For Data Saturdays"},{"content":"Creating a New Data Saturdays Event There\u0026rsquo;s a new process to create a Data Saturdays Event page, so I thought I would write an explanation and a run through\nWhat is Data Saturdays ? Firstly, not everyone will know what a Data Saturday event is, so lets start with that. There are two parts to it.\nA Data Saturday is an event that provides (usually free) training and information sessions about Azure Data and SQL Server. At present they are hosted online.\nThe Data Saturdays resource is an open-source repository which enables event organisers to easily build a web presence as an entry point into their event. It integrates with other free event management solutions such as Sessionize enabling Call For Speakers, easily integrating the schedule, room links and speaker walls. The website is https://datasaturdays.com\nHere is a screenshot of the first Data Saturday \u0026ldquo;in\u0026rdquo; Pordenone.\nThe marvelous Gianluca Sartori and I started this to enable Pordenone to hold an event. We open-sourced the code and hosted it in the SQL Collaborative GitHub organisation alongside community tools such as dbatools with a MIT licence so that it is free for anyone to use and to collaborate with. The website is hosted on GitHub Pages which generates static pages using Jekyll. We figured that this not only enabled a quick free solution but also offered opportunities for people to enrich their skills by collaborating.\nWe wanted to include other community leaders to assist with guiding the project and we were proud that everyone we asked to be involved accepted. The people who are Admins of the project (who can approve changes to the code and therefore the website) in addition to Gianluca and I are : -\nSteve Jones Monica Rathbun Randolph West Johan Ludvig Bratt√•s Andy Mallon Elizabeth Noble Warwick Rudd Matt Gordon We have now enabled 10 Data Saturday events to exist, which we still think is amazing! However with growth comes challenges.\nCreating an event the old way The old method of creating an event involved the organiser providing the required information and an admin creating the static HTML page. Copying and pasting, ensuring that the template stayed the same but the detail was altered. Of course, when things are done manually humans can make errors and we made errors. The beauty of hosting the website in code in GitHub is that we can quickly change the code when we notice and fix them but this was not ideal.\nAutomation Automation AUTOMATION ! I love automation, I get a real buzz out of taking manual monotonous tasks and automating them. I looked at the process we were following and took the bait and decided to automate it. I have created a data-driven process for creating and updating the event web-page and the rest of this blog post is an accompaniment to the official documentation in the Wiki in the Data Saturdays GitHub repository. I might also blog about how I did it.\nIf you wish to just watch a video, you can find that here\nCreating a New Data Saturday Event How do you create a new event? The steps are laid out in the wiki\nTooling We suggest that you use Visual Studio Code as the editor to make these changes. Visual Studio Code is a superb free lightweight cross-platform code editor. To reduce the frustration we also suggest that you add the YAML Extension to Visual Studio Code as this will help to identify any problems with YAML.\nFork the Data Saturdays Repository I have previously written a blog post that explains how to contribute to an open-source repository which you can also use as reference for some of these steps\nWe are using GitHub as the source control for the website, so you will need to signup for a GitHub account if you do not have one already. This is free. Once you have that, navigate to the Data Saturdays repository and click on the Fork button\nIt will ask you where you want to fork it and you should choose your own GitHub account\nIt will only take a few seconds and you will have a fork of the repository in your own account.\nClone the Repository to your machine To work with the code, you need to clone it to your own machine (There are other options like codespaces which I love, but we will leave that for another time) Click on the green Code button and copy the URL using the button\nthen in Visual Studio Code CTRL + SHIFT + P will open the Command Palette and search for clone\nIf you do not see Git:Clone you will need to install git from https://git-scm.com/downloads\nCreate a new branch You create a new branch to hold your changes by clicking on the branch name in the bottom left\nand give it a new name\nCreate the Markdown File Now you can start to create the data for your event. First you need to see what the next available number is. Check the _data/events directory to see what has gone before you.\nIn the _posts directory, create a new file with the following naming convention YYYY-MM-DD-datasaturdayXXXX.md where XXXX is the next number available. An example name is 2021-06-12-datasaturday0007.md\nIn the file you place the following content\n1 2 3 4 5 6 7 8 --- layout: post title: \u0026#34;The Name of the Data Saturday in double quotes\u0026#34; subtitle: \u0026#34;Data Saturday\u0026#34; tags: [event] comments: false data: datasaturdayXXXX --- The 3 dashes are important to keep. The name must be in double quotes and the data must match your number. It should look like this.\nSave the file.\nCreate the data file. This is the most important file. This file is the one that will feed the page that you use. This is the file that you will update as your event timeline progresses.\nIn the _data/events directory create a new file named datasaturdayXXXX.yml (The XXXX is your number again) example datasaturday0007.yml\nIn this file paste all the following\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 name: \u0026#34;This is the name of your event inside the double quotes\u0026#34; date: The date of your event in YYYY-MM-DD HH:mm:ss TZ IE 2021-06-12 08:00:00 -0000 description: \u0026#34;Your event description inside double quotes, you may use HTML. You MUST escape double quotes with a backslash \\ (Look in the repo or wiki for examples of how to enter images) Line breaks need to be entered as \u0026lt;br\u0026gt; \u0026#34; registrationurl: This is your registration URL join: description: Click on the room you want to join. You can change rooms at any time to attend the sessions that you prefer. rooms: - name: Room 1 url: you can add more rooms if you have a virtual event. You can remove these if you do not know yet. scheduleurl: This is your schedule URL from Sessionize. You can leave this blank until you have it. sponsors: - link: https://yoursponsorlink image: your sponsor image height: image height if required speakerlisturl: This is your Call For Speakers URL when you start, once you have chosen your sessions change this to your Sessionize SpeakerWall API URL callforspeakers: true (until your call for speaker finishes!) volunteerrequesturl: If you want a link for people to volunteer place it here organizers: - name: Your name twitter: https://twitter.com/TWITTERNAME email: Contact email or not Now you have to fill in your own data. The fields have explanations in them, the wiki has descriptions and you can always refer back to this blog post also. Some are obvious like name and date, some will take a little thought like description and some you won\u0026rsquo;t have yet like your Sessionize API URLs.\nThis file can be altered any time that you like during your event timeline as more information becomes available or you wish to change things. Each time, you can create a Pull Request to the Data Saturdays repository but before that It is really important that you check your YAML.\nOnce your data file is ready\nCheck your YAML If you have followed our advice and used Visual Studio Code and the YAML extension then you can check that your YAML is correctly formed by looking at the problems tab\nThe example above has no problems so the YAML is correct. If it is not you will see\nNormally with YAML the problem is spaces, try to line up the text until the problem goes away.\nSync your local repository with GitHub Once your changes have been made, you will need to commit them with a commit message. We suggest that it is something relevant to your event\nThen you will need to press the publish button in Visual Studio Code to publish this branch to GitHub\nCreate a Pull Request in the Data Saturdays Repository Last step is to create a Pull Request. Open your browser at your local GitHub repository. You will see a green button saying compare and pull request.\nWhen you click that it will automatically open a pull request for you. Add some details about who you are and your event and an admin will then review it and merge it. Once it has been merged, the site will be rebuilt and will include your new event page.\nContinue to update your event AS you progress along your event timeline, you will need to edit the data file and create a new Pull Request. You will do this\nWhen you get a new sponsor When you have enough volunteers When your Call for Speaker closes When your event is published and you have your SpeakerWall and Schedule API URLs from Sessionize To add links to your virtual rooms To add your feedback links After your event has finished As you change those things, create new Pull Requests, and they are merged, your event page will be updated.\n","date":"2021-03-18T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/datasaturdays.png","permalink":"https://blog.robsewell.com/blog/creating-a-new-data-saturdays-event/","title":"Creating a New Data Saturdays event"},{"content":"What Happens Next? When PASS announced that operations would cease, a common question was \u0026ldquo;OK, so what happens now? To SQL Saturdays?, to the summit?, to the local user groups?\u0026rdquo; The answers to some of those questions will lie in other conversations but for right now, let\u0026rsquo;s talk about Local User Groups.\nUser Groups User groups are a fantastic way to expand your knowledge and your network. A place to find like minded people performing similar roles that you can learn from and get support from.\nAttending my local user group for the first time really helped me to improve. At the time I was really struggling in my job with no idea how to get support from people who know what should be done. I joined the group, I made many friends and I learnt a lot. My OneNote is full of notes I have taken during User Group sessions. I became involved in running the user group and the associated SQL Saturday and then became aware of the wider community.\nWe are always very proud of the number of speakers who started their speaking career at our user group and have gone to great things. I too, started my speaking career at our user group.\nOur little user group in Exeter in the UK (or in Teams these days!) and user groups in general have a big place in my heart and I am a strong supporter of them. (If you would like a speaker, please always feel free to get in touch with me.) so I too, wondered\nWhat is going to happen to the user groups now?\nMicrosoft Values The Community Microsoft have announced the support that they will be giving to user groups. You can watch Buck Woody t explain everything that the Microsoft Data Platform is doing to empower the community on Youtube\n(While you are there, I suggest subscribing to the channel, it\u0026rsquo;s really awesome)\nMicrosoft always has and always will support the technical professionals who use their products and they recognise that at the grass roots, the local user groups are the core of the community.\nSo Microsoft are taking over the User Groups? NO.\nMicrosoft\u0026rsquo;s philosophy is that local user groups should be Community-owned but they can be Microsoft-empowered. They have created a series of assets, resources, and benefits for user group leaders to run their groups. I am astonished at the scale of what has been achieved in a short amount of time. There are a lot of benefits that will make it easier for user groups to have a stable presence and all the tools that they need to enable their user group .\nWhat is being offered ? If you want to skip right to the sign up please fill out the form linked on this page\nThe Azure Data Community will provide\nThe Community Landing Page aka.ms/datacommunity which is central gateway for useful Azure Data Community resources from blog posts and videos to the user groups. Meetup Pro fees, fully paid for by Microsoft saving user groups leaders from paying out of their own pocket or finding sponsorship. A centralised way to find groups and events in the Data Community https://www.meetup.com/pro/azuredatatechgroups/ Full Microsoft Teams subscriptions with all the bells \u0026amp; whistles for qualified Community Groups using a customisable template for the user group to define as they wish. This will be fantastic for running virtual events, sharing slides and demo code as well as keping in touch wiht your fellow user group members\nCommunity Leader Collaboration (via Teams \u0026amp; Meetup) enabling all the user groups leaders to be able to communicate, collaborate and share resources.\nDo I have to use this ? NO.\nYou are in charge, if you already run your user group and don\u0026rsquo;t need these resources, let the team know and they will get you listed on the community gateway site.\nWhat are the requirements? Throughout the duration of the Group‚Äôs participation in the Program, the Group will:\nHave a group leader and a designated co-leader who have each accepted these T\u0026amp;Cs. Not charge other members a fee to attend Group meetings, except in cases in which the venue and/or hosting costs (e.g., food and beverages) are passed through to members. Maintain a published code of conduct that is easily accessible from the Group‚Äôs home page. See guidance https://aka.ms/atg/guidance. Maintain a regular meeting cadence including having meetings at least six times per year that relate to or cover Azure Data products \u0026amp; services or relate to Diversity, Equity \u0026amp; Inclusion (DE\u0026amp;I) or Professional Development targeted to data professionals. Comply with Program‚Äôs Code of Conduct located at https://aka.ms/atg/code_of_conduct. Adhere to Microsoft‚Äôs Trademark and Brand guidelines, when using any Microsoft trademarks or referring to Microsoft‚Äôs software, products or services (see https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general.aspx. Which technologies do these benefits apply to? SQL Server (on Windows, Linux, and in Containers, on-premises and in Microsoft Azure) Azure Data Lake Azure Cosmos DB Azure HDInsight, Hadoop and Spark on Azure Azure Search Data Warehousing (Azure SQL Data Warehouse, Fast Track and APS) Azure Stream Analytics Cortana Intelligence Suite Information Management (ADF, SSIS, and Data Sync) SQL Server Reporting Services and Analysis Services SQL Server Machine Learning Services Azure Database for MySQL Azure Database for PostgreSQL Azure SQL (Database, Pools, Serverless, Hyperscale, Managed Instance, Virtual Machines) Azure SQL Edge Big Data Clusters Azure Databricks Azure Arc Enabled Data Services Azure Synapse Analytics Azure Data Catalog Sign up vis the form linked on this page\nThank you to all of those people involved in making this happen.\n","date":"2021-02-17T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2021/MSCommunity.png","permalink":"https://blog.robsewell.com/blog/microsoft-values-the-community/","title":"Microsoft Values The Community"},{"content":"Tooling for TSQL2sDay T-SQL Tuesday is the brainchild of Adam Machanic (Blog Twitter). The first T-SQL Tuesday invitation was in December 2009 and it is still going strong. It is a monthly blog party on the second Tuesday of each month. Currently, Steve Jones (Blog Twitter) organises the event and maintains a website with all previous posts. Everyone is welcome to participate in this monthly blog post.\nThis month‚Äôs T-SQL Tuesday is hosted by Mikey Bronowski ( Blog Twitter ). Mikey says:\nWithout tools, most of the work would be much harder to do or could not be done at all. Write a blog post about the most helpful and effective tools you use or know of.\nThe original post is here.\nI miss Mikey, I worked in an office with him until the end of 2019. We used to sit next to each other which meant that I required some tools of my own!\nWe had a lot of fun and worked hard in Yorkshire (although he never learnt to make a proper cup of tea!) and both of us learnt new sayings\nTools Writing that gave me an idea for this blog post. Of course, I could have written about PowerShell, Azure Data Studio, dbatools, dbachecks or many of the other tools I use to deliver change and automate. You will find several good posts in this TSQL2sday series from people like Jess Pomfret and John Martin showing some of those.\nInstead, I am going to share some of my working from home tools that I cannot do without.\nCoffee Hello, my name is Rob and I drink a lot of coffee! I like to have coffee, I drink a lot of it so we got a coffee machine to make it easier.\n[\nIt takes beans, which we get delivered via Amazon and turns them into any sort of coffee you like with a press of a button.\nStanding Desk Spending a lot of time sitting down without moving much is not much good you and also really easy to do when you are working from home so I got a standing desk from Autonomous Which I love. I normally start my morning sitting down, stand up for the standup! Then I will alternate throughout the day between sitting and standing or using my wibbly wobbly Autonomous stool\nBicycle With Covid I was not able to play any cricket last year. I really enjoyed playing cricket. I have played it since I was very small and having a reason to be out in the open, expending some energy and getting my competitive juices flowing was good for me. I have cycled in the past and enjoyed it. With the cricket team I cycled from John o\u0026rsquo; Groats to Lands End for charity a few years ago, so I bought a bike and now I again have a reason to go out in the fresh air and get my competitive juices going again, although I only compete against myself and compare my results on Strava with my previous times. Last year I started riding in April and rode 2897 miles. This gets me out into the beautiful south west UK countryside, expends some energy and keeps me moving, which I think is a good thing!\nZwift I told you a little fib, well maybe I just neglected the whole story. I did ride 2897 last year but not all of them are outside. I found that I really enjoy cycling in the morning when I wake up but its not really safe when it is so dark or pleasant to do when the weather is rough, so I have a cycle trainer which I connect to Zwift. This enables me to ride with other people from all over the globe and explore various worlds, both real representations of London, New York and Paris and the virtual world of Watopia where you can see dolphins, whales, shipwrecks, dinosaurs or ride up and through a volcano. This can be really fun and certainly makes it easier for me to cycle more regularly. It has a number of gamifications and even allows you to ride through a tube station at midnight ! (note this was not at midnight but The Jam didnt sing about the tube station at 6-30am!!)\nMost Important of all The People The most important tool though is the most useful.\nThe community.\nThe people who take their time to share knowledge, whether by blog posts, presentations, answering #sqlhelp on Twitter or just replying to a DM or an email. Those who organise virtual events or user groups, open source projects and community tooling like Data Saturdays and those who get paid for it as vendor evangelists or Microsoft or other companies employees.\nThe sheer range of knowledge that is readily and freely available with a simple courteous question is amazing and has certainly saved me a bunch of time and helped me out of some difficult to solve situations. Remember to be polite and appreciative of the time that they take out of their day and you will find they are willing to share their knowhow and help.\nThank you to all of those people\n","date":"2021-02-09T00:00:00Z","image":"https://blog.robsewell.com/assets/images/TSQL2sDay150x150.jpg","permalink":"https://blog.robsewell.com/blog/tooling-for-tsql2sday/","title":"Tooling for TSql2sDay"},{"content":"It started with a tweet As with many things in my life it started with a tweet\nThat looks awesome, I thought, so I watched the YouTube video.Scott has written a C# application that would change the scene depending on some text in the PowerPoint slide notes. Then, by applying a Chroma filter to the display capture and placing the webcam capture appropriately, when the slide changed, the Obs scene changed and the webcam became embedded in the slide!!!!!!!\nIt is truly awesome but it is for Obs and I use StreamLabs and I wondered if it could be done with PowerShell.\n(If you just want the code, you can find it here)\nListen to PowerPoint Events with PowerShell Create a Com Object The first thing that we need to do is to find out when the PowerPoint Slide has changed.\nYou can create a PowerPoint Com Object with\n`$Application = New-Object -ComObject PowerPoint.Application`\rand make it visible with\n`$Application.Visible = 'MsoTrue'`\rGet the Slide Number and Notes Next step is to get the slide number. It is not truly required for the code, but I like to print it out so that I know which slide I was on for trouble shooting.\nLooking at Scotts code here I worked out that the slide number via PowerShell was\n`$slideNumber = $PowerPoint.SlideShowWindows[1].view.Slide.SlideIndex`\rThe notes (by looking at code) can be accessed at\n`$notes = $PowerPoint.SlideShowWindows[1].View.Slide.NotesPage.Shapes[2].TextFrame.TextRange.Text`\rthen parse the notes to get the scene name which is defined as OBS:SceneName\n`$SceneName = ($notes -split \u0026quot;`r\u0026quot;)[0] -replace 'OBS:', ''`\rThe first part gets the first line and it was thanks to Andreas on twitch who got this working, Thank you Andreas.\nListen to an Event With PowerShell, you can subscribes to events and take action when they fire. The event that we are going to subscribe to is called SlideShowNextSlide\n`$subscriber = Register-ObjectEvent -InputObject $PowerPoint -EventName SlideShowNextSlide -Action $action `\rWe have defined an $action variable in the code but we need to provide an action and this is where things got a little tricky.\nAutomating StreamLabs OBS In Scotts code he uses OBS.WebSocket.NET to control OBS. Excellent, PowerShell and .NET.Unfrotunately, StreamLabs uses an RPC-based API https://stream-labs.github.io/streamlabs-obs-api-docs/docs/index.html\nThis documentation specifies\nYou can access services\u0026rsquo; methods and properties by sending JSON-RPC messages to the named pipe slobs.\nThank you Keith Hill So Rob traversed a rabbit warren of investigation to understand how to send messages to this API with PowerShell and eventually stumbled across the marvelous Keith Hill blog twitter and a blog post from 2014\nhttps://rkeithhill.wordpress.com/2014/11/01/windows-powershell-and-named-pipes/\nCreate a connection and send and receive messages Now I had everything I needed to create a connection to SLOBS via named pipes. SLOBS needs to be started here!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Create Client $npipeClient = New-Object System.IO.Pipes.NamedPipeClientStream($Env:ComputerName, \u0026#39;slobs\u0026#39;, [System.IO.Pipes.PipeDirection]::InOut, [System.IO.Pipes.PipeOptions]::None, [System.Security.Principal.TokenImpersonationLevel]::Impersonation) $npipeClient.Connect() $npipeClient # Create Reader and writer and send and receive message $pipeReader = New-Object System.IO.StreamReader($npipeClient) $pipeWriter = New-Object System.IO.StreamWriter($npipeClient) $pipeWriter.AutoFlush = $true # Send message $pipeWriter.WriteLine($scenesMessage) # Receive message $pipeReader.ReadLine() Which messages? Next I needed to get the messages to send formatted correctly. Looking at the API docs I saw\n1 2 3 4 5 6 7 8 { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;getScenes\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;resource\u0026#34;: \u0026#34;ScenesService\u0026#34; } } So I was able to get the current available scenes with\n1 2 3 $scenesMessage = \u0026#39;{\u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;,\u0026#34;id\u0026#34;: 6,\u0026#34;method\u0026#34;: \u0026#34;getScenes\u0026#34;,\u0026#34;params\u0026#34;: {\u0026#34;resource\u0026#34;: \u0026#34;ScenesService\u0026#34;}}\u0026#39; $pipeWriter.WriteLine($scenesMessage) ($pipeReader.ReadLine() | ConvertFrom-Json).result | Select Name, id Change Scenes The last part of the jigsaw was to change the scene via the named pipe connection\n1 2 3 4 5 6 7 $scenesMessage = \u0026#39;{\u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;,\u0026#34;id\u0026#34;: 6,\u0026#34;method\u0026#34;: \u0026#34;getScenes\u0026#34;,\u0026#34;params\u0026#34;: {\u0026#34;resource\u0026#34;: \u0026#34;ScenesService\u0026#34;}}\u0026#39; $pipeWriter.WriteLine($scenesMessage) $scenes = ($pipeReader.ReadLine() | ConvertFrom-Json).result | Select Name, id $SceneId = ($scenes | Where Name -eq $SceneName).id $MakeSceneActiveMessage = \u0026#39;{ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;makeSceneActive\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;resource\u0026#34;: \u0026#34;ScenesService\u0026#34;,\u0026#34;args\u0026#34;: [\u0026#34;\u0026#39; + $SceneId + \u0026#39;\u0026#34;]}}\u0026#39; $pipeWriter.WriteLine($MakeSceneActiveMessage) $switchResults = $pipeReader.ReadLine() | ConvertFrom-Json Which looks like this :-)\nSetting up PowerPoint and Scenes With the PowerShell set up, we next need to set it up to use the scenes. I followed Scotts example and used OBS:SceneName as the reference to the Scene. I added this to the first line of the notes on a slide\nand then created a text box with a green fill\nIn StreamLabs, I set up the scene with the same name, the order of the sources is important. They are displayed from top to bottom, front to back so the Display Capture will be on top of the Sony Camera here\nThen I right clicked on the Display Capture and chose Filters\nand chose a Chroma Key filter\nWith the PowerPoint in SlideShow mode, I set the Chroma Key filter colour to match the colour of the green box, placed the camera source in the correct location and saved.\nThe image below shows form left to right, the Chroma Key settings, the scene in SLOBS and the PowerPoint slideshow\nNormally, I would do this on seperate screens of course!\nI set up each slide like this and then I closed the PowerPoint and ran the code, you can find it here,)leaving PowerShell running in the background. This opened PowerPoint and I opened the deck and started the slide show and as I navigate through the slide, the scene changes and so does the webcam position :-)\nYou can see a test run below\nand the demo pptx can be found here\n","date":"2020-09-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/09/scottwitter.png","permalink":"https://blog.robsewell.com/blog/using-powershell-to-automate-streamlabs-obs-and-show-your-webcam-in-powerpoint/","title":"Using PowerShell to Automate StreamLabs OBS and Show Your Webcam in PowerPoint"},{"content":"Automation This month it is hosted by Elizabeth Noble blog and twitter.\nThank you Elizabeth\nElizabeth asks\nMy invitation to you is I want to know what you have automated to make your life easier?\nFrom the Past I am in the process of migrating my blog to GitHub pages and whilst doing so, I read my first ever technical blog post You have to start somewhere In it I mention this blog post by John Sansom The Best Database Administrators Automate Everything which I am pleased to see is still available nearly a decade later\nHere is a quote from his blog entry\nAutomate Everything That‚Äôs right, I said everything. Just sit back and take the time to consider this point for a moment. Let it wander around your mind whilst you consider the processes and tasks that you could look to potentially automate. Now eliminate the word potentially from your vocabulary and evaluate how you could automate e-v-e-r-y-t-h-i-n-g that you do.\nEven if you believe that there is only a remote possibility that you will need to repeat a given task, just go ahead and automate it anyway! Chances are that when the need to repeat the process comes around again, you will either be under pressure to get it done, or even better have more important Proactive Mode tasks/projects to be getting on with\nI love Automation I have tried my best at all times to follow this advice in the last decade and pretty much I am happy that I have managed it.\nI use PowerShell (a lot!) to automate all sorts of routine tasks including migrating this blog I use Jupyter Notebooks to enable myself and others to automate Run Books, Training, Documentation, Demonstrations, Incident Response. You can find my notebooks here I use Azure DevOps to automate infrastructure creation and changes with terraform and delivery of changes to code as well as unit testing. I use GitHub actions to create this blog, publish the ADSNotebook module I use Chocolatey to install and update software I have used Desired State Configuration to ensure that infrastructure is as it is expected to be At every point I am looking for a means to automate the thing that I am doing because it is almost guaranteed that there will be a time in the future after you have done a thing that there will be a need to do it again or to do it slightly differently.\nWhats the last thing that you automated? Following my blog post about Notifying a Teams Channel about a SQL Agent Job result I was asked if this could be tweaked to reduce the time spent getting information about SSIS Execution failures.\nFinding SSIS failures When you run an SSIS package in an Agent Job and it fails, the Agent Job History shows something along these lines\nThe job failed. The Job was invoked by User MyDomain\\MyUserName. The last step to run was step 1 (scheduling ssis package). Executed as user: NT Service\\SQLSERVERAGENT. Microsoft (R) SQL Server Execute Package Utility Version 11.0.5058.0 for 64-bit Copyright (C) Microsoft Corporation. All rights reserved. Started: 4:17:12 PM Package execution on IS Server failed. Execution ID: 123456789, Execution Status:4. To view the details for the execution, right-click on the Integration Services Catalog, and open the [All Executions] report Started: 4:17:12 PM Finished: 4:17:12 PM Elapsed: 4.493 seconds. The package execution failed. The step failed.\nThe next step is to open SSMS, go to the SSISDb and click through to the SSIS reports and then scroll through to find the package and then the message. This is not particularly efficient and the SSIS reports are not known for their speedy executions!\nThis meant that the team member responsible for checking in the morning, could see which instance and which job had failed from the Teams message but then had to manually follow the above steps to find an error message that they could take action on.\nAutomate it In the SSISDB database there is an event_messages view so if I could query that and filter by the Execution ID then I could get the message and place it into the Teams message. Now the Teams message contains the error for the SSIS execution and each time this happens it probably saves the team member 4 or 5 minutes :-)\nIn the code below, I\ncheck if the failure comes from an SSIS instance if($Inst -in ($SSISInstances)){\nGet the Execution ID from the Error message $ExecutionId = [regex]::matches($BaseerrMessage, 'Execution ID: (\\d{3,})').groups[1].value\nCreate a query for the SSISDB\n$SSISQuery = @\u0026quot; SELECT * FROM catalog.event_messages em WHERE em.operation_id = $ExecutionId AND (em.event_name = 'OnError') ORDER BY em.event_message_id; \u0026quot;@\nSet the Error Message and the Execution Path to variables $errMessage = $SSISQueryResults.Message $ExecutionPath = $SSISQueryResults.execution_path\nGet the Error Message for none SSIS failures }else{ $errMessage = $j.group[-1].Message $ExecutionPath = 'the job' }\nCreate the Teams message\nYou will see that I used SELECT * because someone will always ask for some extra information in the future!\nThe full script is below, Happy Automating!\n$webhookurl = \u0026quot;https://outlook.office.com/webhook/ the rest of it here\u0026quot;\r$SSISInstances = # to identify SSIS instances\r$ProdInstances = # ALL instances for checking\r$startdate = (Get-Date).AddHours(-1)\r$AllFailedJobs = foreach ($Instance in $ProdInstances) {\rWrite-Host \u0026quot;Connecting to $instance\u0026quot;\rtry{\r$smo = Connect-DbaInstance $Instance -ErrorAction Stop\rWrite-Host \u0026quot;Connected successfully to $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed to connect to $Instance\u0026quot;\r$errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\rWrite-Host \u0026quot;Getting Agent Jobs on $instance\u0026quot;\rtry {\r$AgentJobs = Get-DbaAgentJobHistory -SqlInstance $smo -EnableException -StartDate $startdate\rWrite-Host \u0026quot;Successfully got Agent Jobs on $instance\u0026quot;\r}\rcatch {\rWrite-Host \u0026quot;Failed to get agent jobs on $Instance\u0026quot;\r$errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r$jobs = $agentJobs # | Where-Object { $Psitem.Job -match '^Beard-\\d\\d\\d\\d\\d' -or $Psitem.Job -like 'BeardJob*' } # if you need to filter\r$FailedJobs = $jobs | Where-Object { $Psitem.Status -ne 'Succeeded' }\r$FailedJobs | Group-Object Job\rtry{\r$smo.ConnectionContext.Disconnect()\rWrite-Host \u0026quot;Disconnecting $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed disconnect from $Instance\u0026quot;\r$errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r}\rWrite-Host \u0026quot;We have $($AllFailedJobs.Count) Failed Jobs\u0026quot;\r[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\rforeach ($j in $AllFailedJobs) {\r$Inst = $j.group[-1].SqlInstance\r$jName = $j.name\r$sname = $j.group[-1].StepName\r$edate = $j.group[-1].EndDate\rif($Inst -in ($SSISInstances)){\r$BaseerrMessage = $j.group[-1].Message\r$ExecutionId = [regex]::matches($BaseerrMessage, 'Execution ID: (\\d{3,})').groups[1].value\r$SSISQuery = @\u0026quot;\rSELECT * FROM catalog.event_messages em\rWHERE em.operation_id = $ExecutionId\rAND (em.event_name = 'OnError')\rORDER BY em.event_message_id;\r\u0026quot;@\r$SSISQueryResults = Invoke-DbaQuery -SqlInstance $Inst -Database SSISDB -Query $SSISQuery\r$errMessage = $SSISQueryResults.Message\r$ExecutionPath = $SSISQueryResults.execution_path\r}else{\r$errMessage = $j.group[-1].Message\r$ExecutionPath = 'the job'\r}\r$Text = @\u0026quot;\r# **$Inst**\r## **$JName**\r- The Job step that failed is - **$sname**\r- It failed at - **$edate**\r- It failed in $ExecutionPath with the message\r- $errMessage\r\u0026quot;@\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;There was a Job Failure\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;Job Failures \u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;in the Last 1 hour\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://blog.robsewell.com/assets/images/sobrob.jpg\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\r}\rif(-not $AllFailedJobs){\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;There were no job failures in the last hour at $ (Get-Date)\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;There were no job failures at $ (Get-Date)\u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;in the Last hour\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://blog.robsewell.com/assets/images/happyrob.jpg\u0026quot;\r\u0026quot;text\u0026quot; = \u0026quot;All is well\u0026quot;\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\r}\r","date":"2020-09-08T00:00:00Z","image":"https://blog.robsewell.com/assets/images/TSQL2sDay150x150.jpg","permalink":"https://blog.robsewell.com/blog/tsql2sday-130-automate-your-stress-away-getting-more-ssis-agent-job-information/","title":"#tsql2sday #130 - Automate your stress away - Getting more SSIS Agent Job information"},{"content":"Last night I started the experiment that has been in my head for a while now, to move from SQL Dba With A Beard to RobSewell.com\nThank you Chrissy Blog Twitter for the push!\nI followed the instructions from her blog post Migrating my WordPress sites to GitHub Pages but chose to use the Minimal Mistakes theme\nI like the search at the top and the 404 page :-)\nNow I need to export the wordpress from SQLDbaWithABeard.com (Its currently running)\nI will still blog on there for a while I think but export it to here also or blog on here and export to there, one or the other\n","date":"2020-09-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/the-first-page-with-github-pages/","title":"The first page with GitHub Pages"},{"content":"Following on from my posts about using Secret Management Good bye Import-CliXml and running programmes as a different user, I have another use case.\nAfter creating Azure SQL Databases in an Elastic Pool using a process pretty similar to this one I blogged about last year, I needed to be able to programmatically create users and assign permissions.\nI need a user to login with When I created my Azure SQL Server with Terraform, I set the Azure Admin to be a SPN as you can see in the image from the portal and set it to have an identity using the documentation for azurerm_mssql_server.\nThis allows this user to manage the access for the SQL Server as long as the SQL Server Azure AD identity has Directory Reader privileges. The SQL Server is called temp-beard-sqls and as you can see the identity is assigned to the role.\nThe privileges required to do this for a single identity are quite high\nso now, you can assign an Azure Active Directory Group to that Role and allow less-privileged users to add the identity to this group . The documentation is here and there is a tutorial here explaining the steps you need to take.\nWhat is an Azure SPN? An Azure service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources.\nhttps://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?toc=%2Fazure%2Fazure-resource-manager%2Ftoc.json\u0026amp;view=azure-cli-latest\nI created the SPN using Azure CLI straight from the Azure Portal by clicking this button\nand running\naz ad sp create-for-rbac --name ServicePrincipalName\rThis will quickly create a SPN for you and return the password\nYes I have deleted this one\nAdd Azure Key Vault to Secret Management In my previous posts, I have been using the Default Key Vault which is limited to your local machine and the user that is running the code. It would be better to use Azure Key Vault to store the details for the SPN so that it safely stored in the cloud and not on my machine and also so that anyone (or app) that has permissions to the vault can use it.\nFirst you need to login to Azure in PowerShell (You will need to have the AZ* modules installed)\nConnect-AzAccount\rBe aware, the login box can appear behind the VS Code or Azure Data Studio window!\nOnce connected, if you have several Azure subscriptions, you can list them with\nGet-AzSubscription\rYou can choose your subscription with\n$AzureSubscription = Set-AzContext -SubscriptionName \u0026quot;NAME OF SUBSCRIPTION\u0026quot;\rFor the Secret Management Module to manage the Azure Key Vault, you first need to register it.\nEnsure that you have permissions to connect by following the details in the network security documentation¬†https://docs.microsoft.com/en-us/azure/key-vault/general/network-security¬†and the secure access documentation¬†https://docs.microsoft.com/en-us/azure/key-vault/general/secure-your-key-vault\nThen you can run¬†Register-SecretVault¬†. You need to provide the local name for the key vault, the module name¬†Az.KeyVault, and a¬†VaultParameters¬†hashtable with the KeyVault name and the Azure Subscription ID. You can register other types of Key Vaults to the Secret Management module in this way and they will require different values for the¬†VaultParameters¬†parameter.\n$KeyVaultName = 'beard-key-vault'\rRegister-SecretVault -Name BeardKeyVault -ModuleName Az.KeyVault -VaultParameters @{ AZKVaultName = $KeyVaultName; SubscriptionId = $AzureSubscription.Subscription.Id }\rAdding the SPN details to the Azure Key Vault Using the values for AppID ‚Äì (Note NOT the display name) and the values for the password from the Azure CLI output or by creating a new secret for the SPN with PowerShell or via the portal. You can use the following code to add the SPN details and the tenantid to the Azure Key Vault using the Secret Management module\n$ClientId = Read-Host \u0026quot;Enter ClientID\u0026quot; -AsSecureString\r$SecretFromPortal = Read-Host \u0026quot;Enter Client Secret\u0026quot; -AsSecureString\r$tenantid = Read-Host \u0026quot;Enter TenantId\u0026quot; -AsSecureString\rSet-Secret -Vault BeardKeyVault -Name service-principal-guid -Secret $ClientId\rSet-Secret -Vault BeardKeyVault -Name service-principal-secret -SecureStringSecret $SecretFromPortal\rSet-Secret -Vault BeardKeyVault -Name Tenant-Id -Secret $tenantid\rYou can also do this with the Az.KeyVault module by following the instructions here\nYou can see the secrets in the portal\nand also at the command line with the Secret Management module using\nGet-SecretInfo -Vault RegisteredNameOfVault\rCan my user connect? If I try to connect in Azure Data Studio to my Azure SQL Database with my AAD account to the temp-sql-db-beard database. It fails.\nBy the way a great resource for troubleshooting the SQL error 18456 failure states can be found here https://sqlblog.org/2020/07/28/troubleshooting-error-18456\ndbatools to the rescue üôÇ dbatools is an open source community collaboration PowerShell module for administrating SQL Server. You can find more about it at dbatools.io and get the book that Chrissy and I are writing about dbatools at dbatools.io\\book\nYou can connect to Azure SQL Database with an Azure SPN using the following code. It will get the secrets from the Azure Key Vault that have been set above and create a connection. Lets see if I can run a query as the SPN.\n$SqlInstance = 'temp-beard-sqls.database.windows.net'\r$databasename = 'master'\r$appid = Get-Secret -Vault BeardKeyVault -Name service-principal-guid -AsPlainText\r$Clientsecret = Get-Secret -Vault BeardKeyVault -Name service-principal-secret\r$credential = New-Object System.Management.Automation.PSCredential ($appid,$Clientsecret)\r$tenantid = Get-Secret -Vault BeardKeyVault -Name Sewells-Tenant-Id -AsPlainText\r$AzureSQL = Connect-DbaInstance -SqlInstance $SqlInstance -Database $databasename -SqlCredential $credential -Tenant $tenantid -TrustServerCertificate\rInvoke-DbaQuery -SqlInstance $AzureSql -Database master -SqlCredential $credential -Query \u0026quot;Select SUSER_NAME() as 'username'\u0026quot;\rExcellent üôÇ\nAdd a user to the user database I can then add my user to the temp-sql-db-beard Database. I need to create a new connection to the user database as you cannot use the USE [DatabaseName] statement\n$Userdatabasename = 'temp-sql-db-beard'\r$AzureSQL = Connect-DbaInstance -SqlInstance $SqlInstance -Database $Userdatabasename -SqlCredential $credential -Tenant $tenantid -TrustServerCertificate\rWhilst you can use dbatools to create new users in Azure SQL Database at present you cant create AAD users. You can run a T-SQL Script to do this though. This script will create a contained database user in the database. I have added the role membership also but this can also be done with Add-DbaDbRoleMember from dbatools\n$Query = @\u0026quot;\rCREATE USER [rob@sewells-consulting.co.uk] FROM EXTERNAL PROVIDER\rALTER ROLE db_datareader ADD MEMBER [rob@sewells-consulting.co.uk]\r\u0026quot;@\rInvoke-DbaQuery -SqlInstance $AzureSql -Database $Userdatabasename -SqlCredential $credential -Query $Query\rLets check the users on the database with dbatools\nGet-DbaDbUser -SqlInstance $AzureSql -Database $Userdatabasename |Out-GridView\rI have my user and it is of type External user. Lets see if I can connect\nBingo üôÇ\nHappy Automating\nBecause I dont like to see awesome people struggling with PowerShell\nHere is the same code using just the Az.KeyVault module\n$appid = (Get-AzKeyVaultSecret -vaultName \u0026quot;beard-key-vault\u0026quot; -name \u0026quot;service-principal-guid\u0026quot;).SecretValueText\r$Clientsecret = (Get-AzKeyVaultSecret -vaultName \u0026quot;beard-key-vault\u0026quot; -name \u0026quot;service-principal-secret\u0026quot;).SecretValue\r$credential = New-Object System.Management.Automation.PSCredential ($appid,$Clientsecret)\r$tenantid = (Get-AzKeyVaultSecret -vaultName \u0026quot;beard-key-vault\u0026quot; -name \u0026quot;Sewells-Tenant-Id\u0026quot;).SecretValueText\r$AzureSQL = Connect-DbaInstance -SqlInstance $SqlInstance -Database $databasename -SqlCredential $credential -Tenant $tenantid -TrustServerCertificate\r","date":"2020-08-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/08/image-16.png","permalink":"https://blog.robsewell.com/blog/creating-azure-sql-database-aad-contained-database-users-with-an-spn-using-powershell-secrets-management-azure-key-vault-and-dbatools/","title":"Creating Azure SQL Database AAD Contained Database Users with an SPN using PowerShell, Secrets Management, Azure Key Vault, and dbatools"},{"content":"Following on from yesterdays post about creating an overview of SQL Agent Job Results and sending it to a Teams channel, I was given another challenge\nCan you write a job step that I can add to SQL Agent jobs that can send the result of that job to a Teams Channel\nA person with a need\nThe use case was for some migration projects that had steps that were scheduled via SQL Agent Jobs and instead of the DBA having to estimate when they would finish and keep checking so that they could let the next team know that it was time for their part to start, they wanted it to notify a Teams channel. This turned out especially useful as the job finished earlier than expected at 3am and the off-shore team could begin their work immediately.\nUsing SQL Agent Job tokens with PowerShell You can use SQL Agent job tokens in Job step commands to reference the existing instance or job but I did not know if you could use that with PowerShell until I read Kendra Little‚Äôs blog post from 2009.\nThank you Kendra\nNothing is ever as easy as you think So I thought, this is awesome, I can create a function and pass in the Instance and the JobId and all will be golden.\nNope\njob_id \u0026lt;\u0026gt; $(JobID) If we look in the sysjobs table at the Agent Job that we want to notify Teams about the result.\nWe can see that the job_id is\ndc5937c3-766f-47b7-a5a5-48365708659a\rIf we look at the JobId property with PowerShell\nWe get\ndc5937c3-766f-47b7-a5a5-48365708659a\rAwesome, they are the same\nBut\nIf we look at the value of the $(JobID) SQL Agent Job Token,\nwe get\nC33759DC6F76B747A5A548365708659A\rwhich makes matching it to the JobId tricky\nI tried all sorts of ways of casting and converting this value in SQL and PowerShell and in the end I just decided to manually convert the value\n$CharArray = $JobID.ToCharArray()\r$JobGUID = $CharArray[8] + $CharArray[9] + $CharArray[6] + $CharArray[7] + $CharArray[4] + $CharArray[5] + $CharArray[2] + $CharArray[3] + '-' + $CharArray[12] + $CharArray[13] + $CharArray[10] + $CharArray[11] + '-' + $CharArray[16] + $CharArray[17] + $CharArray[14] + $CharArray[15] + '-' + $CharArray[18] + $CharArray[19] + $CharArray[20] + $CharArray[21] + '-' + $CharArray[22] + $CharArray[23] + $CharArray[24] + $CharArray[25] + $CharArray[26] + $CharArray[27] + $CharArray[28] + $CharArray[29] + $CharArray[30] + $CharArray[31] + $CharArray[32] + $CharArray[33]\rSend the information to Teams Following the same pattern as yesterdays post, I created a function to send a message, depending on the outcome of the job and post it to the Teams function.\nAgain, I used Enter-PsSession to run the Teams notification from a machine that can send the message. (I have also included the code to do this without requiring that below so that you can send the message from the same machine that runs the job if required)\nThis code below is saved on a UNC share or the SQL Server as SingleNotifyTeams.ps1\nParam(\r$SqlInstance,\r$JobID\r)\r$webhookurl = \u0026quot;\u0026quot;\r$NotifyServer = 'BeardNUC2'\rfunction Notify-TeamsSQlAgentJob {\rParam(\r$SQLInstance,\r$JobID,\r$webhookurl\r)\r$SQLInstance = $SQLInstance # Import-Module 'C:\\Program Files\\WindowsPowerShell\\Modules\\dbatools\\1.0.107\\dbatools.psd1'\r[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\r$CharArray = $JobID.ToCharArray()\r$JobGUID = $CharArray[8] + $CharArray[9] + $CharArray[6] + $CharArray[7] + $CharArray[4] + $CharArray[5] + $CharArray[2] + $CharArray[3] + '-' + $CharArray[12] + $CharArray[13] + $CharArray[10] + $CharArray[11] + '-' + $CharArray[16] + $CharArray[17] + $CharArray[14] + $CharArray[15] + '-' + $CharArray[18] + $CharArray[19] + $CharArray[20] + $CharArray[21] + '-' + $CharArray[22] + $CharArray[23] + $CharArray[24] + $CharArray[25] + $CharArray[26] + $CharArray[27] + $CharArray[28] + $CharArray[29] + $CharArray[30] + $CharArray[31] + $CharArray[32] + $CharArray[33]\r$Job = Get-DbaAgentJob -SQlInstance $SQLInstance | Where jobid -eq $JobGuiD\r$JobName = $Job.Name\r$Jobsteps = Get-DbaAgentJobStep -SQlInstance $SQLInstance -Job $JobName\r$JobStepNames = $Jobsteps.Name -join ' , '\r$JobStartDate = $job.JobSteps[0].LastRunDate\r$JobStatus = $job.LastRunOutcome\r$lastjobstepid = $jobsteps[-1].id\r$Jobstepsmsg = $Jobsteps | Out-String\r$JobStepStatus = ($Jobsteps | Where-Object {$_.id -ne $lastjobstepid -and $_.LastRunDate -ge $JobStartDate} ).ForEach{\r\u0026quot; $($_.Name) - $($_.LastRunDate) **$($_.LastRunOutCome)** \u0026quot;\r} $Text = @\u0026quot;\r# **$SqlInstance** ## **$JobName** $jobstepMsg\rStarted at $JobStartDate - The individual Job Steps status was $JobStepStatus \u0026quot;@\rif (( $jobsteps | Where id -ne $lastjobstepid).LastRunOutcome -contains 'Failed') {\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;There was a Job Failure\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;The Job Failed\u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;Work to do - Please investigate the following job by following the steps in the plan at LINKTOPLAN\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://fit93a.db.files.1drv.com/y4mTOWSzX1AfIWx-VdUgY_Qp3wqebttT7FWSvtKK-zAbpTJuU560Qccv1_Z_Oxd4T4zUtd5oVZGJeS17fkgbl1dXUmvbldnGcoThL-bnQYxrTrMkrJS1Wz2ZRV5RVtZS9f4GleZQOMuWXP1HMYSjYxa6w09nEyGg1masI-wKIZfdnEF6L8r83Q9BB7yIjlp6OXEmccZt99gpb4Qti9sIFNxpg\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r}\relse {\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;The Job Succeeded\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;The Job Succeeded\u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;All is well - Please continue with the next step in the plan at LINKTOPLAN\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://6f0bzw.db.files.1drv.com/y4mvnTDG9bCgNWTZ-2_DFl4-ZsUwpD9QIHUArsGF66H69zBO8a--FlflXiF7lrL2H3vgya0ogXIDx59hn62wo2tt3HWMbqnnCSp8yPmM1IFNwZMzgvSZBEs_n9B0v4h4M5PfOY45GVSjeFh8md140gWHaFpZoL4Vwh-fD7Zi3djU_r0PduZwNBVGOcoB6SMJ1m4NmMmemWr2lzBn57LutDkxw\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$NotifyCommand = {\r$parameters = @{\r\u0026quot;URI\u0026quot; = $Using:webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $Using:TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\r}\r$Session = New-PSSession -ComputerName $NotifyServer\rInvoke-Command -Session $Session -ScriptBlock $NotifyCommand\r}\r$msg = 'ServerName = ' + $SQLInstance + 'JobId = ' + $JobID\rWrite-Host $msg\rNotify-TeamsSQLAgentJob -SQlInstance $SqlInstance -JobID $JobID -webhookurl $webhookurl\rThen it can be called in a SQL Agent job step, again following the guidelines at dbatools.io/agent\nIt is called slightly differently as you ned to pass in the SQL Agent tokens as parameters to the script\npowershell.exe -File path to Notify-TeamsSQLAgentJob.ps1 -SQLInstance $(ESCAPE_SQUOTE(SRVR)) -JobID $(ESCAPE_NONE(JOBID))\rSQL Agent Job Step Success and Failure We need to take another step to ensure that this works as expected. We have to change the On Failure action for each job step to the ‚ÄúGo To Notify Teams‚Äù step\nMaking people smile You can also add images (make sure the usage rights allow) so that the success notification can look like this\nand the failure looks like this\nHappy Automating !\nHere is the code that does not require remoting to another server to send the message\nParam(\r$SqlInstance,\r$JobID\r)\r$webhookurl = \u0026quot;https://outlook.office.com/webhook/5a8057cd-5e1a-4c84-9227-74a309f1c738@b122247e-1ebf-4b52-b309-c2aa7436fc6b/IncomingWebhook/affb85f05804438eb7ffb57665879248/f32fc7e6-a998-4670-8b33-635876559b80\u0026quot;\rfunction Notify-TeamsSQlAgentJob {\rParam(\r$SQLInstance,\r$JobID,\r$webhookurl\r)\r$SQLInstance = $SQLInstance # Import-Module 'C:\\Program Files\\WindowsPowerShell\\Modules\\dbatools\\1.0.107\\dbatools.psd1'\r[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\r$CharArray = $JobID.ToCharArray()\r$JobGUID = $CharArray[8] + $CharArray[9] + $CharArray[6] + $CharArray[7] + $CharArray[4] + $CharArray[5] + $CharArray[2] + $CharArray[3] + '-' + $CharArray[12] + $CharArray[13] + $CharArray[10] + $CharArray[11] + '-' + $CharArray[16] + $CharArray[17] + $CharArray[14] + $CharArray[15] + '-' + $CharArray[18] + $CharArray[19] + $CharArray[20] + $CharArray[21] + '-' + $CharArray[22] + $CharArray[23] + $CharArray[24] + $CharArray[25] + $CharArray[26] + $CharArray[27] + $CharArray[28] + $CharArray[29] + $CharArray[30] + $CharArray[31] + $CharArray[32] + $CharArray[33]\r$Job = Get-DbaAgentJob -SQlInstance $SQLInstance | Where jobid -eq $JobGuiD\r$JobName = $Job.Name\r$Jobsteps = Get-DbaAgentJobStep -SQlInstance $SQLInstance -Job $JobName\r$JobStepNames = $Jobsteps.Name -join ' , '\r$JobStartDate = $job.JobSteps[0].LastRunDate\r$JobStatus = $job.LastRunOutcome\r$lastjobstepid = $jobsteps[-1].id\r$Jobstepsmsg = $Jobsteps | Out-String\r$JobStepStatus = ($Jobsteps | Where-Object {$_.id -ne $lastjobstepid -and $_.LastRunDate -ge $JobStartDate} ).ForEach{\r\u0026quot; $($_.Name) - $($_.LastRunDate) **$($_.LastRunOutCome)** \u0026quot;\r} $Text = @\u0026quot;\r# **$SqlInstance** ## **$JobName** $jobstepMsg\rStarted at $JobStartDate - The individual Job Steps status was $JobStepStatus \u0026quot;@\rif (( $jobsteps | Where id -ne $lastjobstepid).LastRunOutcome -contains 'Failed') {\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;There was a Job Failure\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;The Job Failed\u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;Work to do - Please investigate the following job by following the steps in the plan at LINKTOPLAN\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://fit93a.db.files.1drv.com/y4mTOWSzX1AfIWx-VdUgY_Qp3wqebttT7FWSvtKK-zAbpTJuU560Qccv1_Z_Oxd4T4zUtd5oVZGJeS17fkgbl1dXUmvbldnGcoThL-bnQYxrTrMkrJS1Wz2ZRV5RVtZS9f4GleZQOMuWXP1HMYSjYxa6w09nEyGg1masI-wKIZfdnEF6L8r83Q9BB7yIjlp6OXEmccZt99gpb4Qti9sIFNxpg\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r}\relse {\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;The Job Succeeded\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;The Job Succeeded\u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;All is well - Please continue with the next step in the plan at LINKTOPLAN\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://6f0bzw.db.files.1drv.com/y4mvnTDG9bCgNWTZ-2_DFl4-ZsUwpD9QIHUArsGF66H69zBO8a--FlflXiF7lrL2H3vgya0ogXIDx59hn62wo2tt3HWMbqnnCSp8yPmM1IFNwZMzgvSZBEs_n9B0v4h4M5PfOY45GVSjeFh8md140gWHaFpZoL4Vwh-fD7Zi3djU_r0PduZwNBVGOcoB6SMJ1m4NmMmemWr2lzBn57LutDkxw\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\r}\r$msg = 'ServerName = ' + $SQLInstance + 'JobId = ' + $JobID\rWrite-Host $msg\rNotify-TeamsSQLAgentJob -SQlInstance $SqlInstance -JobID $JobID -webhookurl $webhookurl\r","date":"2020-07-29T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/07/image-18.png","permalink":"https://blog.robsewell.com/blog/notifying-a-teams-channel-of-a-sql-agent-job-result/","title":"Notifying a Teams Channel of a SQL Agent Job result"},{"content":"Microsoft Teams is fantastic for collaboration. It enables groups of people, teams if you like to be able to communicate, collaborate on documents, hold meetings and much much more.\nSQL Agent Job Overview Using dbatools we can create a simple script to gather the results of Agent Jobs form a list of instances. Maybe it would be good to be able to get the job runs results every 12 hours so that at 6am in the morning the early-bird DBA can quickly identify if there are any failures that need immediate action and at 6pm , the team can check that everything was ok before they clock off.\nHere is an example of such a script\n$SqlInstances = (Get-Vm -ComputerName BEARDNUC,BEARDNUC2).Where{$_.State -eq 'Running' -and $_.Name -like '*SQL*'}.Name\r$AllJobs = \u0026quot;\rSqlInstance...|...Total...|...Successful...|...FailedJobs...|...FailedSteps...|...Canceled... --------------------------------------------- \u0026quot;\rforeach ($Instance in $SQLInstances) {\rWrite-Host \u0026quot;Connecting to $instance\u0026quot;\rtry{\r$smo = Connect-DbaInstance $Instance -ErrorAction Stop\rWrite-Host \u0026quot;Connected successfully to $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed to connect to $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\rWrite-Host \u0026quot;Getting Agent Jobs on $instance\u0026quot;\rtry {\r$AgentJobs = Get-DbaAgentJobHistory -SqlInstance $smo -EnableException -StartDate $startdate Write-Host \u0026quot;Successfully got Agent Jobs on $instance\u0026quot;\r}\rcatch {\rWrite-Host \u0026quot;Failed to get agent jobs on $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r$jobs = $agentJobs $NumberOfJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}).Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobSteps = ($Jobs |Where-Object {$PSitem.StepId -ne 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfSuccessfulJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Succeeded'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfCanceledJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Canceled'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\rWrite-Host \u0026quot;SqlInstance $Instance - Number of Jobs $NumberOfJobs - Number of Successful Jobs $NumberOfSuccessfulJobs - Number of Failed Jobs $NumberOfFailedJobs\u0026quot;\r$AllJobs = $AllJobs + \u0026quot;$($Instance.Split('.')[0])..........\u0026lt;b\u0026gt;$NumberOfJobs\u0026lt;/b\u0026gt;................\u0026lt;b\u0026gt;$NumberOfSuccessfulJobs\u0026lt;/b\u0026gt;.........................\u0026lt;b\u0026gt;$NumberOfFailedJobs\u0026lt;/b\u0026gt;............................\u0026lt;b\u0026gt;$NumberOfFailedJobSteps\u0026lt;/b\u0026gt;..............................\u0026lt;b\u0026gt;$NumberOfCanceledJobs\u0026lt;/b\u0026gt;........\r\u0026quot;\rtry{\r$smo.ConnectionContext.Disconnect()\rWrite-Host \u0026quot;Disconnecting $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed disconnect from $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r}\rWrite-Host \u0026quot;Since $startdate\u0026quot;\rWrite-Host \u0026quot;$AllJobs\u0026quot;\rand an example of running it.\nCreate a Teams Channel If you have permissions, you can create a new Teams channel by clicking on the 3 ellipses and add channel\nThen fill in the blanks\nCreate a Webhook Connector for the channel Next, you need to have a connector for the channel, click on the 3 ellipses for the channel and click on connectors\nThen you can choose the Incoming Webhook connector and click configure\nGive the connector a name and upload an image if you wish and click create\nThe resulting screen will give you a URL that you can copy. If you need to find it again, then use the 3 ellipses again, click connectors and look at configured. You can then choose the webhook that you have created and click manage and you will find the URL.\nSend to Teams using PowerShell Now you can send a message to that Teams channel using PowerShell. You will need to add the webhook URL from your Teams connector\n[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\r$webhookurl = \u0026quot;\u0026quot;\r$Text = @\u0026quot;\r# Here is a Title\rand a message\rImage is from\rhttps://www.flickr.com/photos/157270154@N05/38494483572\rPhoto by CreditDebitPro\r\u0026quot;@\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;This is my summary\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;Something Important \u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;I have something to say\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://live.staticflickr.com/4568/38494483572_a98d623854_k.jpg\u0026quot;\r\u0026quot;text\u0026quot; = $text\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\rThe code above will send a message that looks like this\nRunning as a SQL Agent Job Now we can run this code as a SQL Agent Job and schedule it. Now, you may not be able to run that code on your SQL Server. It cannot connect to the internet, so how can we contact the Teams webhook?\nThere are probably a number of ways to do this but the solution that I took, was to allow a proxy account the ability to use PSRemoting and run the part of the script that connects to Teams on a different machine, that does have connectivity.\nThe script I used was as follows. You will need to add in the SQL Instances or better still dynamically gather them from your source of truth. You will need the webhook URL and the name of the server that can connect to Teams\n$SQLInstances = 'SQL2005Ser2003','SQL2008Ser12R2','SQL2014Ser12R2','SQL2016N1','SQL2016N2','SQL2016N3','SQL2017N5','SQL2019N20','SQL2019N21','SQL2019N22','SQL2019N5'\r$startdate = (Get-Date).AddHours(-12)\r$webhookurl = \u0026quot;\u0026quot;\r$NotifyServer = 'BeardNUC2'\r$AllJobs = \u0026quot;\rSqlInstance...|...Total...|...Successful...|...FailedJobs...|...FailedSteps...|...Canceled... --------------------------------------------- \u0026quot;\rforeach ($Instance in $SQLInstances) {\rWrite-Host \u0026quot;Connecting to $instance\u0026quot;\rtry{\r$smo = Connect-DbaInstance $Instance -ErrorAction Stop\rWrite-Host \u0026quot;Connected successfully to $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed to connect to $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\rWrite-Host \u0026quot;Getting Agent Jobs on $instance\u0026quot;\rtry {\r$AgentJobs = Get-DbaAgentJobHistory -SqlInstance $smo -EnableException -StartDate $startdate Write-Host \u0026quot;Successfully got Agent Jobs on $instance\u0026quot;\r}\rcatch {\rWrite-Host \u0026quot;Failed to get agent jobs on $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r$jobs = $agentJobs $NumberOfJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}).Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobSteps = ($Jobs |Where-Object {$PSitem.StepId -ne 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfSuccessfulJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Succeeded'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfCanceledJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Canceled'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\rWrite-Host \u0026quot;SqlInstance $Instance - Number of Jobs $NumberOfJobs - Number of Successful Jobs $NumberOfSuccessfulJobs - Number of Failed Jobs $NumberOfFailedJobs\u0026quot;\r$AllJobs = $AllJobs + \u0026quot;$($Instance.Split('.')[0])..........\u0026lt;b\u0026gt;$NumberOfJobs\u0026lt;/b\u0026gt;................\u0026lt;b\u0026gt;$NumberOfSuccessfulJobs\u0026lt;/b\u0026gt;.........................\u0026lt;b\u0026gt;$NumberOfFailedJobs\u0026lt;/b\u0026gt;............................\u0026lt;b\u0026gt;$NumberOfFailedJobSteps\u0026lt;/b\u0026gt;..............................\u0026lt;b\u0026gt;$NumberOfCanceledJobs\u0026lt;/b\u0026gt;........\r\u0026quot;\rtry{\r$smo.ConnectionContext.Disconnect()\rWrite-Host \u0026quot;Disconnecting $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed disconnect from $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r}\rWrite-Host \u0026quot;Since $startdate\u0026quot;\rWrite-Host \u0026quot;$AllJobs\u0026quot;\r$NotifyCommand = {\r[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\r$webhookurl = $Using:TeamsWebhook\r$allJobsMessage = $Using:AllJobs $Text = @\u0026quot;\r# Overview of SQL Agent Jobs in Production since $($Using:startdate) $allJobsMessage\r\u0026quot;@\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;Overview for the last 12 hours\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;Job Failures \u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;Overview for the last 12 hours since $($Using:startdate)\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://live.staticflickr.com/4568/38494483572_a98d623854_k.jpg\u0026quot;\r\u0026quot;text\u0026quot; = $allJobsMessage\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\r}\r$Session = New-PSSession -ComputerName $NotifyServer\rInvoke-Command -Session $Session -ScriptBlock $NotifyCommand\rThen, follow the steps at dbatools.io/agent to create an agent job to run the script above on an instance with the dbatools module available to the SQL Service account. Use or create a proxy with permissions on the notify server and create an Agent Job.\nUSE [msdb]\rGO\r/****** Object: Job [I am a Job that notifies Teams] Script Date: 27/07/2020 20:27:27 ******/\rBEGIN TRANSACTION\rDECLARE @ReturnCode INT\rSELECT @ReturnCode = 0\r/****** Object: JobCategory [[Uncategorized (Local)]] Script Date: 27/07/2020 20:27:28 ******/\rIF NOT EXISTS (SELECT name FROM msdb.dbo.syscategories WHERE name=N'[Uncategorized (Local)]' AND category_class=1)\rBEGIN\rEXEC @ReturnCode = msdb.dbo.sp_add_category @class=N'JOB', @type=N'LOCAL', @name=N'[Uncategorized (Local)]'\rIF (@@ERROR \u0026lt;\u0026gt; 0 OR @ReturnCode \u0026lt;\u0026gt; 0) GOTO QuitWithRollback\rEND\rDECLARE @jobId BINARY(16)\rEXEC @ReturnCode = msdb.dbo.sp_add_job @job_name=N'12 Hour Teams Notify', @enabled=1, @notify_level_eventlog=0, @notify_level_email=0, @notify_level_netsend=0, @notify_level_page=0, @delete_level=0, @description=N'This job will notify Teams every 12 hours', @category_name=N'[Uncategorized (Local)]', @owner_login_name=N'THEBEARD\\SQL_SVC', @job_id = @jobId OUTPUT\rIF (@@ERROR \u0026lt;\u0026gt; 0 OR @ReturnCode \u0026lt;\u0026gt; 0) GOTO QuitWithRollback\r/****** Object: Step [Notify Teams] Script Date: 27/07/2020 20:27:28 ******/\rEXEC @ReturnCode = msdb.dbo.sp_add_jobstep @job_id=@jobId, @step_name=N'Notify Teams', @step_id=1, @cmdexec_success_code=0, @on_success_action=1, @on_success_step_id=0, @on_fail_action=2, @on_fail_step_id=0, @retry_attempts=0, @retry_interval=0, @os_run_priority=0, @subsystem=N'CmdExec', @command=N'powershell.exe -File C:\\temp\\AgentJobs\\NotifyTeams.ps1', @flags=0, @proxy_name=N'TheBeardIsMighty'\rIF (@@ERROR \u0026lt;\u0026gt; 0 OR @ReturnCode \u0026lt;\u0026gt; 0) GOTO QuitWithRollback\rEXEC @ReturnCode = msdb.dbo.sp_update_job @job_id = @jobId, @start_step_id = 1\rIF (@@ERROR \u0026lt;\u0026gt; 0 OR @ReturnCode \u0026lt;\u0026gt; 0) GOTO QuitWithRollback\rEXEC @ReturnCode = msdb.dbo.sp_add_jobserver @job_id = @jobId, @server_name = N'(local)'\rIF (@@ERROR \u0026lt;\u0026gt; 0 OR @ReturnCode \u0026lt;\u0026gt; 0) GOTO QuitWithRollback\rCOMMIT TRANSACTION\rGOTO EndSave\rQuitWithRollback:\rIF (@@TRANCOUNT \u0026gt; 0) ROLLBACK TRANSACTION\rEndSave:\rGO\rWhen the job runs\nThe results are posted to the Teams Channel\nIf you can run the Agent Job on a machine that can connect to Teams and your SQL Instances then you can remove the need to use a remote session by using this code\n$SQLInstances = 'SQL2005Ser2003','SQL2008Ser12R2','SQL2014Ser12R2','SQL2016N1','SQL2016N2','SQL2016N3','SQL2017N5','SQL2019N20','SQL2019N21','SQL2019N22','SQL2019N5'\r$startdate = (Get-Date).AddHours(-12)\r$webhookurl = \u0026quot;\u0026quot;\r# Import-Module 'C:\\Program Files\\WindowsPowerShell\\Modules\\dbatools\\1.0.107\\dbatools.psd1'\r$AllJobs = \u0026quot;\rSqlInstance...|...Total...|...Successful...|...FailedJobs...|...FailedSteps...|...Canceled... --------------------------------------------- \u0026quot;\rforeach ($Instance in $SQLInstances) {\rWrite-Host \u0026quot;Connecting to $instance\u0026quot;\rtry{\r$smo = Connect-DbaInstance $Instance -ErrorAction Stop\rWrite-Host \u0026quot;Connected successfully to $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed to connect to $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\rWrite-Host \u0026quot;Getting Agent Jobs on $instance\u0026quot;\rtry {\r$AgentJobs = Get-DbaAgentJobHistory -SqlInstance $smo -EnableException -StartDate $startdate Write-Host \u0026quot;Successfully got Agent Jobs on $instance\u0026quot;\r}\rcatch {\rWrite-Host \u0026quot;Failed to get agent jobs on $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r$jobs = $agentJobs $NumberOfJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}).Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfFailedJobSteps = ($Jobs |Where-Object {$PSitem.StepId -ne 0}| Where-Object {$PSItem.Status -eq 'Failed'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfSuccessfulJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Succeeded'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\r$NumberOfCanceledJobs = ($Jobs |Where-Object {$PSitem.StepId -eq 0} | Where-Object {$PSItem.Status -eq 'Canceled'}).StepName.Count.ToString(\u0026quot;00\u0026quot;)\rWrite-Host \u0026quot;SqlInstance $Instance - Number of Jobs $NumberOfJobs - Number of Successful Jobs $NumberOfSuccessfulJobs - Number of Failed Jobs $NumberOfFailedJobs\u0026quot;\r$AllJobs = $AllJobs + \u0026quot;$($Instance.Split('.')[0])..........\u0026lt;b\u0026gt;$NumberOfJobs\u0026lt;/b\u0026gt;................\u0026lt;b\u0026gt;$NumberOfSuccessfulJobs\u0026lt;/b\u0026gt;.........................\u0026lt;b\u0026gt;$NumberOfFailedJobs\u0026lt;/b\u0026gt;............................\u0026lt;b\u0026gt;$NumberOfFailedJobSteps\u0026lt;/b\u0026gt;..............................\u0026lt;b\u0026gt;$NumberOfCanceledJobs\u0026lt;/b\u0026gt;........\r\u0026quot;\rtry{\r$smo.ConnectionContext.Disconnect()\rWrite-Host \u0026quot;Disconnecting $instance\u0026quot;\r}\rcatch{\rWrite-Host \u0026quot;Failed disconnect from $Instance\u0026quot; $errorMessage = $_ | Out-String\rWrite-Host $errorMessage\rContinue\r}\r}\rWrite-Host \u0026quot;Since $startdate\u0026quot;\rWrite-Host \u0026quot;$AllJobs\u0026quot;\r[System.Net.ServicePointManager]::ServerCertificateValidationCallback = { $true }\r[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\r$allJobsMessage = $AllJobs $Text = @\u0026quot;\r# Overview of SQL Agent Jobs in Production since $($startdate) $allJobsMessage\r\u0026quot;@\r$JSONBody = [PSCustomObject][Ordered]@{\r\u0026quot;@type\u0026quot; = \u0026quot;MessageCard\u0026quot;\r\u0026quot;@context\u0026quot; = \u0026quot;http://schema.org/extensions\u0026quot;\r\u0026quot;summary\u0026quot; = \u0026quot;Overview for the last 12 hours\u0026quot;\r\u0026quot;themeColor\u0026quot; = '0078D7'\r\u0026quot;sections\u0026quot; = @(\r@{\r\u0026quot;activityTitle\u0026quot; = \u0026quot;Job Results \u0026quot;\r\u0026quot;activitySubtitle\u0026quot; = \u0026quot;Overview for the last 12 hours since $($startdate)\u0026quot;\r\u0026quot;activityImage\u0026quot; = \u0026quot;https://live.staticflickr.com/4568/38494483572_a98d623854_k.jpg\u0026quot;\r\u0026quot;text\u0026quot; = $allJobsMessage\r\u0026quot;markdown\u0026quot; = $true\r}\r)\r}\r$TeamMessageBody = ConvertTo-Json $JSONBody -Depth 100\r$parameters = @{\r\u0026quot;URI\u0026quot; = $webhookurl\r\u0026quot;Method\u0026quot; = 'POST'\r\u0026quot;Body\u0026quot; = $TeamMessageBody\r\u0026quot;ContentType\u0026quot; = 'application/json'\r}\rInvoke-RestMethod @parameters\rHappy automating!\n","date":"2020-07-28T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/07/image-11.png","permalink":"https://blog.robsewell.com/blog/sending-a-sql-agent-job-results-overview-to-a-microsoft-teams-channel/","title":"Sending a SQL Agent Job results overview to a Microsoft Teams Channel"},{"content":"Following on from my last post about the Secret Management module. I was asked another question.\nCan I use this to run applications as my admin account?\nA user with a beard\nIt is good practice to not log into your work station with an account with admin privileges. In many shops, you will need to open applications that can do administration tasks with another set of account credentials.\nUnfortunately, people being people, they will often store their admin account credentials in a less than ideal manner (OneNote, Notepad ++ etc) to make it easier for them, so that when they right click and run as a different user, they can copy and paste the password.\nUse the Secret Management module Again, I decided to use a notebook to show this as it is a fantastic way to share code and results and because it means that anyone can try it out.\nThe notebook may not render on a mobile device.\nUsing the notebook, I can quickly store my admin password safely and open and run the applications using the credential\n","date":"2020-07-20T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/07/runas.png","permalink":"https://blog.robsewell.com/blog/using-secret-management-module-to-run-ssms-vs-code-and-azure-data-studio-as-another-user/","title":"Using Secret Management module to run SSMS, VS Code and Azure Data Studio as another user"},{"content":"Don‚Äôt want to read all this? There are two dotnet interactive notebooks here with the relevant information for you to use.\nhttps://beard.media/dotnetnotebooks\nJaap is awesome I have to start here. For the longest time, whenever anyone has asked me how I store my credentials for use in my demos and labs I have always referred them to Jaap Brassers t blog post\nhttps://www.jaapbrasser.com/quickly-and-securely-storing-your-credentials-powershell/\nJoel is also awesome! When people wanted a method of storing credentials that didn\u0026rsquo;t involve files on disk I would suggest Joel Bennett‚Äôs t module BetterCredentials which uses the Windows Credential Manager\nhttps://www.powershellgallery.com/packages/BetterCredentials/4.5\nMicrosoft? Also awesome! In February, Microsoft released the SecretManagement module for preview.\nhttps://devblogs.microsoft.com/powershell/secrets-management-development-release/\nSydney t gave a presentation at the European PowerShell Conference which you can watch on Youtube.\nGood Bye Import-CliXML So now I say, it is time to stop using Import-Clixml for storing secrets and use the Microsoft.PowerShell.SecretsManagement module instead for storing your secrets.\nNotebooks are as good as blog posts I love notebooks and to show some people who had asked about storing secrets, I have created some. So, because I am efficient lazy I have embedded them here for you to see. You can find them in my Jupyter Notebook repository\nhttps://beard.media/dotnetnotebooks\nin the Secrets folder\nInstalling and using the Secrets Management Module These notebooks may not display on a mobile device unfortunately\nUsing the Secret Management Module in your scripts Here is a simple example of using the module to provide the credential for a docker container and then to dbatools to query the container\nThese notebooks may not display on a mobile device unfortunately\n","date":"2020-07-18T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/07/image-1.png","permalink":"https://blog.robsewell.com/blog/good-bye-import-clixml-use-the-secrets-management-module-for-your-labs-and-demos/","title":"Good Bye Import-CliXML ‚Äì Use the Secrets Management module for your labs and demos"},{"content":"I have always been extremely proud to be a Cloud and Datacenter Management MVP, and lucky enough to be involved with both the PowerShell community as well as the Data Platform community.\nToday, July 1st is the date that many MVPs receive their renewal email to let them know that they have been awarded for another year. There is a lot of F5‚Äôing and frequent checking of emails and ‚ÄúHave you heard yet?‚Äù DMs going around.\nWhen I received the news, I was using Azure DevOps to run PowerShell and Terraform to build an Azure SQL Elastic Pool (yes, I will write a blog post about it!). I love technology and within my work, like many people, I work across many different disciplines. Azure, Azure DevOps, SQL Server and Microsoft Data Platform products are the main focus of my time.\nI didn‚Äôt notice the significance of the information.\nI was pleased as punch to be renewed again, proud that what I do is recognised by Microsoft, honoured to spend another year as an MVP. Then my friends pointed out the big news that I had missed.\nPhoto by pixpoetry on Unsplash\nThere are two award categories.\nI have been awarded for both Cloud and Datacenter Management and Data Platform.\nI am beyond words.\nProud, Surprised and Honoured.\nThank you to all of the people who help and support me. You help more than you will ever know.\nI am going to go and prop my jaw shut!\n","date":"2020-07-01T00:00:00Z","permalink":"https://blog.robsewell.com/blog/surprised-and-honoured-and-proud/","title":"Surprised and Honoured and Proud"},{"content":"Azure Data Studio is a great tool for connecting with your data platform whether it is in Azure or on your hardware. Jupyter Notebooks are fantastic, you can have words, pictures, code and code results all saved in one document.\nI have created a repository in my GitHub https://beard.media/Notebooks where I have stored a number of Jupyter notebooks both for Azure Data Studio and the new .NET interactive notebooks.\nAnother thing that you can do with notebooks is run them as Agent Jobs and save the results of the run.\nNotebooks running T-SQL This works easily for T-SQL notebooks. I am going to use this one that I created that uses T-SQL to gather permissions using old code that was in a share somewhere. We can run the notebook and get the permissions and save the notebook and the results will be available for all time (unless you delete the notebook!)\nSQL Agent Extension in Azure Data Studio In Azure Data Studio, if you press CTRL + SHIFT + X it will open the Extensions tab\nYou can add extra functionality to Azure Data Studio. Search in the top bar for Agent and press the install button to install the extension. You can connect to and instance in the connections tab (CTRL + SHIFT + D) and right click on it and click Manage. This will open up the server dashboard (why isn‚Äôt it instance dashboard?)\nand you will also have the SQL Agent dashboard available\nIts pretty neat, it has green and red bars against the jobs showing success or failure and the larger the bar the longer the run time. On the left you will see a book. Click that\nNotebooks in Agent Jobs You can create an Agent Job to run a notebook. As a notebook is just a json file, it can be stored in a database table. This interface will create two tables one to store the templates and one for the results. Click New Notebook Job\nThen navigate to the notebook and select it.\nChoose a database for the storage of the template and the results and one for the execution context.\nThe name of the job will be the file name of the notebook. You can change this but there is a bug where you can only enter one character at a time in the name before it changes focus so beware!\nOnce the job is created, you will see two tables in the storage database notebooks.nb_materialized and notebooks.nb_template\nThe materialised table is empty right now\nbut the template table has a row for the job which includes the notebook in json format.\nIf you click on the jobs in the Notebook Jobs window in the SQL Agent extension, you can see more information about the job run\nYou can also run the job from here. It doesn‚Äôt have to be run from here, it is just a normal agent job which you can run or schedule in any normal manner. Running it from here gives a pop-up\nYou have to refresh to see when the job is finished and it will be red if the job failed, green if it succeeded or orange if some cells failed like this!\nBut this is the good bit. Clicking on that icon will open the notebook that was created by that agent job run. Lets see what we get\nYou can see that we have the results of the queries that we wrote in the notebook alongside the documentation (or maybe explanation of the expected results)\nIf we scroll down a little (and change the theme colour so that you can see the error)\nMsg , Level , State , Line Duplicate column names are not permitted in SQL PowerShell. To repeat a column, use a column alias for the duplicate column in the format Column_Name AS New_Name.\nWe have got an error from running the code via SQL PowerShell which is how the job is run. This error is also inserted into the notebooks.nb_template table\nI edited the notebook locally to remove that block of code\nThen edited the job and selected the updated notebook\nand re-ran the job and got a green tick.\nNow I can open the notebook from the latest run, but notice that from this view I can also open the previous notebook.\nIf I look in the nb_template table, the last_run_notebook_error has cleared\nand if I look in the nb materialized table I can see two rows, one for each job run. The error from the first run is also stored in this table. The notebook column has the json for the notebook if you wish to access it in a different manner.\nTomorrow, we will see what the job steps look like and how to make this run on an instance which does not and cannot have the required PowerShell.\nSpoiler Alert ‚Äì May contain dbatools üôÇ\n","date":"2020-03-30T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/03/image-22.png","permalink":"https://blog.robsewell.com/blog/running-jupyter-notebooks-as-agent-jobs/","title":"Running Jupyter Notebooks as Agent Jobs"},{"content":"My last post had a lot of information about the new .NET PowerShell notebooks including installation instructions.\n.NET Notebooks are Jupyter Notebooks that use .NET core to enable C#, F# and PowerShell kernels.\nUse Cases One of the main benefits that I see for Jupyter Notebooks for Ops folk is that the results of the query are saved with the notebook. This makes them fantastic for Incident resolution.\nIf you have an incident at 3am and you know that you will need that information in the wash up meeting the next day instead of copying and pasting results into a OneNote document or a text file, you can simply run the queries in a notebook and save it.\nIn the meeting, you can simply open the notebook and the results will be available for everyone to see.\nEven better, if you have a template notebook for those scenarios and you can then compare them to previous occurrences.\nUsing Pester Using Pester to validate that an environment is as you expect it is a good resource for incident resolution, potentially enabling you to quickly establish an area to concentrate on for the issue. However, if you try to run Pester in a .NET Notebook you will receive an error\n| ^ The term 'Get-CimInstance' is not recognized as the name of a\r| cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included,\r| verify that the path is correct and try again.\rImport-Module: The module to process \u0026lsquo;Pester.psm1\u0026rsquo;, listed in field \u0026lsquo;ModuleToProcess/RootModule\u0026rsquo; of module manifest \u0026lsquo;C:\\Users\\mrrob\\Documents\\PowerShell\\Modules\\Pester\\4.9.0\\Pester.psd1\u0026rsquo; was not processed because no valid module was found in any module directory. Thats odd, why is it failing there? Dongbo Wang from the PowerShell team explains in the issue that I raised\nYes, it was the CimCmdlets module from the system32 module path that got imported (via the¬†WinCompat¬†feature added in PS7). This is because currently the PS kernel don‚Äôt ship all the built-in modules along with it ‚Ä¶ The built-in modules are not published anywhere and are platform specific, it‚Äôs hard for an application that host powershell to ship them along. We have the issue¬†PowerShell/PowerShell#11783¬†to track this work.\nYou can see all of this including all the results in this notebook that I have created and shared on GitHub and also below as a gist to embed in this blog post\nSharing Code AND Results üôÇ Notebooks ‚Äì A brilliant way of sharing what you did and the results that you got enabling others to follow along. You can do this with this Notebook. Download it and open it in your Jupyter Lab and you will be able to run it and see all of the errors and the fix on your machine.\n","date":"2020-02-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/02/image-16.png","permalink":"https://blog.robsewell.com/blog/net-powershell-notebooks-using-pester/","title":".NET PowerShell Notebooks ‚Äì Using Pester"},{"content":"I am sat in the PowerShell Saturday in Hamburg. You can see me on the right of this picture writing my previous blog post!\n@JanDamaschke spricht √ºber Asynchrones Logging in #powershell mit Classes und Runspaces (https://twitter.com/hhpsug?ref_src=twsrc%5Etfw) #pssaturday\n‚Äî Christoph Burmeister (@chrburmeister) February 22, 2020\nI was talking with my friend Mathias Jessen @IISResetMe on Twitter about notebooks and he said that another great use case was to use them on Stack OverFlow\nNow Mathias is an active answerer on Stack OverFlow\nand he puts a lot of effort into writing his answers, formatting them, including code and results. Basically exactly the same as a Notebook. However, with a Notebook, you can enable people to run the code as well on their own machines.\nMathias says he will use notebooks to help people when he answers their PowerShell questions on Stack OverFlow. If you are a Stack OverFlow Answerer then you can too.\n","date":"2020-02-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/02/image-16.png","permalink":"https://blog.robsewell.com/blog/use-jupyter-notebooks-to-help-people-on-stackoverflow/","title":"Use Jupyter Notebooks to Help People on StackOverFlow"},{"content":"Data Science folk used Notebooks for documentation and to show re-runnable research. Azure Data Studio included this notebook functionality and added SQL kernel where with a little bit of faffing you could run PowerShell and then a Python kernel that enabled PowerShell. It seems that notebooks are so cool that everyone is creating them these days! I was browsing twitter when I saw this tweet.\n.NET Notebooks Preview 2 is here! Preview 2 includes üéâ@PowerShell_Team, @nteractio, and a new tool. Check out our blog to learn more. Congratulations to @jonsequitur @colombod and our entire teamhttps://t.co/WqNWQWR3Bo@dotnet #jupyter #PowerShell #interactiveprogramming.\n‚Äî Maria Naggaga (@LadyNaggaga) February 6, 2020\nPowerShell 7 Notebooks üôÇ A notebook experience for PowerShell 7 that sounds amazing. This will enable a true cross-platform PowerShell Notebook experience which is lacking from the Python version as it uses Windows PowerShell on Windows and PowerShell Core on other OS‚Äôs\nThe first thing I asked was ‚Äì Will this come to Azure Data Studio. I got an immediate response from Sydney Smith PowerShell Program Manager saying it is on the roadmap\nMoving this kernel into ADS is on our roadmap! Right now our kernel uses hosted pwsh 7 but we would love to know if you have scenarios that don\u0026rsquo;t work with 7\n‚Äî Sydney Smith (@sydneysmithreal) February 6, 2020\nInstall dependencies To be able to run the notebook, you need to install some dependencies. First install the .NET CORE SDK which you can download from https://dotnet.microsoft.com/download This needs admin permissions to install.\nYou also need a Python installation ‚Äì You can use Anaconda, which you can download from here https://www.anaconda.com/distribution/ This does not need admin to install\nAdd Anaconda to Windows Terminal I have added the Anaconda prompt to Windows Terminal so that I have one entry point for all my CLIs. Open the settings file and add the code below. (It will also give you an icon and background.\n{\r// Make changes here to the Anaconda.exe profile\r\u0026quot;guid\u0026quot;: \u0026quot;{0caa0dad-35be-5f56-a7ff-afceeeaa6101}\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;Anaconda\u0026quot;,\r\u0026quot;commandline\u0026quot;: \u0026quot;cmd.exe /K C:\\\\Users\\\\mrrob\\\\Anaconda3\\\\Scripts\\\\activate.bat\u0026quot;,\r\u0026quot;hidden\u0026quot;: false,\r\u0026quot;backgroundImage\u0026quot;: \u0026quot;C:\\\\Users\\\\mrrob\\\\Anaconda3\\\\Menu\\\\anaconda-navigator.ico\u0026quot;,\r\u0026quot;icon\u0026quot;: \u0026quot;C:\\\\Users\\\\mrrob\\\\Anaconda3\\\\Menu\\\\anaconda-navigator.ico\u0026quot;,\r\u0026quot;backgroundImageAlignment\u0026quot;: \u0026quot;topRight\u0026quot;,\r\u0026quot;backgroundImageStretchMode\u0026quot;: \u0026quot;uniform\u0026quot;,\r\u0026quot;backgroundImageOpacity\u0026quot;: 0.1\r}\rand it appears in the drop down\nWith Anaconda installed, check that that the kernel is available on your path. If like me you have Azure Data Studio installed, you will have additional kernels but the important one line here is\npython3 C:\\Users\\USERNAME\\Anaconda3\\share\\jupyter\\kernels\\python3\nIn Windows Terminal move to a PowerShell 7 prompt and install the dotnet interactive tool\ndotnet tool install --global Microsoft.dotnet-interactive\rThen you can install the .NET kernel in your Anaconda prompt using this command\ndotnet interactive jupyter install\rSometimes new things have errors I had an error when I tried this first time\nCould not execute because the specified command or file was not found. Possible reasons for this include: * You misspelled a built-in dotnet command. * You intended to execute a .NET Core program, but dotnet-interactive does not exist. * You intended to run a global tool, but a dotnet-prefixed executable with this name could not be found on the PATH.\nThis is easily fixed by adding %USERPROFILE%\\.dotnet\\tools to my path with set PATH=%PATH%;%USERPROFILE%\\.dotnet\\tools\nRunning jupyter kernelspec list shows that the .NET kernel is installed for C Sharp, F Sharp and .NET PowerShell\nLets open a Notebook Now you want to play with it! You can run the lab environment using `jupyter lab`\nThis opens a browser\nYou can open existing Azure Data Studio PowerShell notebooks (but not SQL ones)\nSometimes new things have errors Part 2 Unfortunately, I get errors when trying to import Pester which means I can not use my dbachecks notebooks in this blog post. I have raised an issue on the repo here.\nCreate a New Notebook But it is easy to create a new Notebook\nIn the launcher page click the .NET PowerShell button\nWhich will open a new Notebook in the directory that you launched the lab from. You can then add Code or Markdown as I have described before here.\nThen you can add code, markdown and images to create your notebook.\nOnce you have finished using the notebook lab, you can shut it down in the Anaconda prompt with CTRL + C\nHere is a video of running a notebook which anyone can use to create a couple of Docker containers running SQL 2019 and query them with dbatools. You can find the notebook further down this post.\nSharing Notebooks You can create notebooks to run common tasks. Even better, from the lab you can convert the notebook including the results to a variety of formats to share with other none-technical people. I used this functionality this week to export Azure Data Studio Notebooks to HTML and PDF for a Project manager üôÇ\nYou can find the Export Notebook command in the File menu\nExporting to HTML did not export the images but it does include the results\nYou can share notebooks via GitHub ‚Äì Either in a gist like this\nor by providing a straight link to the notebook in GitHub https://github.com/SQLDBAWithABeard/Notebooks/blob/master/notebooks/Exploring%20dbatools.ipynb\nYou can also use Binder https://mybinder.org/\nThis uses Docker to create an interactive Notebook. Create a GitHub repo like https://github.com/SQLDBAWithABeard/Notebooks (or just clone it) Copy your notebooks into the notebooks folder and push the changes to GitHub and then go to https://mybinder.org/ and add your URL to the repository.\nYou can see what it looks like by clicking the button below which Binder creates for you\nUnfortunately the kernel only supports Python for the moment but you can see the possibilities üôÇ\n","date":"2020-02-07T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2020/02/image-13.png","permalink":"https://blog.robsewell.com/blog/new-net-notebooks-are-here-powershell-7-notebooks-are-here/","title":"New .NET Notebooks are here ‚Äì PowerShell 7 notebooks are here."},{"content":"I enjoy maintaining open source GitHub repositories such as dbachecks and ADSNotebook. I absolutely love it when people add more functionality to them.\nTo collaborate with a repository in GitHub you need to follow these steps\nFork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes Make some changes and commit them with useful messages Push the changes to your repository Create a Pull Request from your repository back to the original one You will need to have git.exe available which you can download and install from https://git-scm.com/downloads if required\nFork the repository into your own GitHub A fork is a copy of the original repository. This allows you to make changes without affecting the original project. It does not get updated when the original project gets updated (We will talk about that in the next post) This enables you to code a new feature or a bug fix, test it locally and make sure it is working.\nLet‚Äôs take dbachecks as our example. Start by going to the project in GiHub. In this case the URL is https://github.com/sqlcollaborative/dbachecks You will see a Fork button at the top right of the page\nWhen you click the button the repository is copied into your own GitHub account\nThe page will open at https://github.com/YOURGITHUBUSERNAME/NameOfRepository in this case https://github.com/SQLDBAWithABeard/dbachecks You will be able to see that it is a fork of the original repository at the top of the page\nClone the repository to your local machine Forking the repository has created a remote repository stored on the GitHub servers. Now that the repository has been forked you need to clone it to your local machine to create a local repository so that you can start coding your amazing fix. When you have finished you can then sync it back to your remote repository ready for a Pull Request back to the original repository.\nIn your browser, at your remote repository that you just created (https://github.com/YOURGITHUBUSERNAME/NameOfRepository if you have closed the page) click on Clone or Download and then the icon to the right to copy the url\nYou can clone your repository in VS Code or Azure Data Studio by clicking F1 or CTRL + SHIFT + P in Windows or Linux and ‚áß‚åòP or F1 on a Mac\nthen start typing clone until you see Git:Clone and press enter or click\nPaste in the URL that you just copied and click enter. A dialog will open asking you to select a folder. This is the parent directory where your local repository will be created. The clone will create a directory for your repository so you do not need to. I suggest that you use a folder called GitHub or something similar to place all of the repositories that you are going to clone and create.\nWhen it has finished it will ask you if you wish to open the repository\nif you click Open it will close anything that you have already got opened and open the folder. If you click Add to Workspace it will add the folder to the workspace and leave everything you already had open as it was and surprisingly clicking Open in New Window will open the folder in a new instance of Visual Studio Code or Azure Data Studio!\nand you will also be able to see the local repository files on your computer\nYou can clone the repository at the command line if you wish by navigating to your local GitHub directory and running git clone TheURLYouCopied\nNow your local repository has been created, it‚Äôs time to do your magic coding.\nCreate a new branch for your changes It is a good idea to create a branch for your amazing new feature This enables you to work on coding for that feature in isolation. It has the added advantage that if you mess it right royally up, you can just delete that branch and start again with a new one!\nTo create a branch in VS Code or Azure Data Studio you can click on the branch name at the bottom left.\nOr open the Command Palette and type Branch until you see Git: Create Branch\nYou will be prompted for a branch name\nI like to choose a name that relates to the code that I am writing like configurable_engine or removeerroringexample You can see the name of the branch in the bottom left so that you always know which branch you are working on.\nThe icon shows that the branch is only local and hasn‚Äôt been pushed (published) to the remote repository yet\nMake some changes and commit them with useful messages Now you can start writing your code for your awesome new feature, bug fix or maybe just documentation improvement. Keep your commits small and give them useful commit messages that explain why you have made the change as the diff tooling will be able to show what change you have made\nWrite your code or change the documentation, save the file and in Visual Studio Code or Azure Data Studio you will see that the source control icon has a number on it\nClicking on the icon will show the files that have changes ready\nYou can write your commit message in the box and click CTRL + ENTER to commit your changes with a message\nIf you want to do this at the command line, you can use git status to see which files have changes\nYou will need to git add .or git add .\\pathtofile to stage your changes ready for committing and then git commit -m 'Commit Message' to commit them\nNotice that I did exactly what I just said not to do! A better commit message would have been So that people can find the guide to forking and creating a PR\nPush the changes to your repository You only have the changes that you have made in your local repository on your computer. Now you need to push those changes to GitHub your remote repository. You can click on the publish icon\nYou will get a pop-up asking you if you wish to stage your changes. I click Yes and never Always so that I can use this prompt as a sanity check that I am doing the right thing\nAt the command line you can push the branch, if you do that, you will have to tell git where the branch needs to go. If you just type git push it will helpfully tell you\nfatal: The current branch AwesomeNewFeature has no upstream branch.\rTo push the current branch and set the remote as upstream, use\rgit push --set-upstream origin AwesomeNewFeature\rSo you will need to use that command\nYou can see in the bottom left that the icon has changed\nand if you read the output of the git push command you will see what the next step is also.\nCreate a Pull Request from your repository back to the original one You can CTRL click the link in the git push output if you have pushed from the command line or if you visit either you repository or the original repository in your browser you will see that there is a Compare and Pull Request button\nYou click that and let GitHub do its magic\nand it will create a Pull Request for you ready for you to fill in the required information, ask for reviewers and other options. Once you have done that you can click Create pull request and wait for the project maintainer to review it and (hopefully) accept it into their project\nYou can find the Pull Request that I created here https://github.com/sqlcollaborative/dbachecks/pull/720 and see how the rest of this blog post was created.\nIf you make more changes to the code in the same branch in your local repository and push them, they will automatically be added to this Pull Request whilst it is open. You can do this if the maintainer or reviewer asks for changes.\nShane has asked for a change\nSo I can go to my local repository in Azure Data Studio and make the requested change and save the file. If I look in the source control in Azure Data Studio I can again see there is a change waiting to be committed and if I click on the name of the file I can open the diff tool to see what the change was\nOnce I am happy with my change I can commit it again in the same way as before either in the editor or at the command line. The icon at the bottom will change to show that I have one commit in my local repository waiting to be pushed\nTo do the same thing at the command line I can type git status and see the same thing.\nI can then push my change to my remote repository either in the GUI or by using git push\nand it will automatically be added to the Pull Request as you can see\nNow that the required changes for the review have been made, the review has been approved by Shane and the pull request is now ready to be merged. (You can also see that dbachecks runs some checks against the code when a Pull Request is made)\nMany, many thanks to Shane b | t who helped with the writing of this post even whilst on a ‚Äúno tech‚Äù holiday.\nGo Ahead ‚Äì Contribute to an Open Source Project Hopefully you can now see how easy it is to create a fork of a GitHub repository, clone it to your own machine and contribute. There are many open source projects that you can contribute to.\nYou can use this process to contribute to the Microsoft Docs for example by clicking on the edit button on any page.\nYou can contribute other open source projects like\nPowerShell by Microsoft tigertoolbox by Microsoft Tiger Team dbatools dbachecks ADSNotebook PSDatabaseClone OpenQueryStore by William Durkin and Enrico van de Laar sqlwatch by Marcin Gminski SQLCop by Redgate sp_whoisactive by Adam Machanic sql-server-maintenance-solution by Ola Hallengren SQL-Server-First-Responder-Kit by Brent Ozar Unlimited Pester ReportingServicesTools or go and find the the ones that you use and can help with.\n","date":"2019-11-29T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/11/CreatePR.png","permalink":"https://blog.robsewell.com/blog/how-to-fork-a-github-repository-and-contribute-to-an-open-source-project/","title":"How to fork a GitHub repository and contribute to an open source project"},{"content":"Fixing the Failed to generate the compressed file for module C:\\Program Files\\dotnet\\dotnet.exe error when deploying to the PowerShell Gallery using Azure DevOps The PowerShell module for validating your SQL Server estate dbachecks is deployed via Azure DevOps, you can see how it is working (or not) via this link\nGrrr Automation for the Lose! Until recently, this had worked successfully. In the last few weeks I have been receiving errors\nException : Microsoft.PowerShell.Commands.WriteErrorException: Failed to generate the compressed file for module 'C:\\Program Files\\dotnet\\dotnet.exe failed to pack: error\rC:\\Program Files\\dotnet\\sdk\\3.0.100\\Sdks\\NuGet.Build.Tasks.Pack\\build\\NuGet.Build.Tasks.Pack.targets(198,5): error : 2 Index was outside the bounds of the array. [C:\\Users\\VssAdministrator\\AppData\\Local\\Temp\\cbc14ba6-5832-46fd-be89-04bb552a83ac\\Temp.csproj]\r'.\rAt C:\\Program Files\\WindowsPowerShell\\Modules\\PowerShellGet\\2.2.1\\PSModule.psm1:10944 char:17\r20 Publish-PSArtifactUtility @PublishPSArtifactUtility_Param ...\r~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r+ CategoryInfo : InvalidOperation: (:) [Write-Error], WriteErrorException\r2019-11-25T22:44:46.8459493Z + FullyQualifiedErrorId : FailedToCreateCompressedModule,Publish-PSArtifactUtility\rYou can see these errors in the release pipeline logs here\nConfusion This was very frustrating as it was stopping the continuous delivery to the PowerShell Gallery. It was even more confusing as I was successfully deploying the ADSNotebook module to the gallery using the same method as you can see here.\nRaise an Issue on GitHub I went and looked at the PowerShellGet GitHub repository and opened an issue I also found another issue regarding Required Modules\nBut this doesn\u0026rsquo;t help to get dbachecks released.\nJust Try to Make it Work I asked the wonderful folk in the PowerShell Slack channel ‚Äì Through the magic of automation, you can also interact with them via the powershellhelp channel in the SQL Server Slack as well but there were no answers that could assist.\nSo I had to go searching for an answer. PowerShellGet uses nuget for package management. I found that if I downloaded an earlier version and placed it in my user profile (in the right location) I could publish the module.\nI found this out by removing the nuget.exe from anywhere useful on the machine and trying to publish the module. The error message says\nNuGet.exe upgrade is required to continue\rThis version of PowerShellGet requires minimum version '4.1.0' of NuGet.exe to publish an item to the NuGet-based repositories. NuGet.exe must be available in 'C:\\ProgramData\\Microsoft\\Windows\\PowerShell\\PowerShellGet\\' or 'C:\\Users\\BeardyMcBeardFace\\AppData\\Local\\Microsoft\\Windows\\PowerShell\\PowerShellGet\\', or under one of the paths specified in PATH environment variable value. NuGet.exe can be downloaded from https://aka.ms/psget-nugetexe. For more information, see https://aka.ms/installing-powershellget . Do you want PowerShellGet to upgrade to the latest version of NuGet.exe now?\rIf I said yes then I got the latest version and the error continued.\nHowever, on my laptop I can go to the nuget downloads page and download an earlier version and place it in one of those paths then I could publish the module.\nCan I Automate it? I would rather not have to deploy manually though, and as I use hosted agents my access to the operating system is limited so I wondered if I could place the nuget.exe in the user profile and it would get used or if it would look for the the latest one. Turns out it uses the one in the user profile üôÇ\nSo now I have this code as a step in my Azure DevOps Release pipeline before calling Publish-Module and we have automated the releases again.\nand now deployments to the PowerShell Gallery are just triggered by the build and the pipeline is green again üôÇ\ntest\n","date":"2019-11-26T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/11/image-40.png","permalink":"https://blog.robsewell.com/blog/fixing-the-failed-to-generate-the-compressed-file-for-module-dotnet.exe-error-when-deploying-to-the-powershell-gallery-using-azure-devops/","title":"Fixing the Failed to generate the compressed file for module dotnet.exe error when deploying to the PowerShell Gallery using Azure DevOps"},{"content":"Now that Azure Data Studio has PowerShell Notebooks and there is a PowerShell Module for creating notebooks. I have been asked, more than once, what is the point? What is the use case? How does this help. I hope that this post will spark some ideas of one particular use-case.\nI showed my silly example PowerShell code to create a PowerShell Notebook that created a PowerShell Notebook to my good friend Nick.\nThanks Nick.\nThe Use Case The use case that Nick has is that he is converting some troubleshooting runbooks from their original locations (you know the sort of places ‚Äì Sharepoint Docs, OneNote Notebooks, Shared Folders, the desktop of the Bastion Host) into a single repository of Azure Data Studio SQL or PowerShell Notebooks.\nThe idea is to have a single entry point into the troubleshooting steps and for the on-call DBA to create a Notebook from a template for the issue at hand which could be attached to an incident in the incident management solution. I suppose you could call it an Index Notebook.\nWork Flow When the DBA (or another team) opens this Notebook, they can choose the task that they are going to perform and click the link which will\ncopy the Notebook to the local machine Rename the Notebook with the username and date Open it ready for the work. Once the work has been completed, the DBA can then attach the Notebook to the task or incident that has been created or use it in the Wash-Up/ Post Incident meeting.\nThis ensures that the original template notebook stays intact and unchanged and it is easy (which is always good when you are called out at 3am!) to create a uniquely named notebook .\nAzure DevOps Nick has placed this code into the deploy step in Azure DevOps which will deploy the template Notebooks from source control into the common folder and then this code will dynamically create the index Notebook each time there is a release.\nWhilst the initial use case is incident response, this could easily be adapted for Notebooks used for Common Tasks or Run Books.\nNotebooks There are a number of Notebooks for different issue stored in directories. For this post, I have used the Notebooks from Microsoft that explain SQL 2019 features and troubleshooting which you can find in their GitHub repositories by following this link\nThe Azure DevOps deploys the Notebooks to a directory which then looks something like this\nSome directories of Notebooks in a directory\nCreate an Index Notebook Here is the code to create an index Notebook\nThis creates a Notebook in the root of the folder. It also uses the new -Collapse parameter in New-AdsNoteBookCell that creates the code blocks with the code collapsed so that it looks neater. The index Notebook looks like this in the root of the folder\nThree O‚ÄôClock in the Morning It‚Äôs 3am and I have been called out. I can open up the Index Notebook, find the set of queries I want to run and click the run button.\nA new workbook opens up, named with my name and the time and I can get to work üôÇ I think it‚Äôs neat.\nMaybe you can find him at SQL Bits next year. Did you know that SQL Bits 2020 was announced?\nCheck out https://sqlbits.com for more details\n","date":"2019-11-21T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/11/image-39.png","permalink":"https://blog.robsewell.com/blog/dynamically-creating-azure-data-studio-notebooks-with-powershell-for-an-incident-response-index-notebook/","title":"Dynamically Creating Azure Data Studio Notebooks with PowerShell for an Incident Response Index Notebook"},{"content":"The latest update to the ADSNotebook PowerShell module I blogged about here now enables the creation of PowerShell notebooks with PowerShell.\nYou can install the module with\nInstall-Module ADSNotebook\ror if you have already installed it you can use\nUpdate-Module ADSNotebook\rIn the latest release, there is an extra parameter for New-AdsWorkBook of -Type which will accept either SQL or PowerShell\nCreate a PowerShell Notebook with PowerShell Rob OK!\nHere is some code to create a PowerShell Notebook. First we will create some cells using New-AdsWorkBookCell including all the markdown to add images and links. You can find my notebooks which explain how to write the markdown for your notebooks in my GitHub Presentations Repository\nThen we will create a new workbook using those cells\nThen, when that code is run we can open the Notebook and ta-da\nAnd it is super quick to run as well\nUPDATE ‚Äì Tyler Leonhardt t from the PowerShell team asked\nChallenge accepted, with extra meta, here is the PowerShell to create a PowerShell Notebook which will create a PowerShell Notebook!!\n","date":"2019-11-14T00:00:00Z","permalink":"https://blog.robsewell.com/blog/create-a-powershell-notebook-for-azure-data-studio-with-powershell/","title":"Create a PowerShell Notebook for Azure Data Studio with PowerShell"},{"content":"At PASS Summit today I gave a presentation about SQL Notebooks in Azure Data Studio for the DBA. I demo‚Äôd the PowerShell module ADSNotebook.\nwhich you can also find on GitHub (where I will be glad to take PR‚Äôs to improve it üôÇ )\nThis module has 3 functions\nThis module contains only 3 commands at present\nConvert-ADSPowerShellForMarkdown This will create the markdown link for embedding PowerShell code in a Text Cell for a SQL Notebook as described in this blog post\nNew-ADSWorkBookCell This command will create a workbook text cell or a code cell for adding to the New-ADSWorkBook command\nNew-ADSWorkBook This will create a new SQL Notebook using the cell objects created by New-ADSWorkBookCell\nUsage Convert-ADSPowerShellForMarkdown\nConvert-ADSPowerShellForMarkdown -InputText \u0026quot;Get-ChildItem\u0026quot; -LinkText 'This will list the files' -ToClipBoard\rConverts the PowerShell so that it works with MarkDown and sets it to the clipboard for pasting into a workbook cell\rNew-ADSWorkBookCell\n$introCelltext = \u0026quot;# Welcome to my Auto Generated Notebook\r## Automation\rUsing this we can automate the creation of notebooks for our use\r\u0026quot;\r$Intro = New-ADSWorkBookCell -Type Text -Text $introCelltext\rCreates an Azure Data Studio Text cell and sets it to a variable for passing to New-AdsWorkBook\rNew-ADSWorkBook\n$introCelltext = \u0026quot;# Welcome to my Auto Generated Notebook\r## Automation\rUsing this we can automate the creation of notebooks for our use\r\u0026quot;\r$SecondCelltext = \u0026quot;## Running code\rThe next cell will have some code in it for running\r## Server Principals\rBelow is the code to run against your instance to find the server principals that are enabled\u0026quot;\r$thirdcelltext = \u0026quot;SELECT Name\rFROM sys.server_principals\rWHERE is_disabled = 0\u0026quot;\r$Intro = New-ADSWorkBookCell -Type Text -Text $introCelltext\r$second = New-ADSWorkBookCell -Type Text -Text $SecondCelltext\r$third = New-ADSWorkBookCell -Type Code -Text $thirdcelltext\r$path = 'C:\\temp\\AutoGenerated.ipynb'\rNew-ADSWorkBook -Path $path -cells $Intro,$second,$third\rCreates 3 cells with New-AdsWorkBookCells to add to the workbook,\rtwo text ones and a code one, then creates a SQL Notebook with\rthose cells and saves it as C:\\temp\\AutoGenerated.ipynb\rInstallation You can install this Module from the PowerShell Gallery using\nInstall-Module ADSNotebook\nCompatability This module has been tested on Windows PowerShell 5.1, PowerShell Core 6 and PowerShell 7 on Windows 10 and Ubuntu\nDemo ","date":"2019-11-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/create-azure-data-studio-sql-notebooks-with-powershell/","title":"Create Azure Data Studio SQL Notebooks with PowerShell"},{"content":"I have been asked a couple of times recently what my Visual Studio Code extensions are at the moment so I thought I would write a quick post and also look at workspaces and how you can enable and disable extensions within them\nListing Extensions From the command line you can list your extensions using\ncode --list-extensions\rcode-insiders --list-extensions\rMy list looks like this\nYou can also see them in the view on the left of default Visual Studio Code and open them with CTRL + SHIFT + X (unless like me you have Snagit installed and it has taken that shortcut\nInstalling Extensions You can install extensions by opening the Extensions view in Visual Studio Code and searching for the extension. The list I have below has the precise names for each extension which you can use to search\nYou can also install extensions from the command-line with\ncode --install-extension \u0026lt;extensionid\u0026gt;\rcode-insiders --install-extension \u0026lt;extensionid\u0026gt;\rMy Extensions I am going to list these in alphabetical order by display name for ease (my ease that is!)\nBecause Chrissy LeMaire and I are writing dbatools in a Month of Lunches using AsciiDoc, it makes sense to have an extension enabling previewing and syntax, you can find it here\nFor interacting with Azure I use the Azure Account Extension ‚Äì ms-vscode.azure-account\nI use Azure CLI so I make use of the functionality of the Azure CLI Tools extension ms-vscode.azurecli\nFor interacting with Azure Repos I use the ms-vsts.team extension\nWhen creating ARM templates, this extension is very useful msazurermtools.azurerm-vscode-tools\nI have a few theme extensions, this one is for fun in demos üòâ beardedbear.beardedtheme\nThe blackboard theme is my default one gerane.theme-blackboard\nChasing closing brackets is much easier with the Bracket Pair Colorizer, I use the beta version coenraads.bracket-pair-colorizer-2\nI am rubbish at spelling and typing so I use this to help point out the issues! streetsidesoftware.code-spell-checker\nUsing the Docker extension adds another view to Visual Studio Code to ease working with containers ms-azuretools.vscode-docker\nAs an open-source project maintainer it is good to be able to work with GitHub pull requests without leaving Visual Studio Code github.vscode-pull-request-github_Preview_\nGitLens is absolutely invaluable when working with source control. It has so many features. This is an absolute must eamodio.gitlens\nWorking with Kubernetes? This extension adds another view for interacting with your cluster ms-kubernetes-tools.vscode-kubernetes-tools\nVisual Studio Live Share enables you to collaborate in real-time in Visual Studio Code with your colleagues or friends. I blogged about this here ms-vsliveshare.vsliveshare\nI love writing markdown and this linter assists me to ensure that my markdown is correct davidanson.vscode-markdownlint\nThe Material Icon Theme ensures that there are pretty icons in my editor! pkief.material-icon-theme\nI have both the PowerShell extension ms-vscode.powershell and the PowerShell preview extension ms-vscode.powershell-preview installed but only one can be enabled at a time\nThis suite of extensions enables easy remote development so that you can develop your PowerShell scripts, for example, inside a ubuntu container running PowerShell 7 or inside Windows Subsystem for LInux ms-vscode-remote.vscode-remote-extensionpack_Preview_\nWriting for cross-platform means looking out for line endings and this extension will display them and any whitespace in your editor medo64.render-crlf\nAn absolutely essential extension which enables me to backup all of my Visual Studio Code settings, shortcuts, extensions into a GitHub gist and keep all of my machines feeling the same. shan.code-settings-sync\nFor working with SQL Server within Visual Studio Code and having a view for my instances as well as a linter and intellisense I use ms-mssql.mssql\nYaml files and spaces! I no longer get so confused with this extension to help me üôÇ redhat.vscode-yaml\nWorkspaces Now that is a lot of extensions and I dont need all of them everytime. I use workspaces to help with this. I will create a workspace file for the project I am working on.\nI open or create the folders I will be working on and then click File and Save Workspace As and save the file in the root of the folder.\nNow, the next time I want to open the workspace, I can open the workspace file or if I open the folder Visual Studio Code will helpfully prompt me\nNow I can have all of my settings retained for that workspace\nFor this folder, I am ensuring that the PowerShell extension uses the PSScriptAnalyzer Settings file that I have created so that it will show if the code is compatible with the versions of PowerShell I have chosen. I can define settings for a workspace in the settings file, which you can open using CTRL and ,\nBut I can also enable or disable extensions for a workspace\nSo everytime I open this workspace I am only loading the extensions I want\n","date":"2019-11-01T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/11/image-26.png","permalink":"https://blog.robsewell.com/blog/my-current-vs-code-extensions-and-using-a-workspace-file/","title":"My current VS Code Extensions and using a workspace file"},{"content":"The latest release of the insiders edition of Azure Data Studio brings the first edition of PowerShell Notebooks!\nYou can download the latest insiders edition from the link above, it can be installed alongside the stable release.\nTo access many of the commands available use F1 to open the command palette (like many of my tips this also works in Visual Studio Code). You can then start typing to get the command that you want.\nYou can then hit enter with the command that you want highlighted, use the mouse or use the shortcut which is displayed to the right.\nIn a new notebook, you can click the drop down next to kernel and now you can see that PowerShell is available\nWhen you choose the PowerShell kernel, you will get a prompt asking you to configure the Python installation\nIf you have Python already installed you can browse to the location that it is installed or you can install Python. In the bottom pane you will be able to see the progress of the installation.\nWhen it has completed, you will see\nYou may also get a prompt asking if you would like to upgrade some packages\nAgain this will be displayed in the tasks pane\nAdding¬†PowerShell To¬†add¬†PowerShell¬†Code¬†to¬†the¬†notebook¬†click¬†the¬†Code¬†button¬†at¬†the¬†top of the file\nor¬†the¬†one¬†you¬†can¬†find¬†by¬†highlighting¬†above¬†or¬†below¬†a¬†block\nI did not have intellisense, but you can easily write your code in Azure Data Studio or Visual Studio Code and paste it in the block.\nInterestingly Shawn Melton ( t ) did\nCurious, you state \u0026ldquo;There is not any intellisense, but you can easily write your code in Azure Data Studio or Visual Studio Code and paste it in the block\u0026rdquo;‚Ä¶\nIt works flawlessly for me on Windows. pic.twitter.com/Lx6fGH9F5L\n‚Äî Shawn Melton (@wsmelton) October 17, 2019\nThis was because he had the PowerShell extension installed and I did not (I know !!)\nIf you find you dont have intellisense then install the PowerShell extension!\nClicking the play button (which is only visible when you hover the mouse over it) will run the code\nYou can clear the results from every code block using the clear results button at the top\nOtherwise, you can save the results with the Notebook by saving it. This is the part that is missing from running PowerShell in the Markdown blocks in a SQL Notebook as I described here\nI am looking forward to how this develops. You can find my sample PowerShell notebook (with the code results) here\n","date":"2019-10-17T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/10/image-8.png","permalink":"https://blog.robsewell.com/blog/powershell-notebooks-in-azure-data-studio/","title":"PowerShell Notebooks in Azure Data Studio"},{"content":"Most of my writing time at the moment is devoted to Learn dbatools in a Month of Lunches which is now available but here is a short post following a question someone asked me.\nHow can I get the Installation Date for SQL Server on my estate into a database with dbatools ? You can get the date that SQL Server was installed using the creation date of the NT Authority\\System login using T-SQL\nSELECT create_date FROM sys.server_principals WHERE sid = 0x010100000000000512000000\rWith dbatools To do this with dbatools you can use the command Get-DbaInstanceInstallDate command\nGet-DbaInstanceInstallDate -SqlInstance localhost More than one instance If we want to get the installation date for more than one instance we can simply create an array of instances for the SqlInstance parameter\nGet-DbaInstanceInstallDate -SqlInstance localhost, localhost\\DAVE\rGet the Windows installation date too You can also get the windows installation date with the IncludeWindows switch\nGet-DbaInstanceInstallDate -SqlInstance localhost, localhost\\DAVE -IncludeWindows Gather your instances How you get the instances in your estate is going to be different per reader but here is an example using Registered Servers from my local registered servers list, you can also use a Central Management Server\nGet-DbaRegisteredServer -Group local So we can gather those instances into a variable and pass that to Get-DbaInstanceInstallDate\n$SqlInstances = Get-DbaRegisteredServer -Group local Get-DbaInstanceInstallDate -SqlInstance $SqlInstances Add to database To add the results of any PowerShell command to a database, you can pipe the results to Write-DbaDbTableData\n$SqlInstances = Get-DbaRegisteredServer -Group local $writeDbaDataTableSplat = @{\rSqlInstance = 'localhost'\rTable = 'InstallDate'\rDatabase = 'tempdb'\rSchema = 'dbo'\rAutoCreateTable = $true\r}\rGet-DbaInstanceInstallDate -SqlInstance $SqlInstances | Write-DbaDataTable @writeDbaDataTableSplat\rThis will create a table called InstallDate and put the results of the Get-DbaInstanceInstallDate command. Note ‚Äì If you want to try this code, I would advise using a different database than tempdb!!\nIt is important to note that the table created may not have the most optimal data types and that you may want to pre-create the table.\nSo there you go, all the installation dates for your estate in a database table. Hope that helps you Jonny.\n","date":"2019-10-11T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/10/image-7.png","permalink":"https://blog.robsewell.com/blog/getting-sql-server-installation-date-with-powershell-using-dbatools/","title":"Getting SQL Server installation date with PowerShell using dbatools"},{"content":"It‚Äôs been a busy time!\nAs well as many other things, the fantastical BDFL of dbatools Chrissy Lemaire¬†@cl and myself have written enough of a chunk of¬†Learn dbatools in a Month of Lunches that our publisher¬†Manning Publications have agreed to release it as a MEAP. Not a text book, this book is written in a fun conversational style and split up into chapters that you can read in a lunch-time.\nIt is impossible for me to hear MEAP and not think of this üôÇ\nRoadrunner Speeding GIF from Hungry GIFs\nbut I expect you are wondering what a MEAP is?\nWhat is MEAP? A book can take a year or more to write, so how do you learn that hot new technology today? The answer is MEAP, the Manning Early Access Program. In MEAP, you read a book chapter-by-chapter while it‚Äôs being written and get the final eBook as soon as it‚Äôs finished. If you pre-order the pBook, you‚Äôll get it long before it‚Äôs available in stores.\nhttps://www.manning.com/meap-program\nBasically, to make it easy to get and for those that like to get in early, you can order the book and get the first 4 chapters (three in reality) RIGHT NOW!! (It also means that Chrissy and I have to write the rest of book ‚Äì dang still going to be busy!)\nSimply head over to https://beard.media/bookblog and use the code mlsewell and you can get access to the book too.\n](https://beard.media/bookblog)\nThis will also give you access to the live book.\nlive book\nThe live book is fantastic, you can read the whole book from within your browser. See the three icons that appear to the right of the book?\n3 little icons (no porridge)\nThe left hand one enables you to bookmark an important part so that you can come back to it easily using the bookmarks link in the top right\nbookmarks\nThe middle icon enables you to write notes for yourself, maybe ways that you can use the information or maybe comments about an awesome Italian.\nShoes\nThe last one is the way that you can make comments and engage us , the authors in conversation, ask questions, request clarification or wonder about Dutch data manglers\nI think its down to PII\nIf you select a piece of text, another menu opens up\nThe first icon lets you highlight the text, to make it easier to find later\nHover over the highlight and you can choose different colours for different things.\nor even create pretty pictures for Mathias\nMathias ‚Äì Why isn‚Äôt he an MVP?\nYou can choose to annotate, which is sort of like highlighting and writing a note with the next icon\nWhen you want to share a link to a particular part of the book with someone else, you can highlight part of it and click the link icon\nIt‚Äôs easy to start PowerShell as another user as long as you remember when to press SHIFT\nWhich will highlight the paragraph and open a dialogue at the bottom where you can create and copy the link.\nBy far the most important part for Chrissy and I is the last link. When you find something wrong you can mark it for our attention. Yes, even with Chrissy and I proof reading each others words, the fabulous proof reader Cl√°udio Silva (b¬†|¬†t)¬†and awesome tech editor Mike Shepard (b¬†|¬†t)¬†as well as many community reviewers there are still, and will continue to be, issues. So when you find them, highlight them and click the right hand most link\nwith with more more than than one one\nThis will open up as shown so that you can fill in what was wrong (Please don‚Äôt report this error again Shane b | t has beaten you to it!)\nYou will have noticed on social media and elsewhere that we have left some easter eggs in the book\nYup, we have some easter eggs in #dbatoolsMol\nWe hope you enjoy them https://t.co/iZa3u8iLPC\n‚Äî Rob Sewell (@sqldbawithbeard) August 29, 2019\nWhenever you find them or whenever you want to talk about the book on social media, please use the hashtag #dbatoolsMoL ‚Äì you never know what goodies may end up in your inbox.\nOh and if you have got this far and don‚Äôt know what dbatools in a Month of Lunches is, listen to the hair and read more https://dbatools.io/meap/\n","date":"2019-09-04T00:00:00Z","permalink":"https://blog.robsewell.com/blog/meap-meap-dbatoolsmol-live-book-edition/","title":"MEAP MEAP ‚Äì #dbatoolsMoL ‚Äì Live Book edition"},{"content":"I have done a lot of writing in the last few months but you see no blog posts! My wonderful friend Chrissy and I are writing ‚Äúdbatools in a Month of Lunches‚Äù to be published by Manning. That has taken up a lot of my writing mojo. We have hit a little break whilst we have some reviews done ready for the MEAP (For everyone who asks, the answer is the unfulfilling ‚Äòsoon‚Äô) so it‚Äôs time for a blog post!\nSQL Notebooks are cool I have had a lot of fun with SQL Notebooks recently. I have presented a session about them at a couple of events this month DataGrillen and SQL Saturday Cork. Here is a little snippet\n#dbatools in PowerShell in @AzureDataStudio SQL Notebooks for creating the containers and restoring the #dbachecks historical database for running queries in üôÇ Getting ready for presentation for #DataGrillen pic.twitter.com/wiQ41bblQV\n‚Äî Rob Sewell (@sqldbawithbeard) May 21, 2019\nYes, you can run PowerShell in a SQL Notebook in Azure Data Studio just by clicking a link in the markdown cell. This opens up a lot of excellent possibilities.\nI have had several discussions about how SQL Notebooks can be used by SQL DBAs within their normal everyday roles. (Mainly because I don‚Äôt really understand what the sorcerers of data science do with notebooks!). I have helped clients to look at some of their processes and use SQL Notebooks to help with them. Creating Disaster Recovery or Change Run-books or Incident Response Templates or using them for product demonstrations. Of course, I needed to use PowerShell in that üôÇ\nI have really enjoyed working out how to run PowerShell in the markdown in a SQL Notebook in Azure Data Studio and I think Anthony the kubernetes magician did too!\nI think @sqldbawithbeard is an actual wizard! You should see the things he can do with @AzureDataStudio #DataGrillen pic.twitter.com/KMeZR3CrPK\n‚Äî Anthony E. Nocentino (@nocentino) June 20, 2019\nOK enough magic puns lets talk about PowerShell in SQL Notebooks. You can read about how to create a SQL Notebook and run T-SQL queries here, (you no longer need the Insider Edition by the way)\nPowerShell in Markdown! First, before I go any further, I must say this. I was at the European PowerShell Conference when I was working this out and creating my sessions and I said the words\n‚ÄúCool, I can click a link and run PowerShell, this is neat‚Äù\nA Beardy fellow in Hannover\nThis stopped some red team friends of mine in their tracks and they said ‚ÄúShow me‚Äù. One of them was rubbing their hands with glee! You can imagine the sort of wicked, devious things that they were immediately considering doing.\nYes, it‚Äôs funny but also it carries a serious warning. Without understanding what it is doing, please don‚Äôt enable PowerShell to be run in a SQL Notebook that someone sent you in an email or you find on a GitHub. In the same way as you don‚Äôt open the word document attachment which will get a thousand million trillion europounddollars into your bank account or run code you copy from the internet on production without understanding what it does, this could be a very dangerous thing to do.\nWith that warning out of the way, there are loads of really useful and fantastic use cases for this. SQL Notebooks make great run-books or incident response recorders and PowerShell is an obvious tool for this. (If only we could save the PowerShell output in a SQL Notebook, this would be even better)\nHow on earth did you work this out? Someone asked me how I worked it out. I didn‚Äôt! It began with Vicky Harp PM lead for the SQL Tools team at Microsoft\nDid you know you can add markdown links to open a terminal and paste in a command in @AzureDataStudio notebooks? pic.twitter.com/YHX9pIVQco\n‚Äî Vicky Harp (@vickyharp) May 14, 2019\nI then went and looked at Kevin Cunnane‚Äòs notebook. Kevin is a member of the tools team working on Azure Data Studio. With SQL Notebooks, you can double click the markdown cell and see the code that is behind it. To understand how it is working, lets deviate a little.\nKeyboard Shortcuts IF you click the cog at the bottom left of Azure Data Studio and choose Keyboard Shortcuts\nyou can make Azure Data Studio (and Visual Studio Code) work exactly how you want it to. Typing in the top box will find a command and you can then set the shortcuts that you want to use to save yourself time.\n](https://i1.wp.com/user-images.githubusercontent.com/6729780/59566321-84233d80-9056-11e9-9643-e9e15e85a2f0.png?ssl=1)\nThis also enables you to see the command that is called when you use a keyboard shortcut. For example, you can see that for the focus terminal command it says workbench.action.terminal.focus.\nIt turns out that you can call this as a link in a Markdown document using HTML with \u0026lt;a href=\u0026quot;\u0026quot;\u0026gt; and adding command: prior to the command text. When the link is clicked the command will run. Cool üôÇ\nFor this to be able to work (you read the warning above?) you need to set the Notebook to be trusted by clicking this button.\n](https://i0.wp.com/user-images.githubusercontent.com/6729780/59566360-365b0500-9057-11e9-87fb-1f8cbbb6e9e2.png?ssl=1)\nThis will allow any command to be run. Of course, people with beards will helpfully advise when this is required for a SQL Notebook. (Safe to say people attempting nefarious actions will try the same with your users)\nNow that we know how to run an Azure Data Studio command using a link in a markdown cell the next step is to run a PowerShell command. I headed to the Visual Studio Code documentation and found\nSend text from a keybinding The¬†workbench.action.terminal.sendSequence¬†command can be used to send a specific sequence of text to the terminal, including escape sequence\nThat‚Äôs the command we need, however, we still need to craft the command so that it will work as a link. It needs to be converted into a URL.\nI started by using this website¬†https://www.url-encode-decode.com/¬†to do this. This is¬†how you can check the code in other peoples notebook, use the decode capability.\nEncoding Set-Location C:\\dbachecks gives `Set-Location+C%3A%5Cdbacheck``\nSo I can just put that code into the href link and bingo!\nIf only it was that easy!!\nSome Replacing is required The + needs to be replaced with a space or %20\nYou also need to double the \\ and replace the %3A with a : The \u0026quot; needs to be replaced with \\u022, the ' with \\u027, the curly braces won‚Äôt work unless you remove the %0D%0A. Got all that? Good!\nOnce you have written your PowerShell, encoded it, performed the replacements, you add¬†\\u000D¬†at the end of the code to pass an enter to run the code and then place all of that into a link like this\n\u0026lt;a href=\u0026quot;command:workbench.action.terminal.sendSequence?%7B%22text%22%3A%22 PLACE THE ENCODED CODE HERE %22%7D\u0026quot;\u0026gt;Link Text\u0026lt;/a\u0026gt;\nThis means that if you want to add the PowerShell code to set a location and then list the files and folders in that location to a Markdown cell using PowerShell like this\nSet-Location C:\\dbachecks\rGet-ChildItem\rYou would end up with a link like this\n`\u0026lt;a href=\u0026quot;command:workbench.action.terminal.sendSequence?%7B%22text%22%3A%22 Set-Location C:%5C%5Cdbachecks \\u000D Get-ChildItem \\u000D %22%7D\u0026quot;\u0026gt;Set Location and list files\u0026lt;/a`\u0026gt;\nDoing something more than once? I don‚Äôt want to remember that all of the time so I wrote a PowerShell function. You can find it on GitHub https://github.com/SQLDBAWithABeard/Functions/blob/master/Convert-ADSPowerShellForMarkdown.ps1\nThis will take a PowerShell command and turn it into a link that will work in an Azure Data Studio markdown. It‚Äôs not magic, it‚Äôs PowerShell. There is a ‚ÄìToClipboard parameter which will copy the code to the clipboard ready for you to paste into the cell (On Windows machines only)\nGiants There are many uses for this but here‚Äôs one I think is cool.\nThe link below will go to a notebook, which will show how you the giants upon whose shoulders I stand\nGlenn Berry, Chrissy LeMaire, Andr√© Kamman, Gianluca Sartori\nhave enabled me to create a SQL Notebook with a link which will run some PowerShell to create a SQL Notebook which will have all of the Diagnostic Queries in it.\nYou could possibly use something like it for your incident response SQL Notebook.\nIt‚Äôs also cool that GitHub renders the notebook in a browser (You can‚Äôt run PowerShell or T-SQL from there though, you need Azure Data Studio!)\nhttps://github.com/SQLDBAWithABeard/Presentations/blob/master/2019/Berlin%20SQL%20User%20Group/04%20-%20Glenn%20Berry%20Notebook.ipynb\n","date":"2019-07-17T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/07/image-4.png","permalink":"https://blog.robsewell.com/blog/powershell-in-sql-notebooks-in-azure-data-studio/","title":"PowerShell in SQL Notebooks in Azure Data Studio"},{"content":"In my posts about using Azure Devops to build Azure resources with Terraform, I built a Linux SQL VM. I used the Terraform in this GitHub repository and created this\nConnecting with MobaXterm I had set the Network security rules to accept connections only from my static IP using variables in the Build Pipeline. I use MobaXterm as my SSH client. Its a free download. I click on sessions\n[\nChoose a SSH session and fill in the remote host address from the portal\n[\nfill in the password and\n[\nConfiguring SQL The next task is to configure the SQL installation. Following the instructions on the Microsoft docs site I run\nsudo systemctl stop mssql-server\rsudo /opt/mssql/bin/mssql-conf set-sa-password\renter the sa password and\n[\nNow to start SQL\nsudo systemctl start mssql-server\rInstalling pwsh Installing PowerShell Core (pwsh) is easy with snap\nsudo snap install powershell \u0026ndash;classic\nA couple of minutes of downloads and install\n[\nand pwsh is ready for use\n[\nInstalling dbatools To install dbatools from the Powershell Gallery simply run\nInstall-Module dbatools -Scope CurrentUser\nThis will prompt you to allow installing from an untrusted repository\n[\nand dbatools is ready to go\n#Set a credential\r$cred = Get-Credential\r# Show the databases on the local instance\rGet-DbaDatabase -SqlInstance localhost -SqlCredential $cred\r[\nConnecting with Azure Data Studio I can also connect with Azure Data Studio\n[\nand connect\n[\nJust a quick little post explaining what I did üôÇ\nHappy Linuxing!\n","date":"2019-04-25T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-125.png","permalink":"https://blog.robsewell.com/blog/azure-sql-linux-vm-configuring-sql-installing-pwsh-and-connecting-and-interacting-with-dbatools/","title":"Azure SQL Linux VM ‚Äì configuring SQL, installing pwsh and connecting and interacting with dbatools"},{"content":"In the last few posts I have moved from building an Azure SQL DB with Terraform using VS Code to automating the build process for the Azure SQL DB using Azure DevOps Build Pipelines to using Task Groups in Azure DevOps to reuse the same Build Process and build an Azure Linux SQL VM and Network Security Group. This evolution is fantastic but Task Groups can only be used in the same Azure DevOps repository. It would be brilliant if I could use Configuration as Code for the Azure Build Pipeline and store that in a separate source control repository which can be used from any Azure DevOps Project.\nLuckily, you can üòâ You can use Azure DevOps Job Templates to achieve this. There is a limitation at present, you can only use them for Build Pipelines and not Release Pipelines.\nThe aim of this little blog series was to have a single Build Pipeline stored as code which I can use to build any infrastructure that I want with Terraform in Azure and be able to use it anywhere\nCreating a Build Pipeline Template I created a GitHub repository to hold my Build Templates, feel free to use them as a base for your own but please don‚Äôt try and use the repo for your own builds.\nThe easiest way to create a Build Template is to already have a Build Pipeline. This cannot be done from a Task Group but I still have the Build Pipeline from my automating the build process for the Azure SQL DB using Azure DevOps Build Pipelines blog post.\nThere is a View YAML button. I can click this to view the YAML definition of the Build Pipeline\nI copy that and paste it into a new file in my BuildTemplates repository. (I have replaced my Azure Subscription information in the public repository)\njobs:\r- job: Build\rpool:\rname: Hosted VS2017\rdemands: azureps\rsteps:\r- task: AzureCLI@1\rdisplayName: 'Azure CLI to deploy azure storage for backend'\rinputs:\razureSubscription: 'PUTYOURAZURESUBNAMEHERE'\rscriptLocation: inlineScript\rinlineScript: |\r# the following script will create Azure resource group, Storage account and a Storage container which will be used to store terraform state\rcall az group create --location $(location) --name $(TerraformStorageRG)\rcall az storage account create --name $(TerraformStorageAccount) --resource-group $(TerraformStorageRG) --location $(location) --sku Standard_LRS\rcall az storage container create --name terraform --account-name $(TerraformStorageAccount)\r- task: AzurePowerShell@3\rdisplayName: 'Azure PowerShell script to get the storage key'\rinputs:\razureSubscription: 'PUTYOURAZURESUBNAMEHERE'\rScriptType: InlineScript\rInline: |\r# Using this script we will fetch storage key which is required in terraform file to authenticate backend stoarge account\r$key=(Get-AzureRmStorageAccountKey -ResourceGroupName $(TerraformStorageRG) -AccountName $(TerraformStorageAccount)).Value[0]\rWrite-Host \u0026quot;##vso[task.setvariable variable=TerraformStorageKey]$key\u0026quot;\razurePowerShellVersion: LatestVersion\r- task: qetza.replacetokens.replacetokens-task.replacetokens@3\rdisplayName: 'Replace tokens in terraform file'\rinputs:\rrootDirectory: Build\rtargetFiles: |\r**/*.tf\r**/*.tfvars\rtokenPrefix: '__'\rtokenSuffix: '__'\r- powershell: |\rGet-ChildItem .\\Build -Recurse\rGet-Content .\\Build\\*.tf\rGet-Content .\\Build\\*.tfvars\rGet-ChildItem Env: | select Name\rdisplayName: 'Check values in files'\renabled: false\r- task: petergroenewegen.PeterGroenewegen-Xpirit-Vsts-Release-Terraform.Xpirit-Vsts-Release-Terraform.Terraform@2\rdisplayName: 'Initialise Terraform'\rinputs:\rTemplatePath: Build\rArguments: 'init -backend-config=\u0026quot;0-backend-config.tfvars\u0026quot;'\rInstallTerraform: true\rUseAzureSub: true\rConnectedServiceNameARM: 'PUTYOURAZURESUBNAMEHERE'\r- task: petergroenewegen.PeterGroenewegen-Xpirit-Vsts-Release-Terraform.Xpirit-Vsts-Release-Terraform.Terraform@2\rdisplayName: 'Plan Terraform execution'\rinputs:\rTemplatePath: Build\rArguments: plan\rInstallTerraform: true\rUseAzureSub: true\rConnectedServiceNameARM: 'PUTYOURAZURESUBNAMEHERE'\r- task: petergroenewegen.PeterGroenewegen-Xpirit-Vsts-Release-Terraform.Xpirit-Vsts-Release-Terraform.Terraform@2\rdisplayName: 'Apply Terraform'\rinputs:\rTemplatePath: Build\rArguments: 'apply -auto-approve'\rInstallTerraform: true\rUseAzureSub: true\rConnectedServiceNameARM: 'PUTYOURAZURESUBNAMEHERE'\rNow I can use this yaml as configuration as code for my Build Pipeline üôÇ It can be used from any Azure DevOps project. Once you start looking at the code and the documentation for the yaml schema you can begin to write your pipelines as YAML, but sometimes it is easier to just create build pipeline or even just a job step in the browser and click the view yaml button!\nCreate an AKS Cluster with a SQL 2019 container using Terraform and Build templates I have a GitHub Repository with the Terraform code to build a simple AKS cluster. This could not have been achieved without Richard Cheney‚Äôs article I am not going to explain how it all works for this blog post or some of the negatives of doing it this way. Instead lets build an Azure DevOps Build Pipeline to build it with Terraform using Configuration as Code (the yaml file)\nI am going to create a new Azure DevOps Build Pipeline and as in the previous posts connect it to the GitHub Repository holding the Terraform code.\nThis time I am going to choose the Configuration as code template\nI am going to give it a name and it will show me that it needs the path to the yaml file containing the build definition in the current repository.\nClicking the 3 ellipses will pop-up a file chooser and I pick the build.yaml file\nThe build.yaml file looks like this. The name is the USER/Repository Name and the endpoint is the name of the endpoint for the GitHub service connection in Azure DevOps. The template value is the name of the build yaml file @ the name given for the repository value.\nresources:\rrepositories:\r- repository: templates\rtype: GitHub\rname: SQLDBAWithABeard/Presentations-BuildTemplates-Private\rendpoint: SQLDBAWithABeardGitHub\rjobs:\r- template: AzureTerraform.yaml@templates # Template reference\rYou can find (and change) your GitHub service connection name by clicking on the cog bottom left in Azure DevOps and clicking service connections\nI still need to create my variables for my Terraform template (perhaps I can now just leave those in my code?) For the AKS Cluster build right now I have to add presentation, location, ResourceGroupName, AgentPoolName, ServiceName, VMSize, agent_count\nThen I click save and queue and the job starts running\nIf I want to edit the pipeline it looks a little different\nThe variables and triggers can be found under the 3 ellipses on the top right\nIt also defaults the trigger to automatic deployment.\nIt takes a bit longer to build\nand when I get the Terraform code wrong and the build fails, I can just alter the code, commit it, push and a new build will start and the Terraform will work out what is built and what needs to be built!\nbut eventually the job finishes successfully\nand the resources are built\nand in Visual Studio Code with the Kubernetes extension installed I can connect to the cluster by clicking the 3 ellipses and Add Existing Cluster\nI choose Azure Kubernetes Services and click next\nChoose my subscription and then add the cluster\nand then I can explore my cluster\nI can also see the dashboard by right clicking on the cluster name and Open Dashboard\nRight clicking on the service name and choosing describe\nshows the external IP address, which I can put into Azure Data Studio and connect to my container\nSo I now I can source control my Build Job Steps and hold them in a central repository. I can make use of them in any project. This gives me much more control and saves me from repeating myself repeating myself. The disadvantage is that there is no handy warning when I change the underlying Build Repository that I will be affecting other Build Pipelines and there is no easy method to see which Build Pipelines are dependent on the build yaml file\nHappy Automating\n","date":"2019-04-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-151.png","permalink":"https://blog.robsewell.com/blog/using-azure-devops-build-pipeline-templates-with-terraform-to-build-an-aks-cluster/","title":"Using Azure DevOps Build Pipeline Templates with Terraform to build an AKS cluster"},{"content":"In my last post I showed how to build an Azure DevOps Pipeline for a Terraform build of an Azure SQLDB. This will take the terraform code and build the required infrastructure.\nThe plan all along has been to enable me to build different environments depending on the requirement. Obviously I can repeat the steps from the last post for a new repository containing a Terraform code for a different environment but\nIf you are going to do something more than once Automate It\nwho first said this? Anyone know?\nThe build steps for building the Terraform are the same each time (if I keep a standard folder and naming structure) so it would be much more beneficial if I could keep them in a single place and any alterations to the process only need to be made in the one place üôÇ\nTask Groups Azure DevOps has task groups. On the Microsoft Docs web-page they are described as\nA¬†task group¬†allows you to encapsulate a sequence of tasks, already defined in a build or a release pipeline, into a single reusable task that can be added to a build or release pipeline, just like any other tas\nhttps://docs.microsoft.com/en-us/azure/devops/pipelines/library/task-groups?view=azure-devops\nIf you are doing this with a more complicated existing build pipeline it is important that you read the Before You Create A Task Group on the docs page. This will save you time when trying to understand why variables are not available (Another grey hair on my beard!)\nCreating A Task Group Here‚Äôs the thing, creating a task group is so easy it should be the default way you create Azure DevOps Pipelines. Let me walk you through it\nI will use the Build Pipeline from the previous post. Click edit from the build page\nThen CTRL and click to select all of the steps\nRight Click and theres a Create Task Group button to click !\nYou can see that it has helpfully added the values for the parameters it requires for the location, Storage Account and the Resource Group.\nRemember the grey beard hair above? We need to change those values to use the variables that we will add to the Build Pipeline using\n$(VariableName)\rOnce you have done that click Create\nThis will also alter the current Build Pipeline to use the Task Group. Now we have a Task Group that we can use in any build pipeline in this project.\nUsing the Task Group with a new Build Pipeline to build an Azure Linux SQL VM Lets re-use the build steps to create an Azure SQL Linux VM. First I created a new GitHub Repository for my Terraform code. Using the docs I created the Terraform to create a resource group, a Linux SQL VM, a virtual network, a subnet, a NIC for the VM, a public IP for the VM, a network security group with two rules, one for SQL and one for SSH. It will look like this\nThe next step is to choose the repository\nagain we are going to select Empty job (although the next post will be about the Configuration as Code üôÇ\nAs before we will name the Build Pipeline and the Agent Job Step and click the + to add a new task. This time we will search for the Task Group name that we created\nI need to add in the variables from the variable.tf in the code and also for the Task Group\nand when I click save and queue\nIt runs for less than 7 minutes\nand when I look in the Azure portal\nand I can connect in Azure Data Studio\nAltering The Task Group You can find the Task Groups under Pipelines in your Azure DevOps project\nClick on the Task Group that you have created and then you can alter, edit it if required and click save\nThis will warn you that any changes will affect all pipelines and task groups that are using this task group. To find out what will be affected click on references\nwhich will show you what will be affected.\nNow I can run the same build steps for any Build Pipeline and alter them all in a single place using Task Groups simplifying the administration of the Build Pipelines.\nThe next post will show how to use Azure DevOps templates to use the same build steps across many projects and build pipelines and will build a simple AKS cluster\nThe first post showed how to build an Azure SQLDB with Terraform using VS Code\nThe second post showed how to use Azure DevOps Task Groups to use the same build steps in multiple pipelines and build an Azure Linux SQL Server VM\nHappy Automating!\n","date":"2019-04-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-107.png","permalink":"https://blog.robsewell.com/blog/using-the-same-azure-devops-build-steps-for-terraform-with-different-pipelines-with-task-groups-to-build-an-azure-linux-sql-vm/","title":"Using the same Azure DevOps build steps for Terraform with different Pipelines with Task Groups to build an Azure Linux SQL VM"},{"content":"In my last post I showed how to create a Resource Group and an Azure SQL Database with Terraform using Visual Studio Code to deploy.\nOf course, I haven\u0026rsquo;t stopped there, who wants to manually run code to create things. There was a lot of install this and set up that. I would rather give the code to a build system and get it to run it. I can then even set it to automatically deploy new infrastructure when I commit some code to alter the configuration.\nThis scenario though is to build environments for presentations. Last time I created an Azure SQL DB and tagged it with DataInDevon (By the way you can get tickets for Data In Devon here ‚Äì It is in Exeter on April 26th and 27th)\nIf I want to create the same environment but give it tags for a different event (This way I know when I can delete resources in Azure!) or name it differently, I can use Azure DevOps and alter the variables. I could just alter the code and commit the change and trigger a build or I could create variables and enable them to be set at the time the job is run. I use the former in ‚Äúwork‚Äù situations and the second for my presentations environment.\nI have created a project in Azure DevOps for my Presentation Builds. I will be using GitHub to share the code that I have used. Once I clicked on pipelines, this is the page I saw\nClicking new pipeline, Azure DevOps asked me where my code was\nI chose GitHub, authorised and chose the repository.\nI then chose Empty Job on the next page. See the Configuration as code choice? We will come back to that later and our infrastructure as code will be deployed with a configuration as code üôÇ\nThe next page allows us to give the build a good name and choose the Agent Pool that we want to use. Azure DevOps gives 7 different hosted agents running Linux, Mac, Windows or you can download an agent and run it on your own cpus. We will use the default agent for this process.\nClicking on Agent Job 1 enables me to change the name of the Agent Job. I could also choose a different type of Agent for different jobs within the same pipeline. This would be useful for testing different OS‚Äôs for example but for right now I shall just name it properly.\nState First we need somewhere to store the state of our build so that if we re-run it the Terraform plan step will be able to work out what it needs to do. (This is not absolutely required just for building my presentation environments and this might not be the best way to achieve this but for right now this is what I do and it works.)\nI click on the + and search for Azure CLI.\nand click on the Add button which gives me some boxes to fill in.\nI choose my Azure subscription from the first drop down and choose Inline Script from the second\nInside the script block I put the following code\n# the following script will create Azure resource group, Storage account and a Storage container which will be used to store terraform state\rcall az group create --location $(location) --name $(TerraformStorageRG)\rcall az storage account create --name $(TerraformStorageAccount) --resource-group $(TerraformStorageRG) --location $(location) --sku Standard_LRS\rcall az storage container create --name terraform --account-name $(TerraformStorageAccount)\rThis will create a Resource Group, a storage account and a container and use some variables to provide the values, we will come back to the variables later.\nAccess Key The next thing that we need to do is to to enable the job to be able to access the storage account. We don‚Äôt want to store that key anywhere but we can use our Azure DevOps variables and some PowerShell to gather the access key and write it to the variable when the job is running . To create the variables I clicked on the variables tab\nand then added the variables with the following names TerraformStorageRG, TerraformStorageAccount and location from the previous task and TerraformStorageKey for the next task.\nWith those created, I go back to Tasks and add an Azure PowerShell task\nI then add this code to get the access key and overwrite the variable.\n# Using this script we will fetch storage key which is required in terraform file to authenticate backend storage account\r$key=(Get-AzureRmStorageAccountKey -ResourceGroupName $(TerraformStorageRG) -AccountName $(TerraformStorageAccount)).Value[0]\rWrite-Host \u0026quot;##vso[task.setvariable variable=TerraformStorageKey]$key\u0026quot;\rInfrastructure as Code In my GitHub repository I now have the following folders\nThe manual folders hold the code from the last blog post. In the Build folder, the main.tf file is identical and looks like this.\nprovider \u0026quot;azurerm\u0026quot; {\rversion = \u0026quot;=1.24.0\u0026quot;\r}\rterraform {\rbackend \u0026quot;azurerm\u0026quot; {\rkey = \u0026quot;terraform.tfstate\u0026quot;\r}\r}\rresource \u0026quot;azurerm_resource_group\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.ResourceGroupName}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rresource \u0026quot;azurerm_sql_server\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.SqlServerName}\u0026quot;\rresource_group_name = \u0026quot;${azurerm_resource_group.presentation.name}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rversion = \u0026quot;12.0\u0026quot;\radministrator_login = \u0026quot;__SQLServerAdminUser__\u0026quot;\radministrator_login_password = \u0026quot;__SQLServerAdminPassword__\u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rresource \u0026quot;azurerm_sql_database\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.SqlDatabaseName}\u0026quot;\rresource_group_name = \u0026quot;${azurerm_sql_server.presentation.resource_group_name}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rserver_name = \u0026quot;${azurerm_sql_server.presentation.name}\u0026quot;\redition = \u0026quot;${var.Edition}\u0026quot;\rrequested_service_objective_name = \u0026quot;${var.ServiceObjective}\u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rThe variables.tf folder looks like this.\nvariable \u0026quot;presentation\u0026quot; {\rdescription = \u0026quot;The name of the presentation - used for tagging Azure resources so I know what they belong to\u0026quot;\rdefault = \u0026quot;__Presentation__\u0026quot;\r}\rvariable \u0026quot;ResourceGroupName\u0026quot; {\rdescription = \u0026quot;The Prefix used for all resources in this example\u0026quot;\rdefault = \u0026quot;__ResourceGroupName__\u0026quot;\r}\rvariable \u0026quot;location\u0026quot; {\rdescription = \u0026quot;The Azure Region in which the resources in this example should exist\u0026quot;\rdefault = \u0026quot;__location__\u0026quot;\r}\rvariable \u0026quot;SqlServerName\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL Server to be created or to have the database on - needs to be unique, lowercase between 3 and 24 characters including the prefix\u0026quot;\rdefault = \u0026quot;__SqlServerName__\u0026quot;\r}\rvariable \u0026quot;SQLServerAdminUser\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL Server Admin user for the Azure SQL Database\u0026quot;\rdefault = \u0026quot;__SQLServerAdminUser__\u0026quot;\r}\rvariable \u0026quot;SQLServerAdminPassword\u0026quot; {\rdescription = \u0026quot;The Azure SQL Database users password\u0026quot;\rdefault = \u0026quot;__SQLServerAdminPassword__\u0026quot;\r}\rvariable \u0026quot;SqlDatabaseName\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL database on - needs to be unique, lowercase between 3 and 24 characters including the prefix\u0026quot;\rdefault = \u0026quot;__SqlDatabaseName__\u0026quot;\r}\rvariable \u0026quot;Edition\u0026quot; {\rdescription = \u0026quot;The Edition of the Database - Basic, Standard, Premium, or DataWarehouse\u0026quot;\rdefault = \u0026quot;__Edition__\u0026quot;\r}\rvariable \u0026quot;ServiceObjective\u0026quot; {\rdescription = \u0026quot;The Service Tier S0, S1, S2, S3, P1, P2, P4, P6, P11 and ElasticPool\u0026quot;\rdefault = \u0026quot;__ServiceObjective__\u0026quot;\r}\rIt is exactly the same except that the values have been replaced by the value name prefixed and suffixed with __. This will enable me to replace the values with the variables in my Azure DevOps Build job.\nThe backend-config.tf file will store the details of the state that will be created by the first step and use the access key that has been retrieved in the second step.\nresource_group_name = \u0026quot;__TerraformStorageRG__\u0026quot;\rstorage_account_name = \u0026quot;__TerraformStorageAccount__\u0026quot;\rcontainer_name = \u0026quot;terraform\u0026quot;\raccess_key = \u0026quot;__TerraformStorageKey__\u0026quot;\rI need to add the following variables to my Azure DevOps Build ‚Äì Presentation, ResourceGroupName, SqlServerName, SQLServerAdminUser, SQLServerAdminPassword, SqlDatabaseName, Edition, ServiceObjective . Personally I would advise setting the password or any other sensitive values to sensitive by clicking the padlock for that variable. This will stop the value being written to the log as well as hiding it behind *‚Äôs\nBecause I have tagged the variables with Settable at queue time , I can set the values whenever I run a build, so if I am at a different event I can change the name.\nBut the build job hasn‚Äôt been set up yet. First we need to replace the values in the variables file.\nReplace the Tokens I installed the Replace Tokens Task from the marketplace and added that to the build.\nI am going to use a standard naming convention for my infrastructure code files so I add Build to the Root Directory. You can also click the ellipses and navigate to a folder in your repo. In the Target Files I add ‚Äù/*.tf‚Äù and ‚Äú/*.tfvars‚Äù which will search all of the folders () and only work on files with a .tf or .tfvars extension (/*.tfvars) The next step is to make sure that the replacement prefix and suffix are correct. It is hidden under Advanced\nBecause I often forget this step and to aid in troubleshooting I add another step to read the contents of the files and place them in the logs. I do this by adding a PowerShell step which uses\nGet-ChildItem .\\Build -Recurse\rGet-Content .\\Build\\*.tf\rGet-Content .\\Build\\*.tfvars\rUnder control options there is a check box to enable or disable the steps so once I know that everything is ok with the build I will disable this step. The output in the log of a build will look like this showing the actual values in the files. This is really useful for finding spaces :-).\nRunning the Terraform in Azure DevOps With everything set up we can now run the Terraform. I installed the Terraform task from the marketplace and added a task. We are going to follow the same process as the last blog post, init, plan, apply but this time we are going to automate it üôÇ\nFirst we will initialise\nI put Build in the Terraform Template path. The Terraform arguments are\ninit -backend-config=\u0026quot;0-backend-config.tfvars\u0026quot;\rwhich will tell the Terraform to use the backend-config.tfvars file for the state. It is important to tick the Install terraform checkbox to ensure that terraform is available on the agent and to add the Azure Subscription (or Service Endpoint in a corporate environment\nAfter the Initialise, I add the Terraform task again add Build to the target path and this time the argument is plan\nAgain, tick the install terraform checkbox and also the Use Azure Service Endpoint and choose the Azure Subscription.\nWe also need to tell the Terraform where to find the tfstate file by specifying the variables for the resource group and storage account and the container\nFinally, add another Terraform task for the apply remembering to tick the install Terraform and Use Azure checkboxes\nThe arguments are\napply -auto-approve\rThis will negate the requirement for the ‚ÄúOnly ‚Äúyes‚Äù will be accepted to approve‚Äù from the manual steps post!\nBuild a Thing Now we can build the environment ‚Äì Clicking Save and Queue\nopens this dialogue\nwhere the variables can be filled in.\nThe build will be queued and clicking on the build number will open the logs\n6 minutes later the job has finished\nand the resources have been created.\nIf I want to look in the logs of the job I can click on one of the steps and take a look. This is the apply step\nDo it Again For Another Presentation So that is good, I can create my environment as I want it. Once my presentation has finished I can delete the Resource Groups. When I need to do the presentation again, I can queue another build and change the variables\nThe job will run\nand the new resource group will be created\nall ready for my next presentation üôÇ\nThis is brilliant, I can set up the same solution for different repositories for different presentations (infrastructure) and recreate the above steps.\nThe next post will show how to use Azure DevOps Task Groups to use the same build steps in multiple pipelines and build an Azure Linux SQL Server VM\nThe post after that will show how to use Azure DevOps templates to use the same build steps across many projects and build pipelines and will build a simple AKS cluster\nThe first post showed how to build an Azure SQL Database with Terraform using VS Code\n","date":"2019-04-20T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-49.png","permalink":"https://blog.robsewell.com/blog/building-azure-sql-db-with-terraform-using-azure-devops/","title":"Building Azure SQL Db with Terraform using Azure DevOps"},{"content":"I have been using Terraform for the last week or so to create some infrastructure and decided to bring that knowledge back to a problem that I and others suffer from ‚Äì building environments for presentations, all for the sake of doing some learning.\nWhat is Terraform? According to the website\nHashiCorp Terraform enables you to safely and predictably create, change, and improve infrastructure. It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned\nhttps://www.terraform.io/\nThis means that I can define my infrastructure as code. If I can do that then I can reliably do the same thing again and again, at work to create environments that have the same configuration or outside of work to repeatedly build the environment I need.\nBuilding an Azure SQL Database with Terraform To understand how to build a thing the best place to start is the documentation https://www.terraform.io/docs . For an Azure SQL Db in the docs¬†you will find a block of code that looks like this\nresource \u0026quot;azurerm_resource_group\u0026quot; \u0026quot;test\u0026quot; {\rname = \u0026quot;acceptanceTestResourceGroup1\u0026quot;\rlocation = \u0026quot;West US\u0026quot;\r}\rresource \u0026quot;azurerm_sql_server\u0026quot; \u0026quot;test\u0026quot; {\rname = \u0026quot;mysqlserver\u0026quot;\rresource_group_name = \u0026quot;${azurerm_resource_group.test.name}\u0026quot;\rlocation = \u0026quot;West US\u0026quot;\rversion = \u0026quot;12.0\u0026quot;\radministrator_login = \u0026quot;4dm1n157r470r\u0026quot;\radministrator_login_password = \u0026quot;4-v3ry-53cr37-p455w0rd\u0026quot;\r}\rresource \u0026quot;azurerm_sql_database\u0026quot; \u0026quot;test\u0026quot; {\rname = \u0026quot;mysqldatabase\u0026quot;\rresource_group_name = \u0026quot;${azurerm_resource_group.test.name}\u0026quot;\rlocation = \u0026quot;West US\u0026quot;\rserver_name = \u0026quot;${azurerm_sql_server.test.name}\u0026quot;\rtags = {\renvironment = \u0026quot;production\u0026quot;\r}\r}\rIf you read the code, you can see that there are key value pairs defining information about the resource that is being created. Anything inside a ${} is a dynamic reference. So\nresource_group_name = \u0026ldquo;${azurerm_resource_group.test.name}\u0026rdquo; refers to the name property in the azure_resource_group block called test (or the name of the resource group üôÇ )\nInfrastructure As Code So I can put that code into a file (name it main.tf) and alter it with the values and ‚Äúrun Terraform‚Äù and what I want will be created. Lets take it a step further though because I want to be able to reuse this code. Instead of hard-coding all of the values I am going to use variables. I can do this by creating another file called variables.tf which looks like\nvariable \u0026quot;presentation\u0026quot; {\rdescription = \u0026quot;The name of the presentation - used for tagging Azure resources so I know what they belong to\u0026quot;\rdefault = \u0026quot;dataindevon\u0026quot;\r}\rvariable \u0026quot;ResourceGroupName\u0026quot; {\rdescription = \u0026quot;The Resource Group Name\u0026quot;\rdefault = \u0026quot;beardrules\u0026quot;\r}\rvariable \u0026quot;location\u0026quot; {\rdescription = \u0026quot;The Azure Region in which the resources in this example should exist\u0026quot;\rdefault = \u0026quot;uksouth\u0026quot;\r}\rvariable \u0026quot;SqlServerName\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL Server to be created or to have the database on - needs to be unique, lowercase between 3 and 24 characters including the prefix\u0026quot;\rdefault = \u0026quot;jeremy\u0026quot;\r}\rvariable \u0026quot;SQLServerAdminUser\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL Server Admin user for the Azure SQL Database\u0026quot;\rdefault = \u0026quot;Beard\u0026quot;\r}\rvariable \u0026quot;SQLServerAdminPassword\u0026quot; {\rdescription = \u0026quot;The Azure SQL Database users password\u0026quot;\rdefault = \u0026quot;JonathanlovesR3ge%\u0026quot;\r}\rvariable \u0026quot;SqlDatabaseName\u0026quot; {\rdescription = \u0026quot;The name of the Azure SQL database on - needs to be unique, lowercase between 3 and 24 characters including the prefix\u0026quot;\rdefault = \u0026quot;jsdb\u0026quot;\r}\rvariable \u0026quot;Edition\u0026quot; {\rdescription = \u0026quot;The Edition of the Database - Basic, Standard, Premium, or DataWarehouse\u0026quot;\rdefault = \u0026quot;Standard\u0026quot;\r}\rvariable \u0026quot;ServiceObjective\u0026quot; {\rdescription = \u0026quot;The Service Tier S0, S1, S2, S3, P1, P2, P4, P6, P11 and ElasticPool\u0026quot;\rdefault = \u0026quot;S0\u0026quot;\r}\rand my main.tf then looks like this.\nprovider \u0026quot;azurerm\u0026quot; {\rversion = \u0026quot;=1.24.0\u0026quot;\r}\rresource \u0026quot;azurerm_resource_group\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.ResourceGroupName}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rresource \u0026quot;azurerm_sql_server\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.SqlServerName}\u0026quot;\rresource_group_name = \u0026quot;${azurerm_resource_group. presentation.name}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rversion = \u0026quot;12.0\u0026quot;\radministrator_login = \u0026quot;${var.SQLServerAdminUser}\u0026quot;\radministrator_login_password = \u0026quot;${var.SQLServerAdminPassword} \u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rresource \u0026quot;azurerm_sql_database\u0026quot; \u0026quot;presentation\u0026quot; {\rname = \u0026quot;${var.SqlDatabaseName}\u0026quot;\rresource_group_name = \u0026quot;${azurerm_sql_server.presentation. resource_group_name}\u0026quot;\rlocation = \u0026quot;${var.location}\u0026quot;\rserver_name = \u0026quot;${azurerm_sql_server.presentation. name}\u0026quot;\redition = \u0026quot;${var.Edition}\u0026quot;\rrequested_service_objective_name = \u0026quot;${var.ServiceObjective}\u0026quot;\rtags = {\renvironment = \u0026quot;${var.presentation}\u0026quot;\r}\r}\rYou can find these files in my GitHub Repository here.\nAlright ‚Äì deploy something To deploy the code that I have written I need to download Terraform from https://www.terraform.io/downloads.html and then extract the exe to a folder in my PATH. (I chose C:\\Windows). Then in Visual Studio Code I installed two extensions The Terraform Extension by Mikael Olenfalk which enables syntax highlighting and auto-completion for the tf files and the Azure Terraform extension. You will need also need Node.js from here.\nWith those in place I navigated to the directory holding my files in Visual Studio Code and pressed F1 and started typing azure terraform and chose Azure Terraform Init\nI was then prompted to use Cloud Shell and a browser opened to login. Once I had logged in I waited until I saw this\nI press F1 again and this time choose Azure Terraform plan. This is going to show me what Terraform is going to do if it applies this configuration.\nYou can see the what is going to be created. It is going to create 3 things\nOnce you have checked that the plan is what you want, press F1 again and choose Azure Terraform Apply\nYou are then asked to confirm that this is what you want. Only ‚Äúyes‚Äù will be accepted. Then you will see the infrastructure being created\nand a minute later\nand Jeremy exists in the beardrules resource group\nThen once I have finished with using the sqlinstance. I can press F1 again and choose Azure Terraform Destroy. Again there is a confirmation required.\nand you will see the progress for 46 seconds\nand all of the resources have gone.\nThats a good start. This enables me to create resources quickly and easily and keep the configuration for them safely in source control and easy to use.\nIn my next post I will create an Azure DevOps pipeline to deploy an AZure SQL Db withTerraform.\nThe post after will show how to use Azure DevOps Task Groups to use the same build steps in multiple pipelines and build an Azure Linux SQL Server VM\nThe post after that will show how to use Azure DevOps templates to use the same build steps across many projects and build pipelines and will build a simple AKS cluster\n","date":"2019-04-17T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-42.png","permalink":"https://blog.robsewell.com/blog/building-azure-sql-db-with-terraform-with-visual-studio-code/","title":"Building Azure SQL Db with Terraform with Visual Studio Code"},{"content":"In my last post I showed how to add a folder of scripts to GitHub using Visual Studio Code.\nYou can do it with Azure Data Studio as well. It‚Äôs exactly the same steps!\nThe blog post could end here but read on for some screen shots üòâ\nFollow the previous post for details of setting up a new GitHub account\nCreate a repository in GitHub\nOpen the folder in Azure Data Studio with CTRL K CTRL O (Or File ‚Äì\u0026gt; Open Folder)\nClick on the Source Control icon or CTRL + SHIFT + G and then Initialize Repository\nChoose the folder\nWrite a commit message\nSay yes to the prompt. Press CTRL + ‚Äò to open the terminal\nNavigate to the scripts folder. (I have a PSDrive set up to my Git folder)\nSet-Location GIT:\\\\ADS-Scripts\\\rand copy the code from the GitHub page after ‚Äú‚Ä¶or push an existing repository from the command line‚Äù\nand run it\nand there are your scripts in GitHub\nMake some changes to a script and it will go muddy brown\nand then write a commit message. If you click on the file name in the source control tab then you can see the changes that have been made, that are not currently tracked\nCommit the change with CTRL + ENTER and then click the roundy-roundy icon (seriously anyone know its name ?) click yes on the prompt and your changes are in GitHub as well üôÇ\nRealistically, you can use the previous post to do this with Azure Data Studio as it is built on top of Visual Studio Code but I thought it was worth showing the steps in Azure Data Studio.\nHappy Source Controlling\n","date":"2019-04-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/adding-a-folder-of-scripts-to-github-with-azure-data-studio/","title":"Adding a Folder of Scripts to GitHub with Azure Data Studio"},{"content":"Yesterday there was a tweet from Allen White.\nhttps://twitter.com/SQLRunr/status/1113862196201758720\nAllen wanted to add his scripts folder to source control but didn\u0026rsquo;t have a how to do it handy. So I thought I would write one. Hopefully this will enable someone new to GitHub and to source control get a folder of scripts under source control\nGitHub account If you do not have a GitHub account go to https://github.com and create a new account\nThere is a funky are you a human challenge\nThen you can choose your subscription\nThen answer some questions (Note - you probably want to choose different answers to the what are you interested in question! I\u0026rsquo;d suggest something technical)\nYou need to do the email verification\nNext is a very important step - Please do not skip this. You should set up 2 factor authentication. Yes even if \u0026ldquo;It\u0026rsquo;s just for me there is nothing special here\u0026rdquo;\nClick your user icon top right and then settings\nThen click set up two factor authentication\nand either set up with an app or via SMS (I suggest the app is better)\nOK - Now you have your GitHub account set up. It should have taken you less time than reading this far.\nAdd a Scripts Folder to GitHub OK, Now to add a folder of scripts to a repository. Here is my folder of scripts. They can be any type of files. I would recommend copy the folder to a specific Git folder.\nOpen VS Code - If you don\u0026rsquo;t have VS Code, download it from https://code.visualstudio.com/ From the welcome window choose open folder\nand open your scripts folder\nIn VS Code click the Source Control button\nand up at the top you will see a little icon - initialise repository\nClick that and choose your folder\nWhich will then show all of the changes to the repository (adding all the new files)\nNow we need to add a commit message for our changes. I generally try to write commit messages that are the reason why the change has been made as the what has been changed is made easy to see in VS Code (as well as other source control GUI tools)\nClick the tick or press CTRL + ENTER and this box will pop up\nI never click Always, I click yes, so that I can check if I am committing the correct files. Now we have created a local repository for our scripts folder. Our next step is to publish it to GitHub\nCreate a New Repository in GitHub In GitHub we need to create a remote repository. Click on the New Button. Give your repository a name and decide if you want it to be Public (available for anyone to search and find) or Private (only available to people you explicitly provide access to).\nThis will give you a page that looks like this\nCopy the code after ‚Ä¶or push an existing repository from the command line\n1 2 3 4 5 # make sure prompt is at right place Set-Location C:\\Git\\MyScriptsFolder # Then paste the code git remote add origin https://github.com/SQLDBAWithABeard-Test/TheBeardsFunkyScriptFolder.git git push -u origin master and paste it into PowerShell in VS Code. Make sure that your prompt is at the root of your scripts folder.\nFill in your username and password and your 2FA\nThen you will see a page like this\nand if you refresh your GitHub page you will see\nCongratulations, your code is source controlled :-)\nMaking Changes Now you can make a change to a file\nCommit your change\nHit the roundy-roundy icon (anyone know its proper name ?)\nPress OK and your commit will be pushed to GitHub :-)\nYay - Source Control all the things\n","date":"2019-04-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/adding-a-folder-of-scripts-to-github/","title":"Adding a Folder of Scripts to GitHub"},{"content":"Just a very quick post today. At the weekend I blogged about creating SQL 2019 containers with named volumes enabling you to persist your data and yesterday about creating a random workload using PowerShell and a big T-SQL script.\nThe interesting thing about creating workload is that you can break things :-)\nWhen I created a SQL 2019 container with the data files mapped to a directory on my laptops C Drive with a docker-compose like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 version: \u0026#39;3.7\u0026#39; services: 2019-CTP23: image: mcr.microsoft.com/mssql/server:2019-CTP2.3-ubuntu ports: - \u0026#34;15591:1433\u0026#34; - \u0026#34;5022:5022\u0026#34; environment: SA_PASSWORD: \u0026#34;Password0!\u0026#34; ACCEPT_EULA: \u0026#34;Y\u0026#34; volumes: - C:\\MSSQL\\BACKUP\\KEEP:/var/opt/mssql/backups - C:\\MSSQL\\DockerFiles\\datafiles:/var/opt/sqlserver - C:\\MSSQL\\DockerFiles\\system:/var/opt/mssql restore the AdventureWorks database to use the /var/opt/sqlserver directory and run a workload after a while the container stops and when you examine the logs you find\nI had a whole load of these errors\n1 2 3 4 5 6 7 8 2019-04-02 20:48:24.73 spid58 Error: 17053, Severity: 16, State: 1. 2019-04-02 20:48:24.73 spid58 FCB::MakePreviousWritesDurable: Operating system error (null) encountered. 2019-04-02 20:48:24.74 spid58 Error: 9001, Severity: 21, State: 1. 2019-04-02 20:48:24.74 spid58 The log for database \u0026#39;AdventureWorks2014\u0026#39; is not available. Check the operating system error log for related error messages. Resolve any errors and restart the database. 2019-04-02 20:48:25.05 spid58 Error: 9001, Severity: 21, State: 16. 2019-04-02 20:48:25.05 spid58 The log for database \u0026#39;AdventureWorks2014\u0026#39; is not available. Check the operating system error log for related error messages. Resolve any errors and restart the database. 2019-04-02 20:48:25.06 spid52 Error: 9001, Severity: 21, State: 16. 2019-04-02 20:48:25.06 spid52 The log for database \u0026#39;AdventureWorks2014\u0026#39; is not available. Check the operating system error log for related error messages. Resolve any errors and restart the database. Then some of these\n1 2 019-04-02 20:55:16.26 spid53 Error: 17053, Severity: 16, State: 1. 2019-04-02 20:55:16.26 spid53 /var/opt/sqlserver/AdventureWorks2014_Data.mdf: Operating system error 31(A device attached to the system is not functioning.) encountered. Then it went really bad\n1 2 3 4 5 6 2019-04-02 20:55:16.35 spid53 Error: 3314, Severity: 21, State: 3. 2019-04-02 20:55:16.35 spid53 During undoing of a logged operation in database \u0026#39;AdventureWorks2014\u0026#39; (page (0:0) if any), an error occurred at log record ID (65:6696:25). Typically, the specific failure is logged previously as an error in the operating system error log. Restore the database or file from a backup, or repair the database. 2019-04-02 20:55:16.37 spid53 Database AdventureWorks2014 was shutdown due to error 3314 in routine \u0026#39;XdesRMReadWrite::RollbackToLsn\u0026#39;. Restart for non-snapshot databases will be attempted after all connections to the database are aborted. Restart packet created for dbid 5. 2019-04-02 20:55:16.41 spid53 Error during rollback. shutting down database (location: 1). 1 after that it tried to restart the database 1 2 3 4 5 6 7 8 9 2019-04-02 20:55:16.44 spid53 Error: 3314, Severity: 21, State: 3. 2019-04-02 20:55:16.44 spid53 During undoing of a logged operation in database \u0026#39;AdventureWorks2014\u0026#39; (page (0:0) if any), an error occurred at log record ID (65:6696:25). Typically, the specific failure is logged previously as an error in the operating system error log. Restore the database or file from a backup, or repair the database. 2019-04-02 20:55:16.49 spid53 Error: 3314, Severity: 21, State: 5. 2019-04-02 20:55:16.49 spid53 During undoing of a logged operation in database \u0026#39;AdventureWorks2014\u0026#39; (page (0:0) if any), an error occurred at log record ID (65:6696:1). Typically, the specific failure is logged previously as an error in the operating system error log. Restore the database or file from a backup, or repair the database. Restart packet processing for dbid 5. 2019-04-02 20:55:17.04 spid52 [5]. Feature Status: PVS: 0. CTR: 0. ConcurrentPFSUpdate: 0. 2019-04-02 20:55:17.06 spid52 Starting up database \u0026#39;AdventureWorks2014\u0026#39;. But that caused\n1 2 3 2019-04-02 20:55:17.90 spid76 Error: 9001, Severity: 21, State: 16. 2019-04-02 20:55:17.90 spid76 The log for database \u0026#39;master\u0026#39; is not available. Check the operating system error log for related error messages. Resolve any errors and restart the database. Master eh? Now what will you do?\n1 2 3 4 5 6 2019-04-02 20:55:25.55 spid52 29 transactions rolled forward in database \u0026#39;AdventureWorks2014\u0026#39; (5:0). This is an informational message only. No user action is required. 2019-04-02 20:55:25.90 spid52 1 transactions rolled back in database \u0026#39;AdventureWorks2014\u0026#39; (5:0). This is an informational message only. No user action is required. 2019-04-02 20:55:25.90 spid52 Recovery is writing a checkpoint in database \u0026#39;AdventureWorks2014\u0026#39; (5). This is an informational message only. No user action is required. 2019-04-02 20:55:26.16 spid52 Recovery completed for database AdventureWorks2014 (database ID 5) in 7 second(s) (analysis 424 ms, redo 5305 ms, undo 284 ms.) This is an informational message only. No user action is required. 2019-04-02 20:55:26.21 spid52 Parallel redo is shutdown for database \u0026#39;AdventureWorks2014\u0026#39; with worker pool size [1]. 2019-04-02 20:55:26.27 spid52 CHECKDB for database \u0026#39;AdventureWorks2014\u0026#39; finished without errors on 2018-03-24 00:38:39.313 (local time). This is an informational message only; no user action is required. Interesting, then back to this.\n1 2 3 4 5 2019-04-02 21:00:00.57 spid51 Error: 17053, Severity: 16, State: 1. 2019-04-02 21:00:00.57 spid51 FCB::MakePreviousWritesDurable: Operating system error (null) encountered. 2019-04-02 21:00:00.62 spid51 Error: 9001, Severity: 21, State: 1. 2019-04-02 21:00:00.62 spid51 The log for database \u0026#39;AdventureWorks2014\u0026#39; is not available. Check the operating system error log for related error messages. Resolve any errors and restart the database. 2019-04-02 21:00:00.64 spid51 Error: 9001, Severity: 21, State: 16. It did all that again before\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 This program has encountered a fatal error and cannot continue running at Tue Apr 2 21:04:08 2019 The following diagnostic information is available: Reason: 0x00000004 Message: RETAIL ASSERT: Expression=(false) File=Thread.cpp Line=4643 Description=Timed out waiting for thread terminate/suspend/resume. Stacktrace: 000000006af30187 000000006af2836a 000000006ae4a4d1 000000006ae48c55 000000006af6ab5e 000000006af6ac04 00000002809528df Process: 7 - sqlservr Thread: 129 (application thread 0x1e8) Instance Id: 215cfcc9-8f69-4869-9a52-5aa44a415a83 Crash Id: 53e98400-33f1-4786-98fd-484f0c8d9a7e Build stamp: 0e53295d0e1704ae5b221538dd6e2322cd46134e0cc32be49c887ca84cdb8c10 Distribution: Ubuntu 16.04.6 LTS Processors: 2 Total Memory: 4906205184 bytes Timestamp: Tue Apr 2 21:04:08 2019 Ubuntu 16.04.6 LTS Capturing core dump and information to /var/opt/mssql/log... /usr/bin/find: \u0026#39;/proc/7/task/516\u0026#39;: No such file or directory dmesg: read kernel buffer failed: Operation not permitted No journal files were found. No journal files were found. Attempting to capture a dump with paldumper WARNING: Capture attempt failure detected Attempting to capture a filtered dump with paldumper WARNING: Attempt to capture dump failed. Reference /var/opt/mssql/log/core.sqlservr.7.temp/log/paldumper-debug.log for details Attempting to capture a dump with gdb WARNING: Unable to capture crash dump with GDB. You may need to allow ptrace debugging, enable the CAP_SYS_PTRACE capability, or run as root. failing to capture it\u0026rsquo;s dump!! Oops :-)\nI had to recreate the containers without using the named volumes and then I could run my workload :-)\nNothing particularly useful about this blog post other than an interesting look at the error log when things go wrong :-)\n","date":"2019-04-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/how-to-break-a-sql-2019-container-on-my-laptop/","title":"How to break a SQL 2019 container on my laptop"},{"content":"For a later blog post I have been trying to generate some workload against an AdventureWorks database.\nI found this excellent blog post by Pieter Vanhove t https://blogs.technet.microsoft.com/msftpietervanhove/2016/01/08/generate-workload-on-your-azure-sql-database/ which references this 2011 post by Jonathan Kehayias t\nhttps://www.sqlskills.com/blogs/jonathan/the-adventureworks2008r2-books-online-random-workload-generator/\nBoth of these run a random query in a single thread so I thought I would use PoshRSJob by Boe Prox b | t to run multiple queries at the same time üôÇ\nTo install PoshRSJob, like with any PowerShell module, you run\nInstall-Module -Name PoshRSJob\rI downloaded AdventureWorksBOLWorkload zip from Pieters blog post and extracted to my C:\\temp folder. I created a Invoke-RandomWorkload function which you can get from my functions repository in GitHub. The guts of the function are\n1.. $NumberOfJobs | Start-RSJob -Name \u0026quot;WorkLoad\u0026quot; -Throttle $Throttle -ScriptBlock {\r# Get the queries\r$Queries = Get-Content -Delimiter $Using:Delimiter -Path $Using:PathToScript # Pick a Random Query from the input object $Query = Get-Random -InputObject $Queries # Run the Query\rInvoke-SqlCmd -ServerInstance $Using:SqlInstance -Credential $Using:SqlCredential -Database $Using:Database -Query $Query # Choose a random number of milliseconds to wait\r$a = Get-Random -Maximum 2000 -Minimum 100; Start-Sleep -Milliseconds $a; } which will created $NumberOfJobs jobs and then run $Throttle number of jobs in the background until they have all completed. Each job will run a random query from the query file using Invoke-SqlCmd. Why did I use Invoke-SqlCmd and not Invoke-DbaQuery from dbatools? dbatools creates runspaces in the background to help with logging and creating runspaces inside background jobs causes errors\nThen I can run the function with\nInvoke-RandomWorkload -SqlInstance $SQL2019CTP23 -SqlCredential $cred -Database AdventureWorks2014 -NumberOfJobs 1000 -Delay 10 -Throttle 10\rand create a random workload. Creating lots of background jobs takes resources so when I wanted to run a longer workload I created a loop.\n$x = 10\rwhile($X -gt 0){\rInvoke-RandomWorkload -SqlInstance $SQL2019CTP23 -SqlCredential $cred -Database AdventureWorks2014 -NumberOfJobs 1000 -Delay 10 -Throttle 10\r$x --\r}\rYou can get the function here. The full code is below\n# With thanks to Jonathan Kehayias and Pieter Vanhove\r\u0026lt;#\r.SYNOPSIS\rRuns a random workload against a database using a sql file\r.DESCRIPTION\rRuns a random workload against a database using PoshRSJobs to create parallel jobs to run random queries from a T-SQL file by default it uses the AdventureWorksBOLWorkload.sql from Pieter Vanhove\r.PARAMETER SqlInstance\rThe SQL instance to run the queries against\r.PARAMETER SqlCredential\rThe SQL Credential for the Instance if required\r.PARAMETER Database\rThe name of the database to run the queries against\r.PARAMETER NumberOfJobs\rThe number of jobs to create - default 10\r.PARAMETER Delay\rThe delay in seconds for the output for the running jobs - default 10\r.PARAMETER Throttle\rThe number of parallel jobs to run at a time - default 5\r.PARAMETER PathToScript\rThe path to the T-SQL script holding the queries - default 'C:\\temp\\AdventureWorksBOLWorkload\\AdventureWorksBOLWorkload. sql'\r.PARAMETER Delimiter\rThe delimiter in the T-SQL Script between the queries - default ------\r.PARAMETER ShowOutput\rShows the output from the jobs\r.EXAMPLE\rInvoke-RandomWorkload -SqlInstance $SQL2019CTP23 -SqlCredential $cred -Database AdventureWorks2014 -NumberOfJobs 100 -Delay 10 -Throttle 10 Runs 100 queries with a maximum of 10 at a time against the AdventureWorks2014 database on $SQL2019CTP23\r.EXAMPLE\r$x = 10\rwhile($X -gt 0){\rInvoke-RandomWorkload -SqlInstance $SQL2019CTP23 -SqlCredential $cred -Database AdventureWorks2014 -NumberOfJobs 1000 -Delay 10 -Throttle 10\r$x --\r}\rRuns 1000 queries with a maximum of 10 at a time against the AdventureWorks2014 database on $SQL2019CTP23 10 times in a loop\r.NOTES\rWith thanks to Pieter Vanhove\rhttps://blogs.technet.microsoft.com/msftpietervanhove/2016/01/08/generate-workload-on-your-azure-sql-database/\rand\rJonathan Kehayias\rhttps://www.sqlskills.com/blogs/jonathan/ the-adventureworks2008r2-books-online-random-workload-generator /\r\u0026gt;\rfunction Invoke-RandomWorkload {\r#Requires -Module PoshRsJob\r#Requires -Module SQLServer\rParam(\r[string]$SqlInstance,\r[pscredential]$SqlCredential,\r[string]$Database,\r[int]$NumberOfJobs = 10,\r[int]$Delay = 10,\r[int]$Throttle = 5,\r[string]$PathToScript = 'C:\\temp\\AdventureWorksBOLWorkload\\AdventureWorksBOLWorkload. sql',\r[string]$Delimiter = \u0026quot;------\u0026quot;,\r[switch]$ShowOutput\r)\r#Check if there are old Workload Jobs $WorkloadJobs = Get-RSJob -Name Workload if ($WorkloadJobs) {\rWrite-Output \u0026quot;Removing Old WorkLoad Jobs\u0026quot;\r$WorkloadJobs |Stop-RSJob\r$WorkloadJobs | Remove-RSJob\r}\rWrite-Output \u0026quot;Creating Background Jobs\u0026quot; 1.. $NumberOfJobs | Start-RSJob -Name \u0026quot;WorkLoad\u0026quot; -Throttle $Throttle -ScriptBlock {\r# Get the queries\r$Queries = Get-Content -Delimiter $Using:Delimiter -Path $Using:PathToScript # Pick a Random Query from the input object $Query = Get-Random -InputObject $Queries # Run the Query\rInvoke-SqlCmd -ServerInstance $Using:SqlInstance -Credential $Using:SqlCredential -Database $Using:Database -Query $Query # Choose a random number of milliseconds to wait\r$a = Get-Random -Maximum 2000 -Minimum 100; Start-Sleep -Milliseconds $a; } $runningJobs = (Get-RSJob -Name WorkLoad -State Running). Count\rWhile ($runningJobs -ne 0) {\r$jobs = Get-RSJob -Name WorkLoad\r$runningJobs = $Jobs.Where{$PSItem.State -eq 'Running'} .Count\r$WaitingJobs = $Jobs.Where{$PSItem.State -eq 'NotStarted'}.Count\r$CompletedJobs = $Jobs.Where{$PSItem.State -eq 'Completed'}.Count\rWrite-Output \u0026quot;$runningJobs jobs running - $WaitingJobs jobs waiting - $CompletedJobs -jobs finished\u0026quot;\rStart-Sleep -Seconds $Delay\r}\rWrite-Output \u0026quot;Jobs have finished\u0026quot;\rif ($ShowOutput) {\rWrite-Output \u0026quot;WorkLoad Jobs Output below -\u0026quot;\rGet-RSJob -Name WorkLoad | Receive-RSJob\r}\rWrite-Output \u0026quot;Removing Old WorkLoad Jobs\u0026quot;\rGet-RSJob -Name WorkLoad | Remove-RSJob\rWrite-Output \u0026quot;Finished\u0026quot;\r}\r","date":"2019-04-02T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/04/image-51.png","permalink":"https://blog.robsewell.com/blog/generating-a-workload-against-adventureworks-with-powershell/","title":"Generating a Workload against AdventureWorks with PowerShell"},{"content":"With all things containers I refer to my good friend Andrew Pruski. Known as dbafromthecold on twitter he blogs at https://dbafromthecold.com\nI was reading his latest blog post Using docker named volumes to persist databases in SQL Server and decided to give it a try.\nHis instructions worked perfectly and I thought I would try them using a docker-compose file as I like the ease of spinning up containers with them.\nI created a docker-compose file like this which will map my backup folder on my Windows 10 laptop to a directory on the container and two more folders to the system folders on the container in the same way as Andrew has in his blog.\nversion: '3.7'\rservices:\r2019-CTP23:\rimage: mcr.microsoft.com/mssql/server:2019-CTP2. 3-ubuntu\rports: - \u0026quot;15591:1433\u0026quot;\r- \u0026quot;5022:5022\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rvolumes: - C:\\MSSQL\\BACKUP\\KEEP:/var/opt/mssql/backups\r- C:\\MSSQL\\DockerFiles\\datafiles:/var/opt/sqlserver\r- C:\\MSSQL\\DockerFiles\\system:/var/opt/mssql\rand then from the directory I ran\ndocker-compose up -d\rThis will build the containers as defined in the docker-compose file. The -d runs the container in the background. This was the result.\nUPDATE ‚Äì 2019-03-27\nI have no idea why, but today it has worked as expected using the above docker-compose file. I had tried this a couple of times, restarted docker and restarted my laptop and was consistently getting the results below ‚Äì however today it has worked\nSo feel free to carry on reading, it‚Äôs a fun story and it shows how you can persist the databases in a new container but the above docker-compose has worked!\nThe command completed successfully but as you can see on the left the container is red because it is not running. (I am using the Docker Explorer extension for Visual Studio C\nI inspected the logs from the container using\ndocker logs ctp23_2019-CTP23_1\rwhich returned\nThis is an evaluation version. There are [153] days left in the evaluation period.\nThis program has encountered a fatal error and cannot continue running at Tue Mar 26 19:40:35 20\n19\nThe following diagnostic information is available:\nReason: 0x00000006 Status: 0x40000015 Message: Kernel bug check Address: 0x6b643120\nParameters: 0x10861f680\nStacktrace: 000000006b72d63f 000000006b64317b 000000006b6305ca\n000000006b63ee02 000000006b72b83a 000000006b72a29d\n000000006b769c02 000000006b881000 000000006b894000\n000000006b89c000 0000000000000001\nProcess: 7 ‚Äì sqlservr\nThread: 11 (application thread 0x4)\nInstance Id: e01b154f-7986-42c6-ae13-c7d34b8b257d\nCrash Id: 8cbb1c22-a8d6-4fad-bf8f-01c6aa5389b7\nBuild stamp: 0e53295d0e1704ae5b221538dd6e2322cd46134e0cc32be49c887ca84cdb8c10\nDistribution: Ubuntu 16.04.6 LTS\nProcessors: 2\nTotal Memory: 4906205184 bytes\nTimestamp: Tue Mar 26 19:40:35 2019\nUbuntu 16.04.6 LTS\nCapturing core dump and information to /var/opt/mssql/log‚Ä¶\ndmesg: read kernel buffer failed: Operation not permitted\nNo journal files were found.\nNo journal files were found.\nAttempting to capture a dump with paldumper\nWARNING: Capture attempt failure detected\nAttempting to capture a filtered dump with paldumper\nWARNING: Attempt to capture dump failed. Reference /var/opt/mssql/log/core.sqlservr.7.temp/log/\npaldumper-debug.log for details\nAttempting to capture a dump with gdb\nWARNING: Unable to capture crash dump with GDB. You may need to\nallow ptrace debugging, enable the CAP_SYS_PTRACE capability, or\nrun as root.\nwhich told me that ‚Ä¶‚Ä¶‚Ä¶‚Ä¶. it hadn‚Äôt worked. So I removed the containers with\ndocker-compose down\rI thought I would create the volumes ahead of time like Andrew‚Äôs blog had mentioned with\ndocker volume create mssqlsystem\rdocker volume create mssqluser\rand then use the volume names in the docker-compose file mapped to the system folders in the container, this time the result was\nERROR: Named volume ‚Äúmssqlsystem:/var/opt/sqlserver:rw‚Äù is used in service ‚Äú2019-CTP23‚Äù but no declaration was found in the volumes section.\nSo that didn\u0026rsquo;t work either üôÇ\nI decided to inspect the volume definition using\ndocker volume inspect mssqlsystem\rI can see the mountpoint is /var/lib/docker/volumes/mssqlsystem/_data so I decided to try a docker-compose like this\nversion: \u0026lsquo;3.7\u0026rsquo;\nservices: 2019-CTP23: image: mcr.microsoft.com/mssql/server:2019-CTP2.3-ubuntu ports:\n- \u0026ldquo;15591:1433\u0026rdquo; - \u0026ldquo;5022:5022\u0026rdquo; environment: SA_PASSWORD: \u0026ldquo;Password0!\u0026rdquo; ACCEPT_EULA: \u0026ldquo;Y\u0026rdquo; volumes: - C:\\MSSQL\\BACKUP\\KEEP:/var/opt/mssql/backups - /var/lib/docker/volumes/mssqluser/_data:/var/opt/sqlserver - /var/lib/docker/volumes/mssqlsystem/_data:/var/opt/mssql\nand then ran docker-compose up without the -d flag so that I could see all of the output\nYou can see in the output that the system database files are being moved. That looks like it is working so I used CTRL + C to stop the container and return the terminal. I then ran docker-compose up -d and\nI created a special database for Andrew.\nThis made me laugh out loud‚Ä¶as there\u0026rsquo;s a strong possibility that could happen https://t.co/sh0pnhtPQy\n‚Äî Andrew Pruski üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø (@dbafromthecold) March 23, 2019\nI could then remove the container with\ndocker-compose down\rTo make sure there is nothing up my sleeve I altered the docker-compose file to use a different name and port but kept the volume definitions the same.\nversion: '3.7'\rservices:\r2019-CTP23-Mk1:\rimage: mcr.microsoft.com/mssql/server:2019-CTP2. 3-ubuntu\rports: - \u0026quot;15592:1433\u0026quot;\r- \u0026quot;5022:5022\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rvolumes: - C:\\MSSQL\\BACKUP\\KEEP:/var/opt/mssql/backups\r- /var/lib/docker/volumes/mssqluser/_data:/var/opt/sqlserver\r- /var/lib/docker/volumes/mssqlsystem/_data:/var/opt/mssql\rI ran docker-compose up -d again and connected to the new container and lo and behold the container is still there\nSo after doing this, I have learned that to persist the databases and to use docker-compose files I had to map the volume to the mountpoint of the docker volume. Except I haven‚Äôt, I have learned that sometimes weird things happen with Docker on my laptop!!\n","date":"2019-03-26T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/03/image-27.png","permalink":"https://blog.robsewell.com/blog/persisting-databases-with-named-volumes-on-windows-with-docker-compose/","title":"Persisting databases with named volumes on Windows with docker compose"},{"content":"Azure Data Studio is a cross-platform database tool for data professionals using the Microsoft family of on-premises and cloud data platforms on Windows, MacOS, and Linux.\nRecently Vicky Harp tweeted\nWe\u0026rsquo;re getting very close to release of SQL Notebooks in @AzureDataStudio! You can give the feature an early spin today with the insider build. pic.twitter.com/SEZp7ZdxCp\n‚Äî Vicky Harp (@vickyharp) March 8, 2019\nBy the way, you can watch a recording from SQLBits of Vicky‚Äôs session\nIf you missed #sqlbits, you will definitely want to watch this demo by @vickyharp and @MGoCODE about @AzureDataStudio. Learn the latest about our cross-platform tool, including a new feature, SQL Notebooks #SQLServer https://t.co/diubYwQckn\n‚Äî Azure Data Studio (@AzureDataStudio) March 7, 2019\nSo in the interest of learning about something new I decided to give it a try.\nInstall The Insiders Edition Unlike Visual Studio Code which has a link to the insiders download on the front page, you will have to visit the GitHub repository for the links to download the insiders release of Azure Data Studio. Scroll down and you will see\nTry out the latest insiders build from¬†master:\nWindows User Installer ‚Äì¬†Insiders build Windows System Installer ‚Äì¬†Insiders build Windows ZIP ‚Äì¬†Insiders build macOS ZIP ‚Äì¬†Insiders build Linux TAR.GZ ‚Äì¬†Insiders build See the¬†change log¬†for additional details of what‚Äôs in this release. Once you have installed you can connect to an instance, right click and choose New Notebook or you can use File ‚Äì New Notebook Incidentally, I use the docker-compose file here to create the containers and I map C:\\MSSQL\\BACKUP\\KEEP on my local machine (where my backups are) to /var/opt/mssql/backups on the containers on lines 10 and 17 of the docker-compose so change as required . If you want to follow along then put the ValidationResults.bak in the folder on your local machine. The Create-Ag.ps1 shows the code and creates an AG with dbatools. But I digress!\nInstall Notebook Dependencies Once you click New Notebook you will get a prompt to install the dependencies.\nIt will show its output\nand take a few minutes to run\nIt took all but 11 minutes on my machine\nCreate a Notebook OK, so now that we have the dependencies installed we can create a notebook. I decided to use the ValidationResults database that I use for my dbachecks demos and describe here. I need to restore it from my local folder that I have mapped as a volume to my container. Of course, I use dbatools for this üôÇ\n# U: sqladmin P: dbatools.IO\r$cred = Get-Credential\r$restoreDbaDatabaseSplat = @{\rSqlInstance = $sqlinstance1\rSqlCredential = $cred\rUseDestinationDefaultDirectories = $true\rPath = '/var/opt/mssql/backups/ValidationResults.bak'\r}\rRestore-DbaDatabase @restoreDbaDatabaseSplat\rI had already got a connection saved to the instance in Azure Data Studio, you may need to create a new one using the new connection icon at the top left and filling in the details. The password is in the code above.\nNow I can start with my notebook. I am faced with this\nI click on text and provide an intro\nOnce I had written that and clicked out, I couldn‚Äôt see what to do straight away!\nThen I saw the code and text buttons at the top üôÇ Right, lets get on with it üôÇ I hit the code button and paste in the T-SQL to reset the dates in the database to simulate dbachecks having been run this morning.\nThere‚Äôs a run cell button on the right and when I press it\u0026gt; Cool üôÇ\nIf the SQL query has results then they are shown as well\nThis is fun and I can see plenty of uses for it. Go and have a play with SQL notebooks üôÇ\nSource Control I used CTRL K, CTRL O to open a folder and saved my notebook in my local Presentations folder which is source controlled. When I opened the explorer CTRL + SHIFT + E I can see that the folder and the file are colour coded green and have a U next to them marking them as Untracked. I can also see that the source control icon has a 1 for the number of files with changes and in the bottom left that I am in the master branch.\nIf I click on the source control icon (or CTRL + SHIFT + G) I can see the files with the changes and can enter a commit message\nI then press CTRL + ENTER to commit my change and get this pop-up\nAs I only have one file and it has all the changes for this commit I click yes. If I had changed more than one file and only wanted to commit a single one at a time I would hover my mouse over the file and click the + to stage my change.\nIf I make a further change to the notebook and save it, I can see that the source control provider recognises the change but this time the folder the file is in and the file are colour coded brown with an M to show that they have been modified.\nUnlike Visual Studio Code, when you then click on the source control icon and click on the change it does not show the differences in the notebook although this works with SQL files.\nWhen I have made all my changes and committed them with good commit messages\nI can see that there are 3 local changes ready to be pushed to by remote repository (GitHub in this case) and 0 remote commits in this branch by looking at the bottom left\nI can click on the ‚Äúroundy roundy‚Äù icon (I don\u0026rsquo;t know its proper name üòä) and synchronise my changes. This comes with a pop-up\nPersonally I never press OK, Don‚Äôt Show Again because I like the double check and to think ‚ÄúIs this really what I want to do right now‚Äù. Once I press OK my changes will be synched with the remote repository. Explaining this means that you can find the notebook I have used in my Presentations GitHub Repository which means that you can run the Notebook too using the docker-compose file here and the instructions further up in the post.\n","date":"2019-03-13T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/03/image-7.png","permalink":"https://blog.robsewell.com/blog/whats-a-sql-notebook-in-azure-data-studio/","title":"Whats a SQL Notebook in Azure Data Studio?"},{"content":"\nThe topic for this months¬†T-SQL Tuesday #112¬†hosted by Shane O‚ÄôNeill (Blog¬†/¬†Twitter) is about ‚Äúdipping into your cookie jar‚Äù. This reference means ‚Äúwhen times get tough how do you dip into your reserves to keep going‚Äù. Shane asks the following:\nThat is what I want from the contributors of this T-SQL Tuesday, those memories that they can think back on for sustenance. Like the humble cookie, I want a humble brag.\nMmmm Cookies Photo by¬†Pille-Riin Priske¬†on¬†Unsplash\nI‚Äôm not good at bragging, I‚Äôm generally convinced that all of you are better than me. Yes, I am aware that it is irrational. This has made writing this post really hard. Sure, I get immense pleasure and satisfaction from solving a problem, that‚Äôs a form of instant fulfillment. Certainly, I enjoy teaching people and passing over my knowledge for them to use. I am not going to write about technical things that I have done because they don‚Äôt give me sustenance in that way.\nSo what does give me sustenance when times are hard?\nPeople.\nThe things I am most proud of are the things other people do where I have played a small part. These are the things I look back at and help to energise me. Things like\nA couple of people who I suggested started writing blogs and then speaking who are now seen as experts in their niche. The people I mentored as new speakers who are now speaking all over the continent. The most recent story was a DBA who sat in a full day pre-con at a SQL Saturday, took loads of notes and waited at the end to ask questions. We were looking at some code and she was telling me it wasn‚Äôt very good and apologising for it. It was good, it performed the required actions over a large estate and I told her so. I asked about her job and with a big sigh, she told a story of being stuck in a rut, dealing with a lot of legacy systems, not enjoying it and not being able to move on. We had a long talk.\nCut to this years SQL Bits and she came running up to me all energised. She has a new job, doing something ‚ÄúCool in the cloud‚Äù, she said the things she had learned had helped her to land this role.\nIn all of these cases, it is the person involved who has done all of the hard work but it is these things that keep me going. The thank yous and the smiles I see on those peoples faces as they do the thing that they love and enjoy their success and progression üôÇ\nCake !!!!! Photo by¬†Prince Abid¬†on¬†Unsplash\nHey, thats cake and not cookies Rob.\nI know. The biggest thing that keeps me going when times are tough though is the security I am able to provide. Nearly 20 years ago my life was very different. Without a job, I‚Äôd had to give up a career, struggling dealing with my wife‚Äôs serious illnesses, suddenly responsible for the entire household without the means to provide, I was in a very bleak place and saw no way out.\nSo to have found a career that is my hobby, to be able to work and also to have fun, to have a social world that provides me with friends and entertainment in many countries and the opportunity to experience different cultures and still be able to live comfortably. Thats a blessing and what keeps me going.\nAlso being able to pay my dad back for turning up with sacks of potatoes by taking him to football matches and comedy shows üôÇ\nAcknowledge what you have got, tell your loved ones that you love them, enjoy life and use your cookies when you need them but don‚Äôt forget the cake üôÇ\n","date":"2019-03-12T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2019/03/pille-riin-priske-1238490-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/tsql2sday-nomnomnomnomnom/","title":"#TSQL2sDay ‚Äì NomNomNomNomNom"},{"content":"Saturday 27th April is Global Azure Bootcamp day\nWhat‚Äôs Global Azure Bootcamp?\nThe website says it best\n‚Ä¶. communities will come together once again in the sixth great Global Azure Bootcamp event! Each user group will organize their own one day deep dive class on Azure the way they see fit and how it works for their members. The result is that thousands of people get to learn about Azure and join together online under the social hashtag #GlobalAzure!\nSaturday Is Free Learning I am a part of the team organising the event in Exeter. Now there is a little story here. We had chosen this date by chance to hold an event we call Data In Devon giving people in the South West (of UK) the chance to access a whole day of high quality data and technical sessions for free on a Saturday.\nWhen the Global Azure Bootcamp was announced, we had a conversation with the organisers and they agreed that we could add Exeter as a venue as we had already decided to have a whole track dedicated to Azure. You can find our schedule here https://sqlsouthwest.co.uk/data-in-devon-saturday-schedule/ and you can register to attend via this form¬†Now, we have some costs obviously, not a lot but venues are not free and neither is food üòâ. We have a couple of sponsors (feel free to contact me if your company is interested in sponsoring the event) but we also have some paid training days on Friday 25th April.\nFriday Is Training Day It‚Äôs a great opportunity to get cheap high-quality training from some of the best in their areas of expertise. There are still some tickets for ¬£175 and the price will rise only to ¬£200. I think that ¬£200 is fantastic value to be able to spend a day learning from\nAlex Whittles ‚Äì Data Platform MVP ‚Äì Bi in Azure John Martin ‚Äì Data Platform MVP ‚Äì Infrastructure as Code with Terraform Terry McCann ‚Äì Data Platform MVP ‚Äì Machine Learning: From model to production using the cloud, containers and Dev Ops¬†William Durkin ‚Äì Data Platform MVP ‚Äì Performance Pain Reduction for Data Platform Projects\nand myself ‚Äì Getting up to speed with PowerShell\nYou can sign up for any of these sessions by following the instructions here https://sqlsouthwest.co.uk/training-day-schedule/#Pricing We don‚Äôt have a fancy website or booking system as we wanted to keep costs down.\nThe details of my training day are below\nGetting up to speed with PowerShellS PowerShell is cross-platform, it works exactly the same on Windows, on Linux and Mac. It is awesome for automation and amazing for administration.\nWe will cover\nthe basics about PowerShell, PowerShell security how to open PowerShell , how to install PowerShell . 4 vital commands to enable you to be able to help yourself The PowerShell Gallery and how to find, install and use additional modules Reading the language Working with output Why Red text is a good thing and how to learn from the errors We will even delve into scripting with PowerShell and how to validate your environment There will also be the opportunity to learn about any areas of PowerShell, Automation, CI/CD that you have questions about. This is a beginner level session in which I will teach you to be comfortable with PowerShell and confident in being able to use it in the future\nAttendees wanting to follow along should bring a laptop.\n","date":"2019-03-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/%23dataindevon-getting-up-to-speed-with-powershell-or-spend-a-day-with-one-of-four-other-mvps-/","title":"#DataInDevon ‚Äì Getting up to speed with PowerShell or spend a day with one of four other MVPs :-)"},{"content":"My wonderful friend Andr√© Kamman wrote a fantastic blog post this week SQL Server Container Instances via Cloudshell about how he uses containers in Azure to test code against different versions of SQL Server.\nIt reminded me that I do something very similar to test dbachecks code changes. I thought this might make a good blog post. I will talk through how I do this locally as I merge a PR from another great friend Cl√°udio Silva who has added agent job history checks.\nGitHub PR VS Code Extension I use the GitHub Pull Requests extension for VS Code to work with pull requests for dbachecks. This enables me to see all of the information about the Pull Request, merge it, review it, comment on it all from VS Code\nI can also see which files have been changed and which changes have been made\nOnce I am ready to test the pull request I perform a checkout using the extension\nThis will update all of the files in my local repository with all of the changes in this pull request\nYou can see at the bottom left that the branch changes from development to the name of the PR.\nRunning The Unit Tests The first thing that I do is to run the Unit Tests for the module. These will test that the code is following all of the guidelines that we require and that the tests are formatted in the correct way for the Power Bi to parse. I have blogged about this here and here and we use this Pester in our CI process in Azure DevOps which I described here.\nI navigate to the root of the dbachecks repository on my local machine and run\n$testresults = Invoke-Pester .\\tests -ExcludeTag Integration -Show Fails -PassThru and after about a minute\nThank you Cl√°udio, the code has passed the tests üòâ\nRunning Some Integration Tests The difference between Unit tests and Integration tests in a nutshell is that the Unit tests are testing that the code is doing what is expected without any other external influences whilst the Integration tests are checking that the code is doing what is expected when running on an actual environment. In this scenario we know that the code is doing what is expected but we want to check what it does when it runs against a SQL Server and even when it runs against multiple SQL Servers of different versions.\nMultiple Versions of SQL Server As I have described before my friend and former colleague Andrew Pruski b | t has many resources for running SQL in containers. This means that I can quickly and easily create fresh uncontaminated instances of SQL 2012, 2014, 2016 and 2017 really quickly.\nI can create 4 instances of different versions of SQL in (a tad over) 1 minute. How about you?\nImagine how long it would take to run the installers for 4 versions of SQL and the pain you would have trying to uninstall them and make sure everything is ‚Äòclean‚Äô. Even images that have been sysprep‚Äôd won‚Äôt be done in 1 minute.\nDocker Compose Up ? So what is this magic command that has enabled me to do this? docker compose uses a YAML file to define multi-container applications. This means that with a file called docker-compose.yml like this\nversion: '3.7'\rservices:\rsql2012:\rimage: dbafromthecold/sqlserver2012dev:sp4\rports: - \u0026quot;15589:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2014:\rimage: dbafromthecold/sqlserver2014dev:sp2\rports: - \u0026quot;15588:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2016:\rimage: dbafromthecold/sqlserver2016dev:sp2\rports: - \u0026quot;15587:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2017:\rimage: microsoft/ mssql-server-windows-developer:2017-latest\rports: - \u0026quot;15586:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rand in that directory just run\ndocker-compose up -d\rand 4 SQL containers are available to you. You can interact with them via SSMS if you wish with localhost comma PORTNUMBER. The port numbers in the above file are 15586, 15587,15588 and 15589\n](https://blog.robsewell.com/assets/uploads/2019/01/containers.png?ssl=1)\nNow it must be noted, as I describe here that first I pulled the images to my laptop. The first time you run docker compose will take significantly longer if you haven‚Äôt pulled the images already (pulling the images will take quite a while depending on your broadband speed)\nCredential The next thing is to save a credential to make it easier to automate.I use the method described by my PowerShell friend Jaap Brasser here.\nEDIT (September or is it March? 2020) - Nowadays I use the Secret Management Module\nI run this code\n$CredentialPath = 'C:\\MSSQL\\BACKUP\\KEEP\\sacred.xml'\rGet-Credential | Export-Clixml -Path $CredentialPath\rand then I can create a credential object using\n$cred = Import-Clixml $CredentialPath Check The Connections I ensure a clean session by removing the dbatools and dbachecks modules and then import the local version of dbachecks and set some variables\n$dbacheckslocalpath = 'GIT:\\dbachecks\\'\rRemove-Module dbatools, dbachecks -ErrorAction SilentlyContinue\rImport-Module $dbacheckslocalpath\\dbachecks.psd1\r$cred = Import-Clixml $CredentialPath $containers = 'localhost,15589', 'localhost,15588', 'localhost, 15587', 'localhost,15586'\rNow I can start to run my Integration tests. First reset the dbachecks configuration and set some configuration values\n# run the checks against these instances\r$null = Set-DbcConfig -Name app.sqlinstance $containers\r# We are using SQL authentication\r$null = Set-DbcConfig -Name policy.connection.authscheme -Value SQL\r# sometimes its a bit slower than the default value\r$null = Set-DbcConfig -Name policy.network.latencymaxms -Value 100 # because the containers run a bit slow!\rThen I will run the dbachecks connectivity checks and save the results to a variable without showing any output\n$ConnectivityTests = Invoke-DbcCheck -SqlCredential $cred -Check Connectivity -Show None -PassThru\rI can then use Pester to check that dbachecks has worked as expected by testing if the failedcount property returned is 0.\nDescribe \u0026quot;Testing the checks are running as expected\u0026quot; -Tag Integration {\rContext \u0026quot;Connectivity Checks\u0026quot; {\rIt \u0026quot;All Tests should pass\u0026quot; {\r$ConnectivityTests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass with default settings\u0026quot;\r}\r}\r}\rWhat is the Unit Test for this PR? Next I think about what we need to be testing for the this PR. The Unit tests will help us.\nChoose some Integration Tests This check is checking the Agent job history settings and the unit tests are\nIt ‚ÄúPasses Check Correctly with Maximum History Rows disabled (-1)‚Äù\nIt ‚ÄúFails Check Correctly with Maximum History Rows disabled (-1) but configured value is 1000‚Äù\nIt ‚ÄúPasses Check Correctly with Maximum History Rows being 10000‚Äù\nIt ‚ÄúFails Check Correctly with Maximum History Rows being less than 10000‚Äù\nIt ‚ÄúPasses Check Correctly with Maximum History Rows per job being 100‚Äù\nIt ‚ÄúFails Check Correctly with Maximum History Rows per job being less than 100‚Äù\nSo we will check the same things on real actual SQL Servers. First though we need to start the SQL Server Agent as it is not started by default. We can do this as follows\ndocker exec -ti integration_sql2012_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2014_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2016_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2017_1 powershell start-service SQLSERVERAGENT\rUnfortunately, the agent service wont start in the SQL 2014 container so I cant run agent integration tests for that container but it‚Äôs better than no integration tests.\nThis is What We Will Test So we want to test if the check will pass with default settings. In general, dbachecks will pass for default instance, agent or database settings values by default.\nWe also want the check to fail if the configured value for dbachecks is set to default but the value has been set on the instance.\nWe want the check to pass if the configured value for the dbachecks configuration is set and the instance (agent, database) setting matches it.\nIf You Are Doing Something More Than Once ‚Ä¶‚Ä¶ Let‚Äôs automate that. We are going to be repeatedly running those three tests for each setting that we are running integration tests for. I have created 3 functions for this again checking that FailedCount or Passed Count is 0 depending on the test.\nfunction Invoke-DefaultCheck {\rIt \u0026quot;All Checks should pass with default for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check)default\u0026quot; -ValueOnly\r$Tests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass with default setting (Yes we may set some values before but you get my drift)\u0026quot;\r}\r}\rfunction Invoke-ConfigCheck {\rIt \u0026quot;All Checks should fail when config changed for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check)configchanged\u0026quot; -ValueOnly\r$Tests.PassedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and fail when we have changed the config values\u0026quot;\r}\r}\rfunction Invoke-ValueCheck {\rIt \u0026quot;All Checks should pass when setting changed for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check) value changed\u0026quot; -ValueOnly\r$Tests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass when we have changed the settings to match the config values\u0026quot;\r}\r}\rNow I can use those functions inside a loop in my Integration Pester Test\n$TestingTheChecks = @('errorlogscount','jobhistory')\rForeach ($Check in $TestingTheChecks) {\rContext \u0026quot;$Check Checks\u0026quot; {\rInvoke-DefaultCheck\rInvoke-ConfigCheck\rInvoke-ValueCheck\r}\r}\rWrite Some Integration Tests So for this new test I have added a value to the TestingTheChecks array then I can test my checks. The default check I can check like this\n# run the checks against these instances (SQL2014 agent wont start :-( ))\r$null = Set-DbcConfig -Name app.sqlinstance $containers.Where {$_ -ne 'localhost,15588'}\r# by default all tests should pass on default instance settings\r$jobhistorydefault = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rNow I need to change the configurations so that they do not match the defaults and run the checks again\n#Change the configuration to test that the checks fail\r$null = Set-DbcConfig -Name agent.history. maximumjobhistoryrows -value 1000\r$null = Set-DbcConfig -Name agent.history.maximumhistoryrows -value 10000\r$jobhistoryconfigchanged = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rNext we have to change the instance settings so that they match the dbachecks configuration and run the checks and test that they all pass.\nWe will (of course) use dbatools for this. First we need to find the command that we need\nFind-DbaCommand jobserver\rand then work out how to use it\nGet-Help Set-DbaAgentServer -Detailed\rThere is an example that does exactly what we want üôÇ So we can run this.\n$setDbaAgentServerSplat = @{\rMaximumJobHistoryRows = 1000\rMaximumHistoryRows = 10000\rSqlInstance = $containers.Where{$_ -ne 'localhost,15588'}\rSqlCredential = $cred\r}\rSet-DbaAgentServer @setDbaAgentServerSplat\r$jobhistoryvaluechanged = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rRun the Integration Tests And then we will check that all of the checks are passing and failing as expected\nInvoke-Pester .\\DockerTests.ps1\rIntegration Test For Error Log Counts There is another integration test there for the error logs count. This works in the same way. Here is the code\n#region error Log Count - PR 583\r# default test\r$errorlogscountdefault = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r# set a value and then it will fail\r$null = Set-DbcConfig -Name policy.errorlog.logcount -Value 10\r$errorlogscountconfigchanged = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r# set the value and then it will pass\r$null = Set-DbaErrorLogConfig -SqlInstance $containers -SqlCredential $cred -LogCount 10\r$errorlogscountvaluechanged = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r#endregion\rMerge the Changes So with all the tests passing I can merge the PR into the development branch and Azure DevOps will start a build. Ultimately, I would like to add the integration to the build as well following Andr√©‚Äòs blog post but for now I used the GitHub Pull Request extension to merge the pull request into development which started a build and then merged that into master which signed the code and deployed it to the PowerShell gallery as you can see here and the result is\nhttps://www.powershellgallery.com/packages/dbachecks/1.1.164\n","date":"2019-01-19T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-docker-to-run-integration-tests-for-dbachecks/","title":"Using Docker to run Integration Tests for dbachecks"},{"content":"Just for fun I decided to spend Christmas Eve getting Windows and Linux SQL containers running together.\nWARNING This is NOT a production ready solution, in fact I would not even recommend that you try it.\nI definitely wouldn‚Äôt recommend it on any machine with anything useful on it that you want to use again.\nWe will be using a re-compiled dockerd.exe created by someone else and you know the rules about downloading things from the internet don‚Äôt you? and trusting unknown unverified people?\nMaybe you can try this in an Azure VM or somewhere else safe.\nAnyway, with that in mind, lets go.\nLinux Containers On Windows You can run Linux containers on Windows in Docker as follows. You need to be running the latest Docker for Windows.\nRight click on the whale in the task bar and select Settings\nNotice that I am running Windows Containers as there is a switch to Linux containers option. If you see Switch to Windows containers then click that first.\nClick on Daemon and then tick the experimental features tick box and press apply.\nDocker will restart and you can now run Linux containers alongside windows containers.\nSo you you can pull the Ubuntu container with\ndocker pull ubuntu:18.04\rand then you can run it with\ndocker run -it --name ubuntu ubuntu:18.04\rThere you go one Linux container running üôÇ\nA good resource for learning bash for SQL Server DBAs is Kellyn Pot‚ÄôVin-Gorman b | t series on Simple Talk\nType Exit to get out of the container and to remove it\ndocker rm ubuntu\rRunning SQL Linux Containers On Windows\nSo can we run SQL Containers ?\nWell, we can pull the image successfully.\ndocker pull mcr.microsoft.com/mssql/server:2019-CTP2.2-ubuntu\rIf you try that without the experimental features enabled you will get this error.\nimage operating system ‚Äúlinux‚Äù cannot be used on this platform\nSo you would think that what you can do is to use the code from Andrew ‚Äòdbafromthecold‚Äô Pruski‚Äôs b | t excellent container series\ndocker run -d -p 15789:1433 --env ACCEPT_EULA=Y --env SA_PASSWORD=Testing1122 --name testcontainer mcr.microsoft.com/mssql/server:2019-CTP2.2-ubuntu\rWhen you do, the command will finish successfully but the container won‚Äôt be started (as can been seen by the red dot in the docker explorer).\nIf you look at the logs for the container. (I am lazy, I right click on the container and choose show logs in VS Code üôÇ ) you will see\nsqlservr: This program requires a machine with at least 2000 megabytes of memory.\n/opt/mssql/bin/sqlservr: This program requires a machine with at least 2000 megabytes of memory.\nNow, if you are running Linux containers, this is an easy fix. All you have to do is to right click on the whale in the taskbar, choose Settings, Advanced and move the slider for the Memory and click apply.\nBut in Windows containers that option is not available.\nIf you go a-googling you will find that Shawn Melton created an issue for this many months ago, which gets referenced by this issue for the guest compute service, which references this PR in moby. But as this hasn‚Äôt been merged into master yet it is not available. I got bored of waiting for this and decided to look a bit deeper today.\nGet It Working Just For Fun So, you read the warning at the top?\nNow let‚Äôs get it working. I take zero credit here. All of the work was done by Brian Weeteling b | G in this post\nSo you can follow Brians examples and check out the source code and compile it as he says or you can download the exe that he has made available (remember the warning?)\nStop Docker for Windows, and with the file downloaded and unzipped, open an admin PowerShell and navigate to the directory the dockerd.exe file is and run\n.\\dockerd.exe\rYou will get an output like this and it will keep going for a while.\nLeave this window open whilst you are using Docker like this. Once you see\nThen open a new PowerShell window or VS Code. You will need to run it as admin. I ran\ndocker ps-a\rto see if it was up and available.\nI also had to create a bootx64.efi file at C:\\Program Files\\Linux Containers which I did by copying and renaming the kernel file in that folder.\nNow I can use a docker-compose file to create 5 containers. Four will be Windows containers from Andrews Docker hub repositories or Microsoft‚Äôs Docker Hub for SQL 2012, SQL 2014, SQL 2016, and SQL 2017 and one will be the latest Ubuntu SQL 2019 CTP 2.2 image. Note that you have to use version 2.4 of docker compose as the platform tag is not available yet in any later version, although it is coming to 3.7 soon.\nversion: '2.4'\rservices:\rsql2019:\rimage: mcr.microsoft.com/mssql/server:2019-CTP2.2-ubuntu\rplatform: linux\rports: - \u0026quot;15585:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2012:\rimage: dbafromthecold/sqlserver2012dev:sp4\rplatform: windows\rports: - \u0026quot;15589:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2014:\rimage: dbafromthecold/sqlserver2014dev:sp2\rplatform: windows\rports: - \u0026quot;15588:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2016:\rimage: dbafromthecold/sqlserver2016dev:sp2\rplatform: windows\rports: - \u0026quot;15587:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2017:\rimage: microsoft/mssql-server-windows-developer:2017-latest\rplatform: windows\rports: - \u0026quot;15586:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rSave this code as docker-compose.yml and navigate to the directory in an admin PowerShell or VS Code and run\ndocker-compose up -d\rand now I have Windows and Linux SQL containers running together. This means that I can test some code against all versions of SQL from 2012 to 2019 easily in containers üôÇ\nSo that is just a bit of fun.\nTo return to the normal Docker, simply CTRL and C the admin PowerShell you ran .\\dockerd.exe in and you will see the logs showing it shutting down.\nYou will then be able to start Docker For Windows as usual.\nI look forward to the time, hopefully early next year when all of the relevant PR‚Äôs have been merged and this is available in Docker for Windows.\nHappy Automating üôÇ\n","date":"2018-12-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/running-windows-and-linux-sql-containers-together/","title":"Running Windows and Linux SQL Containers together"},{"content":"There was a question in the #dbatools slack channel¬†Getting dbatools dbatools enables you to administer SQL Server with PowerShell. To get it simply open PowerShell run\nInstall-Module dbatools\nYou can find more details on the web-site\nFinding the Command To find a command you can use the dbatools command Find-DbaCommand For commands for service run\nFind-DbaCommand Service\nThere are a whole bundle returned\nThis is how you can find any dbatools command. There is also a -Tag parameter on Find-DbaCommand.\nFind-DbaCommand -Tag Service\nThis returns\nHow to use any PowerShell command Always always start with Get-Help\nGet-Help Get-DbaService -Detailed\nThis will show you all the information about the command including examples üôÇ\nAll of these commands below require that the account running the PowerShell is a Local Admin on the host.\nOne Host Many Hosts Now I have used just one host for all of the examples on this page. Do not be fooled, you can always use an array of hosts wherever I have $ComputerName you can set it to as many hosts as you like\n$ComputerName = 'SQL0','SQL1'\nYou can even get those names form a database, Excel sheet, CMS.\nGetting the Services So to get the services on a machine run\n1 2 $ComputerName = \u0026#39;Name of Computer\u0026#39; Get-DbaService -ComputerName $ComputerName You can output into a table format.\nGet-DbaService -ComputerName $ComputerName | Format-Table\nI will use the alias ft for this in some of the examples, that is fine for the command line but use the full command name in any code that you write that other people use\nYou have an object returned so you can output to anything if you want ‚Äì CSV, JSON, text file, email, azure storage, database, the world is your oyster.\nGetting the Services for one instance The Get-DbaService command has a number of parameters. There is an InstanceName parameter enabling you to get only the services for one instance. If we just want the default instance services\nGet-DbaService -ComputerName $ComputerName -InstanceName MSSQLSERVER| Format-Table\nJust the MIRROR instance services\nGet-DbaService -ComputerName $ComputerName -InstanceName MIRROR| Format-Table\nGetting just the Engine or Agent services You can also use the -Type parameter to get only services of a particular type. You can get one of the following: ‚ÄúAgent‚Äù,‚ÄùBrowser‚Äù,‚ÄùEngine‚Äù,‚ÄùFullText‚Äù,‚ÄùSSAS‚Äù,‚ÄùSSIS‚Äù,‚ÄùSSRS‚Äù, ‚ÄúPolyBase‚Äù\nSo to get only the Agent Services\nGet-DbaService -ComputerName $ComputerName -Type Agent\nYou can combine the InstanceName and the Type parameters to get say only the default instance engine service\nGet-DbaService -ComputerName $ComputerName -InstanceName MSSQLSERVER -Type Engine\nStarting and stopping and restarting services You can use Start-DbaService and Stop-DbaService to start and stop the services. They each have ComputerName, InstanceName and Type parameters like Get-DbaService.\nSo if after running\nGet-DbaService -ComputerName $ComputerName | Format-Table\nyou find that all services are stopped\nStart All the Services You can run\nStart-DbaService -ComputerName $ComputerName | Format-Table\nand start them all\nThe full text service was started with the engine service which is why it gave a warning. You can see this if you have all of the services stopped and just want to start the engine services with the type parameter.\n1 2 3 Get-DbaService -ComputerName $ComputerName | Format-Table Start-DbaService -ComputerName $ComputerName -Type Engine Get-DbaService -ComputerName $ComputerName | Format-Table If you just want to start the Agent services, you can use\nStart-DbaService -ComputerName $ComputerName -Type Agent\nYou can start just the services for one instance\nStart-DbaService -ComputerName $ComputerName -InstanceName MIRROR\nStopping the services Stopping the services works in the same way. Lets stop the MIRROR instance services we have just started. This will stop the services for an instance\nStop-DbaService -ComputerName $ComputerName -InstanceName MIRROR\nWe can stop them by type as well, although this will show an extra requirement. If we start our MIRROR instance services again and then try to stop just the engine type.\n1 2 Start-DbaService -ComputerName $ComputerName -InstanceName MIRROR | ft Stop-DbaService -ComputerName $ComputerName -Type Engine You will get a warning due to the dependant services\nWARNING: [10:31:02][Update-ServiceStatus] (MSSQL$MIRROR on SQL0) The attempt to stop the service returned the following error: The service cannot be stopped because other services that are running are dependent on it. WARNING: [10:31:02][Update-ServiceStatus] (MSSQL$MIRROR on SQL0) Run the command with ‚Äò-Force‚Äô switch to force the restart of a dependent SQL Agent\nSo all you have to do is use the force Luke (or whatever your name is!)\nStop-DbaService -ComputerName $ComputerName -Type Engine -Force\nYou can also stop the services for an entire host, again you will need the Force parameter.\n1 2 Start-DbaService -ComputerName $ComputerName |ft Stop-DbaService -ComputerName $ComputerName -Force | ft Restarting Services It will come as no surprise by now to learn that Restart-DbaService follows the same pattern. It also has ComputerName, InstanceName and Type parameters like Get-DbaService, Start-DbaService and Stop-DbaService (Consistency is great, It‚Äôs one of the things that is being worked on towards 1.0 as you can see in the¬†Bill of Health)\nAgain you will need the -Force for dependant services, you can restart all of the services on a host with\nRestart-DbaService -ComputerName $ComputerName -Force\nor just the services for an instance\nRestart-DbaService -ComputerName $ComputerName -InstanceName MIRROR -Force\nor just the Agent Services\nRestart-DbaService -ComputerName $ComputerName -Type Agent\nDoing a bit of coding Now none of that answers @g-kannan‚Äôs question. Restarting only services with a certain service account.\nWith PowerShell you can pipe commands together so that the results of the first command are piped into the second. So we can get all of the engine services on a host for an instance with Get-DbaService and start them with Start-DbaService like this\nGet-DbaService -ComputerName $ComputerName -Type Engine | Start-DbaService\nor get all of the engine services for an instance on a host and stop them\nGet-DbaService -ComputerName $ComputerName -Type Engine¬†-InstanceName Mirror| Stop-DbaService\nor maybe you want to get all of the service that have stopped\n(Get-DbaService -ComputerName $ComputerName -Type Engine).Where{$_.State -eq 'Stopped'}\nYou can do the same thing with syntax that may make more sense to you if you are used to T-SQL as follows\n(Get-DbaService -ComputerName $ComputerName -Type Engine) | Where State -eq 'Stopped'\nand then start only those services you could do\n(Get-DbaService -ComputerName $ComputerName -Type Engine) | Where State -eq 'Stopped' | Start-DbaService\n(note ‚Äì you would just use Start-DbaService in this case as it wont start services that are already started!)\n1 2 3 4 5 6 \\# Stop just one of the engine services Stop-DbaService -ComputerName $ComputerName -InstanceName MIRROR -Type Engine \\# Get the engine services Get-DbaService -ComputerName $ComputerName -Type Engine \\# This will only start the one engine service that is stopped Start-DbaService -ComputerName $ComputerName -Type Engine Come On Rob! Answer the question! So now that you know a lot more about these commands, you can restart only the services using a particular service account by using Get-DbaService to get the services\nGet-DbaService -ComputerName $ComputerName -Type Engine | Where StartName -eq 'thebeard\\\\sqlsvc'\nand then once you know that you have the right ‚Äòquery‚Äô you can pipe that to Restart-DbaService (Like making sure your SELECT query returns the correct rows for your WHERE clause before running the DELETE or UPDATE)\nGet-DbaService -ComputerName $ComputerName -Type Engine | Where StartName -eq 'thebeard\\\\sqlsvc' | Restart-DbaService\nHappy Automating !\n","date":"2018-12-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/getting-sql-services-starting-stopping-and-restarting-them-with-dbatools/","title":"Getting SQL Services, Starting, Stopping and Restarting them with dbatools"},{"content":"In my last post I wrote about a new function for gathering the data and running the FailoverDetection utility by the Tiger Team to analyse availability group failovers. I have updated it following some comments and using it for a day.\nDon‚Äôt forget the named instances Rob! Michael Karpenko wrote a comment pointing out that I had not supported named instances, which was correct as it had not been written for that. Thank you Michael üôÇ I have updated the code to deal with¬†named instances.\nConfusing results I also realised as we started testing the code that if you had run the code once and then ran it again against a different availability group the tool does not clear out the data folder that it uses so you can get confusing results.\nIn the image below I had looked at the default instance and then a MIRROR named instance. As you can see the results json on the left shows the default instance SQLClusterAG while the one on the right shows both the SQLClusterAG and the MirrrAG instance results.\nThis is not so useful if you don‚Äôt notice this at first with the expanded json!! Now you may in this situation want to see the combined results from all of the availability groups on one cluster. You could gather all of the data from each instance and then add it to the data folder easily enough.\nBy cleaning out the data folder before running the utility the results are as expected.\nArchive the data for historical analysis One of the production DBAs pointed out that having gathered the information, it would be useful to hold it for better analysis of repeated issues. I have added an archiving step so that when the tools runs, if there is already data in the data gathering folder, it will copy that to an archive folder and name it with the date and time that the cluster log was created as this is a good estimation of when the analysis was performed. If an archive folder location is not provided it will create an archive folder in the data folder. This is not an ideal solution though, as the utility will copy all of the files and folders from there to its own location so it is better to define an archive folder in the parameters.\nGet-Eventlog is sloooooooooooow I was running the tools and noticed it sat running the system event log task for a long long time. I ran some tests using a variation of the dbatools prompt.\nThis will show in the prompt how long it took to run the previous statement .\nIn the image above (which you can click to get a larger version as with all images on this blog) you can see that it took 18ms to set the date variable, FOUR MINUTES and FORTY THREE seconds to get the system log in the last 2 days using Get-EventLog and 29.1 seconds using Get-WinEvent and a FilterHashtable.\nGetting the function This function requires PowerShell version 5 and the dbatools module.\nYou can get the function from my GitHub Functions Repository here (at the moment ‚Äì will be adding to dbatools see below)\nLoad the function by either running the code or if you have it saved as a file dot-sourcing it.\n. .\\Invoke-SqlFailOverDetection.ps1\nThere are two .‚Äôs with a space in between and then a \\ without a space. so Dot Space Dot Whack path to file.\nThe next thing you should do is what you should always do with a new PowerShell function, look at the help.\nGet-Help Invoke-SqlFailOverDetection -Detailed\nYou will find plenty of examples to get you going and explanations of all of the parameters and more info on my previous post.\nHappy Automating!\n","date":"2018-12-01T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sql-server-availability-group-failoverdetection-utility-powershell-function-improvements-named-instances-archiving-data-speed/","title":"SQL Server Availability Group FailoverDetection Utility PowerShell Function Improvements ‚Äì Named Instances, Archiving Data, Speed"},{"content":"30/11/2018 ‚Äì Function has been updated to deal with named instances.\nLast week the Tiger Team released their Availability Group Failover Detection Utility which will provide root cause analysis on Cluster Logs, SQL Error Logs, and the Availability groups extended events logs. There is a blog post here and the tool can be downloaded from the Tiger Team GitHub Repository\nA Bit of Faffing* It states on the readme for the Tiger Team GitHub Repository.\nRepository for Tiger team for ‚Äúas-is‚Äù solutions and tools/scripts that the team publishes.\nThe important words are ‚Äúas-is‚Äù sometimes these tools need a bit of faffing some looking after!\nThere is a pre-requisite and sometimes a little ‚Äúfixing‚Äù that you need to do to get it to run correctly.\nFirst, install the ‚ÄúMicrosoft Visual C++ Redistributable for Visual Studio 2017‚Äù from here. On the download page, scroll down to the ‚ÄúOther Tools and Frameworks‚Äù section to download the redistributable (x64 version).\nThen when you run FailoverDetection.exe you may get strong name validation errors like.\nUnhandled Exception: System.IO.FileLoadException: Could not load file or assembly ‚ÄòMicrosoft.Sq1Server.XEvent.Linq, Version=15.0.0.0, Culture=neutral, PublicKeyToken=89845dcd808cc91‚Äô or one of it s dependencies. Strong name validation failed. (Exception from HRESULT; 0x8013141A) ‚Äì ‚Äì ‚Äì \u0026gt;.Security.SecurityException: Strong name validation failed. (Exception from HRESULT: 0x8e13141A)\n‚ÄîEnd of inner exception stack trace¬†‚Äî\nat FailoverDetector. XeventParser.LoadXevent(String xelFi1eName, String serverName)\nThen you will need to run the sn.exe tool which is in the zip file. Use this syntax.\n.\\sn.exe -Vr PATHTODLLFile\nI had to do it for two DLLs.\nNOTE ‚Äì If you get an error like this when running sn.exe (or any executable) from PowerShell it means that you have missed the .\\ (dot whack) in front of the executable name.\n* Faffing ‚Äì Doing something that is a bit awkward¬†See Link¬†.\nLogs required for the Tool To run the Failover Detection Utility you need to gather the following information from each replica and place it in the specified data folder.\nSQL error logs Always On Availability Groups Extended Event Logs System Health Extended Event Logs System log Windows cluster log Once you have gathered all of that data then you need to alter the configuration file for the executable.\n1 2 3 4 5 6 7 8 9 { \u0026#34;Data Source Path\u0026#34;: \u0026#34;Path to Data File\u0026#34;, \u0026#34;Health Level\u0026#34;: 3, \u0026#34;Instances\u0026#34;: \\[ \u0026#34;Replica1\u0026#34;, \u0026#34;Replica2\u0026#34;, \u0026#34;Replica3\u0026#34; \\] } Running The Tool Once you have done that you can then run the Failover Detection Utility. You can double click the exe,\nor you can run it from the command line.\nIn both cases it won‚Äôt exit so when you see the Saving Results to JSON file, you can press enter (sometimes twice!).\nThe results can be seen in the JSON file which will be stored in a Results directory in the directory that the the FailoverDetection.exe exists.\nYou can also use some switches with the FailoverDetection utility.\n**‚ÄìAnalyze ‚Äì¬†**When ‚Äú‚ÄìAnalyze‚Äù is specified as a parameter, the utility will load configuration file without copying log data. It assumes the log files have already been copied over. It does everything as default mode except copying log data. This option is useful if you already have the data in the local tool execution subdirectories and want to rerun the analysis.\n‚Äì-Show¬†-The utility after analyzing log data will display the results in the command console. Additionally, the results will be persisted to a JSON file in the results folder.\nThey look like this\nAgain, you need to press enter for the details to come through. The results are still saved to the Results folder as json as well so you won‚Äôt lose them.\nWhen You Are Doing Something More Than Once ‚Ä¶. Automate it üôÇ\nWhen I saw the data that needed to be gathered for this tool, I quickly turned to PowerShell to enable me to easily gather the information. That has turned into a function which will\nDownload and extract the zip file from the Tiger Team GitHub repository Identify all of the replicas for an Availability Group and dynamically create the configuration JSON file Gather all of the required log files and place them in a specified data folder Run the FailoverDetection.exe with any of the switches Includes -Verbose, -Confirm, -Whatif switches so that you can easily see what is happening, be prompted to confirm before actions or see what would happen if you ran the function You still need to press enter at the end though üôÅ and you will still need to install the ‚ÄúMicrosoft Visual C++ Redistributable for Visual Studio 2017‚Äù and runt he strong names tool if needed This function requires PowerShell version 5, the failovercluster module and and the dbatools module.\nYou can get the function from my GitHub Functions Repository here (at the moment ‚Äì will be adding to dbatools see below)\nLoad the function by either running the code or if you have it saved as a file dot-sourcing it.\n. .\\Invoke-SqlFailOverDetection.ps1\nThere are two .‚Äôs with a space in between and then a \\ without a space. so Dot Space Dot Whack path to file.\nThe next thing you should do is what you should always do with a new PowerShell function, look at the help.\nGet-Help Invoke-SqlFailOverDetection -Detailed\nYou will find plenty of examples to get you going and explanations of all of the parameters.\nLet‚Äôs see it in action.\nFirst lets run with a -WhatIf switch which will show us what will happen without performing any state changing actions.\n1 2 3 4 5 6 7 8 9 10 11 12 $InstallationFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Install\u0026#39; $DownloadFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Download\u0026#39; $DataFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Data\u0026#39; $SQLInstance = \u0026#39;SQL0\u0026#39; $invokeSqlFailOverDetectionSplat = @{ DownloadFolder = $DownloadFolder SQLInstance = $SQLInstance DataFolder = $DataFolder InstallationFolder = $InstallationFolder } Invoke-SqlFailOverDetection @invokeSqlFailOverDetectionSplat -WhatIf So you can see that if we run it without the -WhatIf switch it will\nCreate some directories Download the zip file from the repo Extract the zip file Copy the required logs from each of the replicas to the data folder Create the JSON configuration file Run the executable NOTE : ‚Äì I have limited the gathering of the system event log to the last 2 days to limit the amount of time spent dealing with a large system log. I gather all of the SQL Error logs in the Error log path as that works for the first scenario I wrote this for, your mileage may vary.\nSo if we want to run the command we can remove the -WhatIf switch.\n1 2 3 4 5 6 7 8 9 10 11 12 $InstallationFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Install\u0026#39; $DownloadFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Download\u0026#39; $DataFolder = \u0026#39;C:\\temp\\failoverdetection\\new\\Data\u0026#39; $SQLInstance = \u0026#39;SQL0\u0026#39; $invokeSqlFailOverDetectionSplat = @{ DownloadFolder = $DownloadFolder SQLInstance = $SQLInstance DataFolder = $DataFolder InstallationFolder = $InstallationFolder } Invoke-SqlFailOverDetection @invokeSqlFailOverDetectionSplat It can take a little while to run depending on the number of replicas, size of logs etc but once it has started running you can do other things.\nIt will require being run as an account with permissions to all of the folders specified and Windows and SQL permissions on all of the replicas in the Availability Group.\nAs you can see below it has gathered all of the results and placed them in the data folder.\nThe results can be found in the results folder.\nIf I have already run the tool, I can use the Analyze switch to save gathering the data again. I also use the AlreadyDownloaded switch as I do not need to download the zip file again.\n1 2 3 4 5 6 7 8 9 $invokeSqlFailOverDetectionSplat = @{ DownloadFolder = $DownloadFolder SQLInstance = $SQLInstance DataFolder = $DataFolder InstallationFolder = $InstallationFolder AlreadyDownloaded = $true Analyze = $true } Invoke-SqlFailOverDetection @invokeSqlFailOverDetectionSplat and the results are again saved in the results folder.\nI can show the results on the screen as well as saving them as JSON with the Show parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $InstallationFolder = \u0026#39;C:\\temp\\failoverdetection\\Install\u0026#39; $DownloadFolder = \u0026#39;C:\\temp\\failoverdetection\\Download\u0026#39; $DataFolder = \u0026#39;C:\\temp\\failoverdetection\\Data\u0026#39; $SQLInstance = \u0026#39;SQL0\u0026#39; $invokeSqlFailOverDetectionSplat = @{ DownloadFolder = $DownloadFolder SQLInstance = $SQLInstance DataFolder = $DataFolder InstallationFolder = $InstallationFolder AlreadyDownloaded = $true Analyze = $true Show = $true } Invoke-SqlFailOverDetection @invokeSqlFailOverDetectionSplat You will then need to press enter to get the next lot of results.\nWhy Not Add This To dbatools? I haven‚Äôt added this to dbatools (yet) because I wrote it in this way for a particular need and dbatools requires support for PowerShell V3 . I have, however created an issue¬†added to this issue in the dbatools GitHub Repository (as this is how you to start the process of adding things to dbatools) so hopefully we can get it in there soon as well ‚Äì in which case I will come back and update this post.\nHappy Automating!\n","date":"2018-11-28T00:00:00Z","permalink":"https://blog.robsewell.com/blog/gathering-all-the-logs-and-running-the-availability-group-failover-detection-utility-with-powershell/","title":"Gathering all the Logs and Running the Availability Group Failover Detection Utility with PowerShell"},{"content":"Following an upgrade to SQL Server the backup share had a number of backups, some from the old version and some from the newer version. I was asked if I had a script to be able to get the SQL Version from the backup file from all of the files in the backup share.\nWith dbatools¬†this was easy to accomplish with Read-DbaBackuoHeader\n1 2 3 4 5 6 7 8 $backupshare = \u0026#34;$share\\\\keep\u0026#34; $Instance = \u0026#34;SQL0\\\\Mirror\u0026#34; $information = foreach ($BackupFile in (Get-ChildItem $backupshare)) { $FileName = @{Name = \u0026#39;FileName\u0026#39;; Expression = {$BackupFile.Name}} Read-DbaBackupHeader -SqlInstance $Instance -Path $BackupFile.FullName | Select-Object $FileName, DatabaseName , CompatibilityLevel, SqlVersion } $information | Format-Table You can get more information about the backup using Read-DbaBackupHeader and as it is PowerShell it is easy to put this information into any format that you wish, maybe into a database with Write-DbaDataTable\nSo I looked at https://t.co/MUw7Dw7CRv\nI saw the words \u0026quot; Support for PS Core on Windows üéâ\u0026quot;\nI updated the module to 0.9.522 and ran a command and\nBOOOOOOOOOOOOOOOOOOM\nGood work fine @psdbatools contirbutors and @cl pic.twitter.com/fzpSIju1Gx\n‚Äî Rob Sewell (@sqldbawithbeard) November 23, 2018\nSupport for PowerShell Core in dbatools is coming along very nicely. Following some hard work by the dbatools team and some PowerShell Community members like Mathias Jessen¬†it is now possible to run a large number of dbatools commands in PowerShell Core running on Windows. There is still a little bit of work to do to get it working on Linux and Mac but I hear the team are working hard on that.\nSo the code example you see above was running on Windows 10 using PowerShell 6.1.1 the current latest stable release. This is excellent news and congratulations to all those working hard to make this work\nIf you want to try PowerShell Core, you can follow the instructions\nHere for Windows Here for Linux Or here for MacOs Happy Automating!\n","date":"2018-11-26T00:00:00Z","permalink":"https://blog.robsewell.com/blog/getting-the-sql-version-from-a-backup-using-dbatools-.-on-powershell-core/","title":"Getting the SQL Version from a backup using dbatools ‚Ä¶‚Ä¶‚Ä¶. on PowerShell Core"},{"content":"Next week is the week when I used to dread looking at Twitter and especially the #PASSsummit¬†hashtag, watching all of those folk having a great time in great sessions and doing all of the networking. Last year I was lucky enough to attend for the first time and take part in Chrissy LeMaire and CKs pre-con, this year I decided to add a US SQL Saturday to the list.\nI shall be attending SQL Saturday Oregon in Portland¬†and presenting about dbatools. I am really lucky, my amazing Dad, now that he is retired, happily plays Dad‚Äôs taxi (still!) and frequently drives me to the airport when I go away on trips. This is the first time he has really gone Wow! When I asked why, it is because Portland is where Grimm is filmed and he has watched the whole series and loved it!\nI am looking forward to ticking off another thing on my list of things to do and presenting at a US SQL Saturday to add to 12 SQL Saturday cities in Europe and multiple other events around the world. If you are there come up and say hi.\nI shall also be room monitoring for the¬†PowerShell for the DBA session by¬†Amy Herold t | b¬†There are still volunteer slots available to help, sign up here if you are going. It is a lot of fun and an excellent way to give something back. SQL Saturdays take a lot of work to organise and organisers are always willing for some help. You will meet new people and have a lot of fun as well.\nTo get to Seattle I am going on the SQL Train. A whole bunch of data platform folk travelling on a train together up to Seattle for PASS Summit. It looks like it will be a lot of fun üôÇ\nOnce in Seattle it is time for the week where others will not want to look at my twitter feed üôÇ A whole week at PASS Summit watching the sessions and networking (there may be beer involved) with our peers.\nMy one piece of advice is please don‚Äôt hide away in your hotel room for all of the time that sessions are not on. I know that dealing with a large amount of people can be tough and you might need some me time ( I will ) but there are a lot of activities both loud and quieter where you will have the opportunity to meet up and make new friends and contacts which may really pay back further down the line and you will have fun too.\nOn the Tuesday I am doing a pre-conference session¬†Professional and Proficient PowerShell: From Writing Scripts to Developing Solutions. A whole day where I will be showing how to write PowerShell modules and all of the tips and tricks that I have learnt over the years.\nWednesday sees me embracing my inner Andr√© Kamman¬†‚Äì unfortunately he is unable to make PASS Summit this year, so I will be delivering the session¬†dbatools Powershell Library ‚Äì The Complete Introduction in his place in room 2AB. I shall try to do it in as cool and relaxed a way as he does (It probably wont work I will get too excited üôÇ )\nOn Thursday I will be talking about dbachecks in room 6C which will also be streamed on PASSTv.\nIn between all of that, I shall be attending some networking events, visiting sessions, hanging out with people new and old and walking down the corridors, so if you see me, stop me and say hi, I‚Äôd love to meet you üôÇ\n(note ‚Äì just before my sessions I may be too nervous to properly have a conversation)\nHopefully, I will meet you there and for the twitter folk stuck back in Europe I empathise üôÇ\n","date":"2018-10-31T00:00:00Z","permalink":"https://blog.robsewell.com/blog/pass-summit-sqltrain-and-my-first-us-sql-saturday/","title":"PASS Summit, SQLTrain and My First US SQL Saturday"},{"content":"Next week is the week when I used to dread looking at Twitter and especially the #PASSsummit¬†hashtag, watching all of those folk having a great time in great sessions and doing all of the networking. Last year I was lucky enough to attend for the first time and take part in Chrissy LeMaire and CKs pre-con, this year I decided to add a US SQL Saturday to the list.\nI shall be attending SQL Saturday Oregon in Portland¬†and presenting about dbatools. I am really lucky, my amazing Dad, now that he is retired, happily plays Dad‚Äôs taxi (still!) and frequently drives me to the airport when I go away on trips. This is the first time he has really gone Wow! When I asked why, it is because Portland is where Grimm is filmed and he has watched the whole series and loved it!\nI am looking forward to ticking off another thing on my list of things to do and presenting at a US SQL Saturday to add to 12 SQL Saturday cities in Europe and multiple other events around the world. If you are there come up and say hi.\nI shall also be room monitoring for the¬†PowerShell for the DBA session by¬†Amy Herold t | b¬†There are still volunteer slots available to help, sign up here if you are going. It is a lot of fun and an excellent way to give something back. SQL Saturdays take a lot of work to organise and organisers are always willing for some help. You will meet new people and have a lot of fun as well.\nTo get to Seattle I am going on the SQL Train. A whole bunch of data platform folk travelling on a train together up to Seattle for PASS Summit. It looks like it will be a lot of fun üôÇ\nOnce in Seattle it is time for the week where others will not want to look at my twitter feed üôÇ A whole week at PASS Summit watching the sessions and networking (there may be beer involved) with our peers.\nMy one piece of advice is please don‚Äôt hide away in your hotel room for all of the time that sessions are not on. I know that dealing with a large amount of people can be tough and you might need some me time ( I will ) but there are a lot of activities both loud and quieter where you will have the opportunity to meet up and make new friends and contacts which may really pay back further down the line and you will have fun too.\nOn the Tuesday I am doing a pre-conference session¬†Professional and Proficient PowerShell: From Writing Scripts to Developing Solutions. A whole day where I will be showing how to write PowerShell modules and all of the tips and tricks that I have learnt over the years.\nWednesday sees me embracing my inner Andr√© Kamman¬†‚Äì unfortunately he is unable to make PASS Summit this year, so I will be delivering the session¬†dbatools Powershell Library ‚Äì The Complete Introduction in his place in room 2AB. I shall try to do it in as cool and relaxed a way as he does (It probably wont work I will get too excited üôÇ )\nOn Thursday I will be talking about dbachecks in room 6C which will also be streamed on PASSTv.\nIn between all of that, I shall be attending some networking events, visiting sessions, hanging out with people new and old and walking down the corridors, so if you see me, stop me and say hi, I‚Äôd love to meet you üôÇ\n(note ‚Äì just before my sessions I may be too nervous to properly have a conversation)\nHopefully, I will meet you there and for the twitter folk stuck back in Europe I empathise üôÇ\n","date":"2018-10-31T00:00:00Z","permalink":"https://blog.robsewell.com/blog/pass-summit-sqltrain-and-my-first-us-sql-saturday/","title":"PASS Summit, SQLTrain and My First US SQL Saturday"},{"content":"It‚Äôs been a few weeks since i have blogged as I have been busy with a lot of other things. One of which is preparing for my SQL Pass Summit pre-con¬†which has lead to me improving the CI/CD for dbachecks¬†by adding auto-creation of online documentation, which you can find at¬†https://dbachecks.readthedocs.io¬†or by running Get-Help with the -Online switch for any dbachecks command.\nGet-Help Invoke-DbcCheck -Online\nI will blog about how dbachecks uses Azure DevOps¬†to do this another time\nPSPowerHour The PowerShell community members¬†Michael T Lombardi¬†and¬†Warren Frame¬†have created PSPowerHour.¬†PSPowerHour is ‚Äúlike a virtual User Group, with a lightning-demo format, and room for non-PowerShell-specific content. Eight community members will give a demo each PowerHour.‚Äù\nChrissy blogged about the first one on the dbatools blog\nYou can watch the videos on the Youtube channel and keep an eye out for more online PSPowerHours via twitter or the GitHub page.\nWhile watching the first group of sessions Andrew Wickham demonstrated using dbatools with trace flags and I thought that needs to be added to dbachecks so I created an issue. Anyone can do this to file improvements as well as bugs for members of the team to code.\nTrace Flags The previous release of dbachecks brought 2 new checks for traceflags. One for traceflags expected to be running and one for traceflags not expected to be running.\nYou will need to have installed dbachecks from the PowerShell Gallery to do this. This can be done using\nInstall-Module -Name dbachecks\nOnce dbachecks is installed you can find the checks using\nGet-DBcCheck\nyou can filter using the pattern parameter\nGet-DBcCheck -Pattern traceflag\nThis will show you\nthe UniqueTag which will enable you to run only that check if you wish AllTags which shows which tags will include that check Config will show you which configuration items can be set for this check The trace flag checks require the app.sqlinstance configuration which is the list of SQL instances that the checks will run against. You can also specify the instances as a parameter for Invoke-DbCheck as well.\nThe configuration for the expected traceflags is¬†policy.traceflags.expected By default it is set to null. You can see what configuration it has using\nGet-DBcConfig policy.traceflags.expected\nSo if you want to check that there are no trace flags running, then you can run\n$instance = \u0026lsquo;sql0\u0026rsquo; Set-DbcConfig -Name app.sqlinstance -Value $instance Invoke-DbcCheck -Check TraceFlagsExpected\nMaybe this instance is required to have trace flag 1117 enabled so that all files in a file group grow equally, you can set the trace flag you expect to be running using\nSet-DbcConfig -Name policy.traceflags.expected -Value 1117\nNow you when you run the check it fails\nInvoke-DbcCheck -Check TraceFlagsExpecte\nand gives you the error message\n[-] Expected Trace Flags 1117 exist on sql0 593ms\nExpected 1117 to be found in collection @(), because We expect that Trace Flag 1117 will be set on sql0, but it was not found.\nSo we have a failing test. We need to fix that. We can use dbatools\nEnable-DbaTraceFlag -SqlInstance $instance -TraceFlag 1117\nThis time when we run the check\nInvoke-DbcCheck -Check TraceFlagsExpected\nit passes\nIf you just need to see what trace flags are enabled you can use\nGet-DbaTraceFlag -SqlInstance $instance\nReset the configuration for the expected trace flag to an empty array and then set the configuration for traceflags we do not expect to be running to 1117\n1 2 Set-DbcConfig -Name policy.traceflags.expected -Value @() Set-DbcConfig -Name policy.traceflags.notexpected -Value 1117 and then run the trace flags not expected to be running check with\nInvoke-DbcCheck -Check TraceFlagsNotExpected\nIt will fail as 1117 is still running\nand give the message\n[-] Expected Trace Flags 1117 to not exist on sql0 321ms\nExpected 1117 to not be found in collection 1117, because We expect that Trace Flag 1117 will not be set on sql0, but it was found.\nSo to resolve this failing check we need to disable the trace flag and we can do that with dbatools using\nDisable-DbaTraceFlag -SqlInstance $instance -TraceFlag 1117\nand now when we run the check\nInvoke-DbcCheck -Check TraceFlagsNotExpected\nit passes\nThe checks also work with multiple traceflags so you can set multiple values for trace flags that are not expexted to be running\nSet-DbcConfig -Name policy.traceflags.notexpected -Value 1117, 1118\nand as we saw earlier, you can run both trace flag checks using\nInvoke-DbcCheck -Check TraceFlag\nYou can use this or any of the 95 available checks to validate that your SQL instances, singular or your whole estate are as you expect them to be.\n","date":"2018-09-29T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-trace-flags-with-dbachecks-online-docs-and-pspowerhour/","title":"Checking Trace Flags with dbachecks, online docs and PSPowerHour"},{"content":"Just a quick post to share some code that I used to solve a problem I had recently.\nI needed to automate the deployment of some Power Bi reports to a Power Bi Report Server PBRS using TFS. I had some modified historical validation dbachecks pbix files that I wanted to automate the deployment of and enable the client to be able to quickly and simply deploy the reports as needed.\nThe manual way It is always a good idea to understand how to do a task manually before automating it. To deploy to PBRS you need to use the Power Bi Desktop optimised for Power Bi Report Server. There are instructions here. Then it is easy to deploy to the PBRS by clicking file and save as and choosing Power Bi Report Server\nIf I then want to set the datasource to use a different set of credentials I navigate to the folder that holds the report in PBRS and click the hamburger menu and Manage\nand I can alter the User Name and Password or the type of connection by clicking on DataSources\nand change it to use the reporting user for example.\nAutomation But I dont want to have to do this each time and there will be multiple pbix files, so I wanted to automate the solution. The end result was a VSTS or TFS release process so that I could simply drop the pbix into a git repository, commit my changes, sync them and have the system deploy them automatically.\nAs with all good ideas, I started with a google and found this post by Bill Anton¬†which gave me a good start ( I could not get the connection string change to work in my test environment but this was not required so I didnt really examine why)\nI wrote a function that I can use via TFS or VSTS by embedding it in a PowerShell script. The function requires the¬†ReportingServicesTools module which you can get by\nInstall-Module -Name ReportingServicesTools\nThe function below is available via the PowerShell Gallery also and you can get it with\nInstall-Script -Name PublishPBIXFile\nThe source code is on Github\nand the code to call it looks like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $folderName = \u0026#39;TestFolder\u0026#39; $ReportServerURI = \u0026#39;http://localhost/Reports\u0026#39; $folderLocation = \u0026#39;/\u0026#39; $pbixfile = \u0026#39;C:\\Temp\\test.pbix\u0026#39; $description = \u0026#34;Descriptions\u0026#34; $publishPBIXFileSplat = @{ ReportServerURI = $ReportServerURI folderLocation = $folderLocation description = $description pbixfile = $pbixfile folderName = $folderName AuthenticationType = \u0026#39;Windows\u0026#39; ConnectionUserName = $UserName1 Secret = $Password1 Verbose = $true } Publish-PBIXFile @publishPBIXFileSplat which uploads the report to a folder which it will create if it does not exist. It will then upload pbix file, overwriting the existing one if it already exists\nand uses the username and password specified\nIf I wanted to use a Domain reporting user instead I can do\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $UserName1 = \u0026#39;TheBeard\\ReportingUser\u0026#39; $publishPBIXFileSplat = @{ ReportServerURI = $ReportServerURI folderLocation = $folderLocation description = $description pbixfile = $pbixfile folderName = $folderName AuthenticationType = \u0026#39;Windows\u0026#39; ConnectionUserName = $UserName1 Secret = $Password1 Verbose = $true } Publish-PBIXFile @publishPBIXFileSplat and it changes\nIf we want to use a SQL Authenticated user then\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $UserName1 = \u0026#39;TheReportingUserOfBeard\u0026#39; $publishPBIXFileSplat = @{ ReportServerURI = $ReportServerURI folderLocation = $folderLocation description = $description pbixfile = $pbixfile folderName = $folderName AuthenticationType = \u0026#39;SQL\u0026#39; # credential = $cred ConnectionUserName = $UserName1 Secret = $Password1 } Publish-PBIXFile @publishPBIXFileSplat Excellent, it all works form the command line. You can pass in a credential object as well as username and password. The reason I enabled username and password? So that I can use TFS or VSTS and store my password as a secret variable.\nNow I simply create a repository which has my pbix files and a PowerShell script and build a quick release process to deploy them whenever there is a change üôÇ\nThe deploy script looks like\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 [CmdletBinding()] Param ( $PBIXFolder, $ConnectionStringPassword ) $VerbosePreference = \u0026#39;continue\u0026#39; $ReportServerURI = \u0026#39;http://TheBeardsAmazingReports/Reports\u0026#39; Write-Output \u0026#34;Starting Deployment\u0026#34; function Publish-PBIXFile { [CmdletBinding(DefaultParameterSetName = \u0026#39;ByUserName\u0026#39;, SupportsShouldProcess)] Param( [Parameter(Mandatory = $true)] [string]$FolderName, [Parameter(Mandatory = $true)] [string]$ReportServerURI, [Parameter(Mandatory = $true)] [string]$FolderLocation, [Parameter(Mandatory = $true)] [string]$PBIXFile, [Parameter()] [string]$Description = \u0026#34;Description of Your report Should go here\u0026#34;, [Parameter()] [ValidateSet(\u0026#39;Windows\u0026#39;, \u0026#39;SQL\u0026#39;)] [string]$AuthenticationType, [Parameter(ParameterSetName = \u0026#39;ByUserName\u0026#39;)] [string]$ConnectionUserName, [Parameter(ParameterSetName = \u0026#39;ByUserName\u0026#39;)] [string]$Secret, [Parameter(Mandatory = $true, ParameterSetName = \u0026#39;ByCred\u0026#39;)] [pscredential]$Credential ) $FolderPath = $FolderLocation + $FolderName $PBIXName = $PBIXFile.Split(\u0026#39;\\\u0026#39;)[-1].Replace(\u0026#39;.pbix\u0026#39;, \u0026#39;\u0026#39;) try { Write-Verbose\u0026#34;Creating a session to the Report Server $ReportServerURI\u0026#34; # establish session w/ Report Server $session = New-RsRestSession-ReportPortalUri $ReportServerURI Write-Verbose\u0026#34;Created a session to the Report Server $ReportServerURI\u0026#34; } catch { Write-Warning\u0026#34;Failed to create a session to the report server $reportserveruri\u0026#34; Return } # create folder (optional) try { if ($PSCmdlet.ShouldProcess(\u0026#34;$ReportServerURI\u0026#34;, \u0026#34;Creating a folder called $FolderName at $FolderLocation\u0026#34;)) { $Null = New-RsRestFolder-WebSession $session-RsFolder $FolderLocation-FolderName $FolderName-ErrorAction Stop } } catch [System.Exception] { If ($_.Exception.InnerException.Message -eq \u0026#39;The remote server returned an error: (409) Conflict.\u0026#39;) { Write-Warning\u0026#34;The folder already exists - moving on\u0026#34; } } catch { Write-Warning\u0026#34;Failed to create a folder called $FolderName at $FolderLocation report server $ReportServerURI but not because it already exists\u0026#34; Return } try { if ($PSCmdlet.ShouldProcess(\u0026#34;$ReportServerURI\u0026#34;, \u0026#34;Uploading the pbix from $PBIXFile to the report server \u0026#34;)) { # upload copy of PBIX to new folder Write-RsRestCatalogItem-WebSession $session-Path $PBIXFile-RsFolder $folderPath-Description $Description-Overwrite } } catch { Write-Warning\u0026#34;Failed to upload the file $PBIXFile to report server $ReportServerURI\u0026#34; Return } try { Write-Verbose\u0026#34;Getting the datasources from the pbix file for updating\u0026#34; # get data source object $datasources = Get-RsRestItemDataSource-WebSession $session-RsItem \u0026#34;$FolderPath/$PBIXName\u0026#34; Write-Verbose\u0026#34;Got the datasources for updating\u0026#34; } catch { Write-Warning\u0026#34;Failed to get the datasources\u0026#34; Return } try { Write-Verbose\u0026#34;Updating Datasource\u0026#34; foreach ($dataSourcein$datasources) { if ($AuthenticationType -eq \u0026#39;SQL\u0026#39;) { $dataSource.DataModelDataSource.AuthType = \u0026#39;UsernamePassword\u0026#39; } else { $dataSource.DataModelDataSource.AuthType = \u0026#39;Windows\u0026#39; } if ($Credential -or $UserName) { if ($Credential) { $UserName = $Credential.UserName $Password = $Credential.GetNetworkCredential().Password } else { $UserName = $ConnectionUserName $Password = $Secret } $dataSource.CredentialRetrieval = \u0026#39;Store\u0026#39; $dataSource.DataModelDataSource.Username = $UserName $dataSource.DataModelDataSource.Secret = $Password } if ($PSCmdlet.ShouldProcess(\u0026#34;$ReportServerURI\u0026#34;, \u0026#34;Updating the data source for the report $PBIXName\u0026#34;)) { # update data source object on server Set-RsRestItemDataSource-WebSession $session-RsItem \u0026#34;$folderPath/$PBIXName\u0026#34;-RsItemType PowerBIReport -DataSources $datasource } } } catch { Write-Warning\u0026#34;Failed to set the datasource\u0026#34; Return } Write-Verbose\u0026#34;Completed Successfully\u0026#34; } foreach ($File in (Get-ChildItem $PBIXFolder\\*.pbix)) { Write-Output\u0026#34;Processing $($File.FullName)\u0026#34; ## to enable further filtering later if ($File.FullName -like \u0026#39;*\u0026#39;) { $folderName = \u0026#39;ThePlaceForReports\u0026#39; $folderLocation = \u0026#39;/\u0026#39; $UserName = \u0026#39;TheBeard\\ReportingUser\u0026#39; $Password = $ConnectionStringPassword $pbixfile = $File.FullName } if ($File.FullName -like \u0026#39;\\*dbachecks\\*\u0026#39;) { $description = \u0026#34;This is the morning daily checks file that....... more info\u0026#34; } if ($File.FullName -like \u0026#39;\\*TheOtherReport\\*\u0026#39;) { $description = \u0026#34;This is hte other report, it reports others\u0026#34; } $publishPBIXFileSplat = @{ ReportServerURI = $ReportServerURI folderLocation = $folderLocation description = $description AuthenticationType = \u0026#39;Windows\u0026#39; pbixfile = $pbixfile folderName = $folderName ConnectionUserName = $UserName Secret = $Password Verbose = $true } $Results = Publish-PBIXFile@publishPBIXFileSplat Write-Output$Results } Although the function does not need to be embedded in the script and can be deployed in a module, I have included it in here to make it easier for people to use quickly. I\nStore the password for the user as a variable in TFS or VSTS\nThen create a PowerShell step in VSTS or TFS and call the script with the parameters as shown below and PowerBi files auto deploy to Power Bi Report Server\nand I have my process complete üôÇ\nHappy Automating üôÇ\n","date":"2018-08-21T00:00:00Z","permalink":"https://blog.robsewell.com/blog/deploying-to-a-power-bi-report-server-with-powershell/","title":"Deploying To a Power Bi Report Server with PowerShell"},{"content":"In dbachecks¬†we enable people to see what checks are available by running Get-DbcCheck. This gives a number of properties including the ‚Äòtype‚Äô of check. This refers to the configuration item or parameter that is required to have a value for this check to run.\nFor example ‚Äì Any check to do with SQL Agent is of type Sqlinstance because it requires an instance to be specified but a check for SPN is of type ComputerName because it requires a computer name to run.\nAutomation for the win Because I believe in automation I do not want to have to hard code these values anywhere but create them when the module is imported so we use a json file to feed Get-DbcCheck and populate the Json file when we import the module. This is done using the method that I described here and means that whenever a new check is added it is automatically available in Get-DbcCheck without any extra work.\nWe use code like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Parse the file with AST $CheckFileAST = [Management.Automation.Language.Parser]::ParseInput($check, [ref]$null, [ref]$null) ## Old code we can use the describes $Describes = $CheckFileAST.FindAll([Func[Management.Automation.Language.Ast, bool]] { param ($ast) $ast.CommandElements -and $ast.CommandElements[0].Value -eq \u0026#39;describe\u0026#39; }, $true) @($describes).ForEach{ $groups += $filename $Describe = $_.CommandElements.Where{$PSItem.StaticType.name -eq \u0026#39;string\u0026#39;}[1] $title = $Describe.Value $Tags = $PSItem.CommandElements.Where{$PSItem.StaticType.name -eq \u0026#39;Object[]\u0026#39; -and $psitem.Value -eq $null}.Extent.Text.ToString().Replace(\u0026#39;, $filename\u0026#39;, \u0026#39;\u0026#39;) # CHoose the type if ($Describe.Parent -match \u0026#34;Get-Instance\u0026#34;) { $type = \u0026#34;Sqlinstance\u0026#34; } elseif ($Describe.Parent -match \u0026#34;Get-ComputerName\u0026#34; -or $Describe.Parent -match \u0026#34;AllServerInfo\u0026#34;) { $type = \u0026#34;ComputerName\u0026#34; } elseif ($Describe.Parent -match \u0026#34;Get-ClusterObject\u0026#34;) { $Type = \u0026#34;ClusteNode\u0026#34; } First we parse the code with the AST and store that in the¬†CheckFileAST variable, then we use the FindAll method to find any command elements that match ‚ÄúDescribe‚Äù which conveniently gets our describes and then we can simply match the Parent object which holds some code to each function that we use to get our values to be passed to the tests¬†Get-ComputerName,¬†Get-Instance,¬†Get-ClusterObject and set the type appropriately.\nwhich when run against a check like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Describe \u0026#34;Backup Path Access\u0026#34; -Tags BackupPathAccess, Storage, DISA, $filename { @(Get-Instance).ForEach{ if ($NotContactable -contains $psitem) { Context \u0026#34;Testing Backup Path Access on $psitem\u0026#34; { It \u0026#34;Can\u0026#39;t Connect to $Psitem\u0026#34; { $false| Should -BeTrue -Because \u0026#34;The instance should be available to be connected to!\u0026#34; } } } else { Context \u0026#34;Testing Backup Path Access on $psitem\u0026#34; { $backuppath = Get-DbcConfigValue policy.storage.backuppath if (-not$backuppath) { $backuppath = (Get-DbaDefaultPath-SqlInstance $psitem).Backup } It \u0026#34;can access backup path ($backuppath) on $psitem\u0026#34; { Test-DbaSqlPath-SqlInstance $psitem -Path $backuppath| Should -BeTrue -Because \u0026#39;The SQL Service account needs to have access to the backup path to backup your databases\u0026#39; } } } } } will find the describe block and get the title ‚ÄúBackup Path Access‚Äù¬†and the tags BackupPathAccess, Storage, DISA, $filename and then find the¬†Get-Instance and set the type to SqlInstance\nUntil Rob breaks it! This has worked wonderfully well for 6 months or so of the life of dbachecks but this week I broke it!\nThe problem was the performance of the code. It is taking a long time to run the tests and I am looking at ways to improve this. I was looking at the Server.Tests file because I thought why not start with one of the smaller files.\nIt runs the following checks\nServer Power Plan Configuration SPNs Disk Space Ping Computer CPUPrioritisation Disk Allocation Unit Instance Connection and it was looping through the computer names for each check like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Describe \u0026#34;Server Power Plan Configuration\u0026#34; -Tags PowerPlan, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Instance Connection\u0026#34; -Tags InstanceConnection, Connectivity, $filename { @(Get-Instance).ForEach{ } } Describe \u0026#34;SPNs\u0026#34; -Tags SPN, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Disk Space\u0026#34; -Tags DiskCapacity, Storage, DISA, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Ping Computer\u0026#34; -Tags PingComputer, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;CPUPrioritisation\u0026#34; -Tags CPUPrioritisation, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Disk Allocation Unit\u0026#34; -Tags DiskAllocationUnit, $filename { @(Get-ComputerName).ForEach{ } } I altered it to have only one loop for the computer names like so\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @(Get-ComputerName).ForEach{ Describe \u0026#34;Server Power Plan Configuration\u0026#34; -Tags PowerPlan, $filename { } Describe \u0026#34;SPNs\u0026#34; -Tags SPN, $filename { } Describe \u0026#34;Disk Space\u0026#34; -Tags DiskCapacity, Storage, DISA, $filename { } Describe \u0026#34;Ping Computer\u0026#34; -Tags PingComputer, $filename { } Describe \u0026#34;CPUPrioritisation\u0026#34; -Tags CPUPrioritisation, $filename { } Describe \u0026#34;Disk Allocation Unit\u0026#34; -Tags DiskAllocationUnit, $filename { } } Describe \u0026#34;Instance Connection\u0026#34; -Tags InstanceConnection, Connectivity, $filename { @(Get-Instance).ForEach{ } } and immediately in testing my checks for the Server Tag decreased in time by about 60% üôÇ\nI was very happy.\nThen I added it to the dbachecks module on my machine, loaded the module and realised that my Json file for Get-DbcCheck was no longer being populated for the type because this line\n1 elseif ($Describe.Parent-match\u0026#34;Get-ComputerName\u0026#34;-or$Describe.Parent-match\u0026#34;AllServerInfo\u0026#34;) was no longer true.\nAST for other things So I googled Management.Automation.Language.Ast¬†the first result lead me to docs.microsoft¬†There are a number of different language elements available there and I found¬†InvokeMemberExpressionAst¬†which will let me find any methods that have been invoked, so now I can find the loops with\n1 2 3 4 $ComputerNameForEach = $CheckFileAST.FindAll([Func[Management.Automation.Language.Ast, bool]] { param ($ast) $ast -is [System.Management.Automation.Language.InvokeMemberExpressionAst] }, $true) When I examined the object returned I could see that I could further limit the result to get only the method for Get-ComputerName and then if I choose the Extent I can get the code of that loop\n1 2 3 4 5 ## New code uses a Computer Name loop to speed up execution so need to find that as well $ComputerNameForEach=$CheckFileAST.FindAll([Func[Management.Automation.Language.Ast,bool]] { param ($ast) $ast-is [System.Management.Automation.Language.InvokeMemberExpressionAst] -and$ast.expression.Subexpression.Extent.Text-eq\u0026#39;Get-ComputerName\u0026#39; }, $true).Extent and now I can match the Tags to the type again :-)\n1 2 3 if ($ComputerNameForEach-match$title) { $type=\u0026#34;ComputerName\u0026#34; } and now Get-DbcCheck is returning the right results and the checks are a little faster\nYou can find dbachecks on the PowerShell Gallery or install it using\nInstall-Module dbachecks -Scope CurrentUser\n","date":"2018-08-16T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/08/server.png","permalink":"https://blog.robsewell.com/blog/using-the-powershell-ast-to-find-a-foreach-method/","title":"Using the PowerShell AST to find a ForEach Method"},{"content":" Photo by¬†Farrel Nobel¬†on¬†Unsplash\nWelcome to another edition of T-SQL Tuesday!\nThis T-SQL Tuesday is hosted by Wayne Sheffield (¬†blog¬†|¬†twitter¬†) and¬†he has asked us to talk about\n[‚Ä¶]¬†a time when you ran up against your own brick wall, and how you worked it out or dealt with it.\nHitting The Wall When stuck in a problem in the past, I could often be found glued to a keyboard and screen for many hours. I would try this way and that way, I would Google and read Stack Overflow looking for ways around the particular issue I was seeing trying different things and finally I would get frustrated and fed up and stop.\nMaybe I would go and walk the dog, maybe just sit somewhere else but I would often find that I had an idea how to solve my problem and quickly go back to the keyboard frustrated that I hadnt thought about this earlier and frequently rinse and repeat.\nA Different Way It took me many years to realise this and I wish I had done so sooner but once I made the connection that leaving the problem to one side for a little while meant that I often found a way to a solution for a problem I started setting a time limit.\n30 minutes\nIf I have been stuck on a problem for 30 minutes, I (mostly, I still sometimes fail at this) stop, take a break, go for a walk or do something different and the number of times that I arrive if not at a solution then at a path to a solution is remarkable.\nAsk The other thing to do at this point in the troublesome problem solving is to ask. Twitter, Google, Slack, Stack Overflow. These are all excellent resources where you can quickly find people who are willing and capable of helping.\nDon‚Äôt be like me and take years to work this out üôÇ\n","date":"2018-08-14T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/08/farrel-nobel-97504-unsplash.jpg","permalink":"https://blog.robsewell.com/blog/hitting-the-wall-tsql2sday/","title":"Hitting the Wall ‚Äì #TSQL2sDay"},{"content":"A Question Shortly after the European PowerShell Conference and the PowerShell and Devops 2018 summit in the USA Mike Robbins b | t contacted me with a question.\nInterested in writing a chapter in a PowerShell book?\nI was intrigued and read on.\nA Conference in a Book There was more to this book than just writing about PowerShell though. Mike was suggesting that a group of wonderful PowerShell experts (Here‚Äôs a Twitter list) got together and created a conference in a book.\nThe book is designed as a conference in a book where each chapter is written independently with content similar to what you would present in a 45 minute presentation.\nThat‚Äôs a neat idea, people who couldn‚Äôt come to one of the conferences would be able to get an experience a little bit like attending a conference but in book form.\nOK there would be no networking, evening entertainment or instance responses to questions but a bundle of useful information that you can take with you and read anywhere.\nIt‚Äôs All For Charity (I hope older UK viewers read that in this voice üôÇ )\nThe bit that clinched it for me was this though\nWe‚Äôre donating all of the royalties from the book to the DevOps Collective Scholarship program https://leanpub.com/causes/devopscollective.\nAll the money raised by buying this book¬†will go to the¬†DevOps Collective OnRamp Scholarship program.\nThis scholarship provides\nticket to PowerShell and DevOps Global Summit OnRamp track specifically designed for entry-level professionals five nights lodging domestic airfare buddy programme and half of the slots are reserved for under-represented groups.\nI really approve of this idea, without the help and support of the SQL and PowerShell technical communities I would not be where I am today and this will help to bring other people in at an early stage in their career. I am proud that I can give a little back.\nFabulous Editors So I said yes.\nI then had to sit down and write some words. I wrote about how we created dbachecks, the challenges we faced and how we overcame them.\nOne of my biggest challenges was writing in the wrong English! The book is written in American English and there are zeds where there should be esses and missing u‚Äôs in words! My spell checker was covered in red squiggles! The second challenge was getting the code to fit the column limit for the book. I show a lot of the AST code that we use to validate that dbachecks code will work correctly and it doesnt split to 80 characters very easily.\nLuckily I had 3 wonderful, patient editors to help me with all of this. Mike Robbins , Michael T Lombardi and Jeff Hicks each helped me to make the chapter read more fluently, make sense and be spelled correctly!\nThank you very much you three for all the work you have put into this book.\nHelp Yourself and Others If you want to attend a PowerShell conference in book form, want 30 chapters of fabulous PowerShell material and want to help grow and diversify our industry then look no further you can get the book here\nYou can also find all of the authors twitters and websites below, You should go and see what they are sharing there as well.\nüëç¬†Thank you to Mike Robbins, Mike Kanakos and Rob Pleau for having all the links below already handy in their blog posts at the time of writing this! Author\nWebsite\nMike F Robbins\nhttps://mikefrobbins.com\nJeff Hicks\nhttps://jdhitsolutions.com\nMichael Lombardi\nhttps://appoint.ly/t/michaeltlombardi\nAdam Murry\nhttps://tikabu.com.au/blog/\nAnthony Nocentino\nhttp://www.centinosystems.com\nBrandon Olin\nhttps://devblackops.io\nBrian Bunke\nhttps://www.brianbunke.com\nDon Jones\nhttps://donjones.com\nDoug Finke\nhttps://dfinke.github.io\nEmin Atac\nhttps://p0w3rsh3ll.wordpress.com\nFred Weinmann\nhttps://allthingspowershell.blogspot.com\nGraham Beer\nhttps://graham-beer.github.io\nIrwin Strachan\nhttps://pshirwin.wordpress.com\nJames Petty\nhttps://scriptautomaterepeat.com\nJeremy Murrah\nhttps://murrahjm.github.io\nJustin Sider\nhttps://invoke-automation.blog\nLuc Dekens\nhttp://www.lucd.info\nMark Kraus\nhttps://get-powershellblog.blogspot.com\nMark Wragg\nhttps://wragg.io\nMike Kanakos\nhttps://www.networkadm.in\nMike Shepard\nhttps://powershellstation.com\nPatrick Gruenauer\nhttps://sid-500.com\nPrateek Singh\nhttps://ridicurious.com\nRob Pleau\nhttps://ephos.github.io\nThomas Lee\nhttps://tfl09.blogspot.com\nThomas Rayner\nhttps://workingsysadmin.com\nThom Schumacher\nhttps://powershellposse.com\nTim Curwick\nhttps://MadWithPowerShell.com\nTim Warner\nhttps://timwarnertech.com\nTommy Maynard\nhttps://tommymaynard.com\nTore Groneng\nhttps://asaconsultant.blogspot.com\nWesley Kirkland\nhttps://wesleyk.me\n","date":"2018-08-01T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/08/book.jpeg","permalink":"https://blog.robsewell.com/blog/a-powershell-conference-in-a-book/","title":"A PowerShell Conference In A Book"},{"content":"I like to write Pester checks to make sure that all is as expected! This is just a quick post as much to help me remember this script üôÇ\nThis is a quick Pester test I wrote to ensure that some SQL Scripts in a directory would parse so there was some guarantee that they were valid T-SQL. It uses the SQLParser.dll and because it was using a build server without SQL Server I have to load the required DLLs from the dbatools module (Thank you dbatools üôÇ )\nIt simply runs through all of the .sql files and runs the parser against them and checks the errors. In the case of failures it will output where it failed in the error message in the failed Pester result as well.\nYou will need dbatools module installed on the instance and at least version 4 of the Pester module as well\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Describe \u0026#34;Testing SQL\u0026#34; { Context \u0026#34;Running Parser\u0026#34; { ## Load assembly $Parserdll = (Get-ChildItem \u0026#39;C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules\\\\dbatools\u0026#39; -Include Microsoft.SqlServer.Management.SqlParser.dll -Recurse)\\[0\\].FullName \\[System.Reflection.Assembly\\]::LoadFile($Parserdll) | Out-Null $TraceDll = (Get-ChildItem \u0026#39;C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules\\\\dbatools\u0026#39; -Include Microsoft.SqlServer.Diagnostics.Strace.dll -Recurse)\\[0\\].FullName \\[System.Reflection.Assembly\\]::LoadFile($TraceDll) | Out-Null $ParseOptions = New-Object Microsoft.SqlServer.Management.SqlParser.Parser.ParseOptions $ParseOptions.BatchSeparator = \u0026#39;GO\u0026#39; $files = Get-ChildItem -Path $Env:Directory -Include *.sql -Recurse ## This variable is set as a Build Process Variable or put your path here $files.ForEach{ It \u0026#34;$($Psitem.FullName) Should Parse SQL correctly\u0026#34; { $filename = $Psitem.FullName $sql = Get-Content -LiteralPath \u0026#34;$fileName\u0026#34; $Script = \\[Microsoft.SqlServer.Management.SqlParser.Parser.Parser\\]::Parse($SQL, $ParseOptions) $Script.Errors | Should -BeNullOrEmpty } } } } ","date":"2018-07-25T00:00:00Z","permalink":"https://blog.robsewell.com/blog/a-powershell-pester-check-for-parsing-sql-scripts/","title":"A PowerShell Pester Check for parsing SQL scripts"},{"content":"in my last post I showed how you can save the results of dbachecks to a database and created a PowerBi report. Inspired by¬†Frank Henninger in the #dbachecks slack channel and Shawn Melton¬†who explained the difficulties with red/green colour blind I then created this one üôÇ\nYou can find it in my GitHub and have a play with it below\n","date":"2018-05-28T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/dark-mode.png","permalink":"https://blog.robsewell.com/blog/dbachecks-dark-mode-historical-validation-powerbi/","title":"dbachecks ‚Äì Dark Mode Historical Validation PowerBi"},{"content":"I gave a presentation at SQL Day in Poland last week on dbachecks and one of the questions I got asked was will you write a command to put the results of the checks into a database for historical reporting.\nThe answer is no and here is the reasoning. The capability is already there. Most good PowerShell commands will only return an object and the beauty of an object is that you can do anything you like with it. Your only limit is your imagination üôÇ I have written about this before here.¬†The other reason is that it would be very difficult to write something that was easily configurable for the different requirements that people will require. But here is one way of doing it.\nCreate a configuration and save it Let‚Äôs define a configuration and call it production. This is something that I do all of the time so that I can easily run a set of checks with the configuration that I want.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # The computername we will be testing Set-DbcConfig -Name app.computername -Value $sql0,$SQl1 # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value $sql0,$SQl1 # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;THEBEARD\\\\EnterpriseAdmin\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;sa\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompression -Value $true # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $true # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value FULL # What should ourt database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;KERBEROS\u0026#39; # Which Agent Operator should be defined? Set-DbcConfig -Name agent.dbaoperatorname -Value \u0026#39;The DBA Team\u0026#39; # Which Agent Operator email should be defined? Set-DbcConfig -Name agent.dbaoperatoremail -Value \u0026#39;TheDBATeam@TheBeard.Local\u0026#39; # Which failsafe operator shoudl be defined? Set-DbcConfig -Name agent.failsafeoperator -Value \u0026#39;The DBA Team\u0026#39; ## Set the database mail profile name Set-DbcConfig -Name agent.databasemailprofile -Value \u0026#39;DbaTeam\u0026#39; # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value master # What is the maximum time since I took a Full backup? Set-DbcConfig -Name policy.backup.fullmaxdays -Value 7 # What is the maximum time since I took a DIFF backup (in hours) ? Set-DbcConfig -Name policy.backup.diffmaxhours -Value 26 # What is the maximum time since I took a log backup (in minutes)? Set-DbcConfig -Name policy.backup.logmaxminutes -Value 30 # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;TheBeard.Local\u0026#39; # Where is my Ola database? Set-DbcConfig -Name policy.ola.database -Value master # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, PseudoSimple,SPN, TestLastBackupVerifyOnly,IdentityUsage,SaRenamed # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 ## I need to set the app.cluster configuration to one of the nodes for the HADR check ## and I need to set the domain.name value Set-DbcConfig -Name app.cluster -Value $SQL0 Set-DbcConfig -Name domain.name -Value \u0026#39;TheBeard.Local\u0026#39; ## I also skip the ping check for the listener as we are in Azure Set-DbcConfig -Name skip.hadr.listener.pingcheck -Value $true Now I can export that configuration to a json file and store on a file share or in source control using the code below. This makes it easy to embed the checks into an automation solution\nExport-DbcConfig -Path Git:\\\\Production.Json\nand then I can use it with\n1 2 Import-DbcConfig -Path Git:\\\\Production.Json Invoke-DbcCheck I would use one of the Show parameter values here if I was running it at the command line, probably fails to make reading the information easier\nAdd results to a database This only gets us the test results on the screen, so if we want to save them to a database we have to use the PassThru parameter for Invoke-DbcCheck. I will run the checks again, save them to a variable\n$Testresults = Invoke-DbcCheck -PassThru -Show Fails\nThen I can use the dbatools Write-DbaDatatable¬†command to write the results to a table in a database. I need to do this twice, once for the summary and once for the test results\n1 2 $Testresults | Write-DbaDataTable -SqlInstance $sql0 -Database tempdb -Table Prod_dbachecks_summary -AutoCreateTable $Testresults.TestResult | Write-DbaDataTable -SqlInstance $sql0 -Database tempdb -Table Prod_dbachecks_detail -AutoCreateTable and I get two tables one for the summary\nand one for the details\nThis works absolutely fine and I could continue to add test results in this fashion but it has no date property so it is not so useful for reporting.\nCreate tables and triggers This is one way of doing it. I am not sure it is the best way but it works! I always look forward to how people take ideas and move them forward so if you have a better/different solution please blog about it and reference it in the comments below\nFirst I created a staging table for the summary results\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CREATE TABLE [dbachecks].[Prod_dbachecks_summary_stage]( [TagFilter] [nvarchar](max) NULL, [ExcludeTagFilter] [nvarchar](max) NULL, [TestNameFilter] [nvarchar](max) NULL, [TotalCount] [int] NULL, [PassedCount] [int] NULL, [FailedCount] [int] NULL, [SkippedCount] [int] NULL, [PendingCount] [int] NULL, [InconclusiveCount] [int] NULL, [Time] [bigint] NULL, [TestResult] [nvarchar](max) NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO and a destination table with a primary key and a date column which defaults to todays date\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 CREATE TABLE [dbachecks].[Prod_dbachecks_summary]( [SummaryID] [int] IDENTITY(1,1) NOT NULL, [TestDate] [date] NOT NULL, [TagFilter] [nvarchar](max) NULL, [ExcludeTagFilter] [nvarchar](max) NULL, [TestNameFilter] [nvarchar](max) NULL, [TotalCount] [int] NULL, [PassedCount] [int] NULL, [FailedCount] [int] NULL, [SkippedCount] [int] NULL, [PendingCount] [int] NULL, [InconclusiveCount] [int] NULL, [Time] [bigint] NULL, [TestResult] [nvarchar](max) NULL, CONSTRAINT [PK_Prod_dbachecks_summary] PRIMARY KEY CLUSTERED ( [SummaryID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbachecks].[Prod_dbachecks_summary] ADD CONSTRAINT [DF_Prod_dbachecks_summary_TestDate] DEFAULT (getdate()) FOR [TestDate] GO and added an INSERT trigger to the staging table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE TRIGGER [dbachecks].[Load_Prod_Summary] ON [dbachecks].[Prod_dbachecks_summary_stage] AFTER INSERT AS BEGIN \\\\ SET NOCOUNT ON added to prevent extra result sets from \\\\ interfering with SELECT statements. SET NOCOUNT ON; INSERT INTO [dbachecks].[Prod_dbachecks_summary] ([TagFilter], [ExcludeTagFilter], [TestNameFilter], [TotalCount], [PassedCount], [FailedCount], [SkippedCount], [PendingCount], [InconclusiveCount], [Time], [TestResult]) SELECT [TagFilter], [ExcludeTagFilter], [TestNameFilter], [TotalCount], [PassedCount], [FailedCount], [SkippedCount], [PendingCount], [InconclusiveCount], [Time], [TestResult] FROM [dbachecks].[Prod_dbachecks_summary_stage] END GO ALTER TABLE [dbachecks].[Prod_dbachecks_summary_stage] ENABLE TRIGGER [Load_Prod_Summary] GO and for the details I do the same thing. A details table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CREATE TABLE [dbachecks].[Prod_dbachecks_detail]( [DetailID] [int] IDENTITY(1,1) NOT NULL, [SummaryID] [int] NOT NULL, [ErrorRecord] [nvarchar](max) NULL, [ParameterizedSuiteName] [nvarchar](max) NULL, [Describe] [nvarchar](max) NULL, [Parameters] [nvarchar](max) NULL, [Passed] [bit] NULL, [Show] [nvarchar](max) NULL, [FailureMessage] [nvarchar](max) NULL, [Time] [bigint] NULL, [Name] [nvarchar](max) NULL, [Result] [nvarchar](max) NULL, [Context] [nvarchar](max) NULL, [StackTrace] [nvarchar](max) NULL, CONSTRAINT [PK_Prod_dbachecks_detail] PRIMARY KEY CLUSTERED ( [DetailID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail] WITH CHECK ADD CONSTRAINT [FK_Prod_dbachecks_detail_Prod_dbachecks_summary] FOREIGN KEY([SummaryID]) REFERENCES [dbachecks].[Prod_dbachecks_summary] ([SummaryID]) GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail] CHECK CONSTRAINT [FK_Prod_dbachecks_detail_Prod_dbachecks_summary] GO A stage table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE [dbachecks].[Prod_dbachecks_detail_stage]( [ErrorRecord] [nvarchar](max) NULL, [ParameterizedSuiteName] [nvarchar](max) NULL, [Describe] [nvarchar](max) NULL, [Parameters] [nvarchar](max) NULL, [Passed] [bit] NULL, [Show] [nvarchar](max) NULL, [FailureMessage] [nvarchar](max) NULL, [Time] [bigint] NULL, [Name] [nvarchar](max) NULL, [Result] [nvarchar](max) NULL, [Context] [nvarchar](max) NULL, [StackTrace] [nvarchar](max) NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO with a trigger\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 CREATE TRIGGER [dbachecks].[Load_Prod_Detail] ON [dbachecks].[Prod_dbachecks_detail_stage] AFTER INSERT AS BEGIN \\\\ SET NOCOUNT ON added to prevent extra result sets from \\\\ interfering with SELECT statements. SET NOCOUNT ON; INSERT INTO [dbachecks].[Prod_dbachecks_detail] ([SummaryID],[ErrorRecord], [ParameterizedSuiteName], [Describe], [Parameters], [Passed], [Show], [FailureMessage], [Time], [Name], [Result], [Context], [StackTrace]) SELECT (SELECT MAX(SummaryID) From [dbachecks].[Prod_dbachecks_summary]),[ErrorRecord], [ParameterizedSuiteName], [Describe], [Parameters], [Passed], [Show], [FailureMessage], [Time], [Name], [Result], [Context], [StackTrace] FROM [dbachecks].[Prod_dbachecks_detail_stage] END GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail_stage] ENABLE TRIGGER [Load_Prod_Detail] GO Then I can use Write-DbaDatatable with a couple of extra parameters, FireTriggers to run the trigger, Truncate and Confirm:$false to avoid any confirmation because I want this to run without any interaction and I can get the results into the database.\n1 2 $Testresults | Write-DbaDataTable -SqlInstance $Instance -Database $Database -Schema dbachecks -Table Prod_dbachecks_summary_stage -FireTriggers -Truncate -Confirm:$False $Testresults.TestResult | Write-DbaDataTable -SqlInstance $Instance -Database $Database -Schema dbachecks -Table Prod_dbachecks_detail_stage -FireTriggers -Truncate -Confirm:$False Which means that I can now query some of this data and also create PowerBi reports for it.\nTo enable me to have results for the groups in dbachecks I have to do a little bit of extra manipulation. I can add all of the checks to the database using\n1 Get-DbcCheck | Write-DbaDataTable -SqlInstance $sql0 -Database ValidationResults -Schema dbachecks -Table Checks -Truncate -Confirm:$False -AutoCreateTable But because the Ola Hallengren Job names are configuration items I need to update the values for those checks which I can do as follows\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $query = \u0026#34; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.systemfull) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$SysFullJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserFull) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserFullJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserDiff) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserDiffJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserLog) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserLogJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.CommandLogCleanup) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$CommandLogJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.SystemIntegrity) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$SysIntegrityJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserIntegrity) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserIntegrityJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserIndex) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserIndexJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.OutputFileCleanup) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$OutputFileJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.DeleteBackupHistory) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$DeleteBackupJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.PurgeBackupHistory) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$PurgeBackupJobName\u0026#39; \u0026#34; Invoke-DbaSqlQuery -SqlInstance $SQL0 -Database ValidationResults -Query $query You can get a sample Power Bi report in my Github which also has the code from this blog post\nThen you just need to open in PowerBi Desktop and\nClick Edit Queries\nClick Data Source Settings\nClick Change Source\nChange the Instance and Database names\nThen have an interactive report like this. Feel free to click around and see how it works. Use the arrows at the bottom right to go full-screen. NOTE ‚Äì it filters by ‚Äútoday‚Äù so if I haven‚Äôt run the check and the import then click on one of the groups under ‚ÄúToday‚Äôs Checks by Group‚Äù\nThis enables me to filter the results and see what has happened in the past so I can filter by one instance\nor I can filter by a group of tests\nor even by a group of tests for an instance\nHopefully, this will give you some ideas of what you can do with your dbachecks results. You can find all of the code and the PowerBi in my GitHub\nHappy Validating!\n","date":"2018-05-23T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/08-filter-by-instance-and-insance.png","permalink":"https://blog.robsewell.com/blog/dbachecks-save-the-results-to-a-database-for-historical-reporting/","title":"dbachecks ‚Äì Save the results to a database for historical reporting"},{"content":"With the latest release of dbachecks we have added a new check for testing that foreign keys and constraints are trusted thanks to Cl√°udio Silva b | t\nTo get the latest release you will need to run\nUpdate-Module dbachecks\rYou should do this regularly as we release new improvements frequently.\nWe have also added better descriptions for the checks which was suggested by the same person who inspired the previous improvement I blogged about here\nInstead of the description just being the name of the check it is now more of a, well, a description really üôÇ\nThis has the added effect that it means that just running Get-DbcCheck in the command line will not fit all of the information on a normal screen\nYou can use the Format-Table command (or its alias ft at the command line) and select the properties to display using\nGet-DbcCheck | ft -Property UniqueTag, Description -Wrap\ror you can use Format-List¬†(or its alias fl at the command line)\nGet-DbcCheck | fl\rOr you can use Out-GridView¬†(or its alias ogv at the command line) (Incidentally, could you also thumbs up this issue on Github to get Out-GridView functionality in PowerShell 6)\nGet-DbcCheck | ogv\rHappy Validating !\n","date":"2018-05-19T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/04-get-dbacheck-ogv.png","permalink":"https://blog.robsewell.com/blog/dbachecks-improved-descriptions/","title":"dbachecks ‚Äì Improved Descriptions"},{"content":"I love showing dbachecks to people. It‚Äôs really cool seeing how people will use it and listening to their experiences. I was showing it to a production DBA a month or so ago and he said\nHow Do I Know Which Checks There Are? OK you just need to run\nGet-DbcCheck\nand it will show you\nIt will show you the group, the type (does it need a computer name or an instance name), The description, the unique tag for running just that check and all the tags that will run that check\nOK he said, you talked about configurations\nHow Do I Know Which Configurations There Are? So to do that you just need to run\nGet-DbcConfig\nand it will show you\nYou can see the name, the current value and the description\nAh thats cool he said so\nHow Do I Know Which Configuration Is For Which Check? Well, you just‚Ä¶. , you know‚Ä¶‚Ä¶ AHHHHHHH\nPing ‚Äì light bulb moment!\nIt‚Äôs always really useful to give something you have built to people who have never seen it before and then listen to what they say. Their new eyes and different experiences or expectations will give you lots of insight\nNone of the amazing contributors to dbachecks had thought of this scenario so I decided to fix this. First I asked for an issue to be raised in GitHub¬†because an issue can be an improvement or a suggestion not just a bug.\nThen I fixed it so that it would do what was required. Thank you Nick for this feedback and for helping to improve dbachecks\nI improved Get-DbcCheck so that now it shows the configuration item related to each check\nIt is easier to see (and sort or search) if you use Out-GridView\nGet-DbcCheck | Out-GridView\rSo now you can see which configuration can be set for each check!\nHappy Validating!\n","date":"2018-05-15T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/03-New-dbccheck.png","permalink":"https://blog.robsewell.com/blog/dbachecks-which-configuration-item-for-which-check/","title":"dbachecks ‚Äì Which Configuration Item For Which Check ?"},{"content":"I am working on my dbatools and dbachecks presentations for SQL Saturday Finland, SQLDays, SQL Saturday Cork and SQLGrillen¬†I want to show the two modules running against a number of SQL Versions so I have installed\n2 Domain Controllers 2 SQL 2017 instances on Windows 2016 with an Availability Group and WideWorldImporters database 1 Windows 2016 jump box with all the programmes I need 1 Windows 2016 with containers using a VSTS build and this set of ARM templates and scripts\nI wanted to create containers running SQL2017, SQL2016, SQL2014 and SQL2012 and restore versions of the AdventureWorks database onto each one.\nMove Docker Location I redirected my docker location from my C:\\ drive to my E:\\ drive so I didnt run out of space. I did this by creating a daemon.json file in¬†C:\\ProgramData\\docker\\config and adding\n{\u0026quot;data-root\u0026quot;: \u0026quot;E:\\containers\u0026quot;}\nand restarting the docker service which created folders like this\nThen I ran\ndocker volume create SQLBackups\nto create a volume to hold the backups that I could mount on the containers\nAdventureWorks Backups I downloaded all the AdventureWorks backups from GitHub¬†and copied them to¬†E:\\containers\\volumes\\sqlbackups\\_data\nGet-ChildItem $Home\\Downloads\\AdventureWorks* | Copy-Item -Destination E:\\containers\\volumes\\sqlbackups\\_data\nGetting the Images To download the SQL 2017 image from the DockerHub¬†I ran\ndocker pull¬†microsoft/mssql-server-windows-developer:latest\nand waited for it to download and extract\nI also needed the images for other versions. My good friend Andrew Pruski b | t has versions available for us to use on his Docker Hub¬†so it is just a case of running\n1 2 3 docker pull dbafromthecold/sqlserver2016dev:sp1 docker pull dbafromthecold/sqlserver2014dev:sp2 docker pull dbafromthecold/sqlserver2012dev:sp4 and waiting for those to download and extract (This can take a while!)\nCreate the containers Creating the containers is as easy as\ndocker run -d -p ExposedPort:InternalPort --name NAME -v VolumeName:LocalFolder -e sa\\_password=THEPASSWORD -e ACCEPT\\_EULA=Y IMAGENAME\nso all I needed to run to create 4 SQL containers one of each version was\n1 2 3 4 docker run -d -p 15789:1433 --name 2017 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y microsoft/mssql-server-windows-developer docker run -d -p 15788:1433 --name 2016 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2016dev:sp1 docker run -d -p 15787:1433 --name 2014 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2014dev:sp2 docker run -d -p 15786:1433 --name 2012 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2012dev:sp4 and just a shade over 12 seconds later I have 4 SQL instances ready for me üôÇ\nStoring Credentials This is not something I would do in a Production environment but I save my credentials using this method that Jaap Brasser b | t shared here\nGet-Credential | Export-Clixml -Path $HOME\\Documents\\sa.cred\nwhich means that I can get the credentials in my PowerShell session (as long as it is the same user that created the file) using\n$cred = Import-Clixml $HOME\\Documents\\sa.cred\nRestoring the databases I restored all of the AdventureWorks databases that each instance will support onto each instance, so 2017 has all of them whilst 2012 only has the 2012 versions.\nFirst I needed to get the filenames of the backup files into a variable\n$filenames = (Get-ChildItem '\\bearddockerhost\\e$\\containers\\volumes\\sqlbackups\\_data').Name\nand the container connection strings, which are the hostname and the port number\n$containers = 'bearddockerhost,15789', 'bearddockerhost,15788', 'bearddockerhost,15787', 'bearddockerhost,15786'\nthen I can restore the databases using dbatools¬†using a switch statement on the version which I get with the NameLevel property of¬†Get-DbaSqlBuildReference\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $cred = Import-Clixml $HOME\\Documents\\sa.cred $containers = \u0026#39;bearddockerhost,15789\u0026#39;, \u0026#39;bearddockerhost,15788\u0026#39;, \u0026#39;bearddockerhost,15787\u0026#39;, \u0026#39;bearddockerhost,15786\u0026#39; $filenames = (Get-ChildItem \u0026#39;\\bearddockerhost\\e$\\containers\\volumes\\sqlbackups\\_data\u0026#39;).Name $containers.ForEach{ $Container = $Psitem $NameLevel = (Get-DbaSqlBuildReference-SqlInstance $Container-SqlCredential $cred).NameLevel switch ($NameLevel) { 2017 { Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path C:\\sqlbackups\\ -useDestinationDefaultDirectories -WithReplace |Out-Null Write-Verbose-Message \u0026#34;Restored Databases on 2017\u0026#34; } 2016 { $Files = $Filenames.Where{$PSitem -notlike \u0026#39;\\*2017\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2016\u0026#34; } 2014 { $Files = $Filenames.Where{$PSitem -notlike \u0026#39;\\*2017\\*\u0026#39; -and $Psitem -notlike \u0026#39;\\*2016\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2014\u0026#34; } 2012 { $Files = $Filenames.Where{$PSitem -like \u0026#39;\\*2012\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2012\u0026#34; } Default {} } } I need to create the file paths for each backup file by getting the correct backups and appending the names to C:\\SQLBackups which is where the volume is mounted inside the container\nAs Get-DbaDatabase gives the container ID as the Computer Name I have highlighted each container below\nThat is how easy it is to create a number of SQL containers of differing versions for your presentations or exploring needs\nHappy Automating!\n","date":"2018-05-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/creating-sql-server-containers-for-versions-2012-2017/","title":"Creating SQL Server Containers for versions 2012-2017"},{"content":"\nIt‚Äôs TSQL Tuesday again! This month our host is Riley Major (b/t) and the subject is¬†Giving Back. He‚Äôs given us two options here (as well as the side option of¬†your favorite 2017 improvement). Pick a way that you‚Äôd like to give back to the community and talk about it, or if you already give back, tell us how and why you started.\nThere will be a lot of excellent posts on this subject and one of the things that I like is that you can go to¬†http://tsqltuesday.com¬†and look up all of the entries which means that for a particular topic you can find a bunch of blog posts from different angles (I miss-typed that as angels first and yes they are all angels!) whether it is career improvement¬†or technical like say Extended Events¬†it is all there waiting for you üôÇ\nI wrote about giving back in 2014¬†when I was about to help organise SQL Saturday Exeter for the first time. Last year I blogged about a few of the wonderful people who made a difference to me. So this post is a little about how I give back and also hopefully some hints that can help you to do the same as well.\nHow There are so many ways that you can give back to the community as Riley‚Äôs post shows.\nSharing your knowledge is a good way. I share my knowledge in blog posts and in sessions at user groups and at conferences. You can too.\nFrom beginner, introductory posts and talks to expert level deep dives every single one of us has learned from the blog posts that other people have spent their time and effort creating.\nYOU can share your knowledge, even if you have only been using a technology for a few months, you have knowledge of the things you have learned and the things that would have made it easier. Write a post about those things.\nAnswering questions is another way. You can do this on Stack Overflow, SQL Server Central, PowerShell.Org , Reddit, Facebook , on Twitter using the #sqlhelp or #PowerShellHelp or just in person. I try to answer questions when I see them on twitter or in the SQL Community Slack\nMentoring or just providing feedback to people. You can offer to proof read blog posts or abstract submissions or you can listen to peoples presentations. I do this and it is a lot of fun\nI also share my knowledge via my GitHub. All of my presentations slides and code are available as well as other code that I use. I also contribute to open-source projects such as dbatools¬†and dbachecks.¬†You can do this too. You could open an issue for an improvement or bug. You can contribute your code, even if you are not confident writing the code you can fix spelling mistakes or add documentation., everything helps\nYou can help with organisation of events. I have helped to organise the PowerShell Europe Conference, PSDay.UK, SQL Saturday Exeter, SQL SouthWest user group, PASS PowerShell Virtual Group. I have also helped with session choices for a number of other events like SQL Grillen and SQLGLA and I have volunteered at many events from SQL Saturdays to SQL Bits. Everything from setting up and tearing down (IE moving heavy things and cleaning up rubbish) to sitting on the information desk, giving out badges, making sure the speakers are on time in their sessions. You can do this too. Just ask the organisers of the events what they need. It is better to do this prior to the event than on the day but I am sure all help is welcomed. Richard Munn and I talked (waffled?) about this at SQL Bits this year\nIf you would like to be considered as a volunteer for next years SQL Bits please email helpers at sqlbits.com\nWhy So why did I start giving back?\nI wanted to be useful. I saw the amount of work that Jonathan and Annette were doing organising SQL Saturday Exeter and SQL South West. I hoped that I could help them with that.\nWhy do I carry on doing it?\nBecause it is fun üôÇ I enjoy speaking, I enjoy sharing my knowledge and talking to people\nThat is good but there is more to it as well\nYou learn so much by writing a presentation or a blog post because you will do research.\nYou will learn even more when people ask you questions in your sessions or leave comments on your blog posts and you have to go and find the answers\nYou learn new and useful skills and demonstrate your knowledge to potential employers\nMy blog is a scrapbook of knowledge that I go back to and use all the time (and sometimes I forget that I have written something and find my own post in the search results!)\nBut the most important reason is that I feel that it pays back some of the benefit that I have gained from all of those people who‚Äôs time and effort I made use of for free when I was learning and continue to do so to this day. All of those blog posts and videos and presentations that I consumed have helped to make me the technician I am today. I have skills and abilities that I would not have without them all and by giving back I hope that I am enabling others to develop and see the benefit of sharing so that they will continue to do so in the future and I can learn from them.\n","date":"2018-05-08T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/08/tsql2sday.jpg","permalink":"https://blog.robsewell.com/blog/tsql2sday-giving-back-reprise/","title":"#tsql2sday ‚Äì Giving Back ‚Äì Reprise"},{"content":"There was an announcement on the Visual Studio Code blog about the public preview of Live Share. This enables you to easily collaborate on code by securely sharing your coding session.\nIt is remarkably easy to set up üôÇ\nInstallation Open Visual Studio Code, open the Extensions side bar (CTRL + SHIFT + X)\nSearch for Live Share\nClick Install and then reload when it has done\nYou will notice in the bottom bar it will say finishing the installation and if you open the terminal (CTRL + ‚Äò) and click on Output and change the drop down on the right to Visual Studio Live Share you can see what it is doing\nIt is installing the dependancies as shown below\n[Client I] Installing dependencies for Live Share‚Ä¶\n[Client I] Downloading package ‚Äò.NET Core Runtime 2.0.5 for win7-x86‚Äô\n[Client I] Download complete.\n[Client I] Downloading package ‚ÄòOmniSharp for Windows (.NET 4.6)‚Äô\n[Client I] Download complete.\n[Client I] Installing package ‚Äò.NET Core Runtime 2.0.5 for win7-x86‚Äô\n[Client V] Extracted packed files\n[Client I] Validated extracted files.\n[Client I] Moved and validated extracted files.\n[Client I] Finished installing.\n[Client I] Installing package ‚ÄòOmniSharp for Windows (.NET 4.6)‚Äô\n[Client V] Extracted packed files\n[Client I] Validated extracted files.\n[Client I] Finished installing.\n[Client I] No workspace id found.\nIncidentally, this will also show the location of the log file\nYou will see in the bottom bar it will now say sign in\nClicking that will open a browser and give you a choice of accounts to sign in with, your GitHub or your Microsoft ID\nChoose the one that you want to use and do your 2FA.\nYou do have 2FA on your Microsoft and GitHub (and all the other services)? If not go and set it up now ‚Äì here for Microsoft and here for GitHub¬†Once you have signed in you will get this notification which you can close\nThe icon in the bottom will change and show your account name and if you click it it will open the menu\nSharing To share your session you click on the Share icon in the bottom bar or the Start collaboration session in the menu above. The first time you do this there will be a pop-up as shown\nYou can decide which way you (or your organisation) want to share. I chose to accept the firewall exception.\nThe invite link is in your clipboard ready to share with your friends and colleagues (other open source contributors ??)\nThey can either open the link in a browser\nor by using the Join Collaboration Session in the menu in VS Code\nOnce they do the sharer will get a notification\nand the person who has joined will have the same workspace opened in their Visual Studio Code\nYou can then collaborate on your code and share the session. In the video below the left hand side is running in my jump box in Azure and the right hand side on my laptop and you can see that if you highlight code in one side it is shown in the other and if you alter it in one side it is changed in the other. I also saved that file in the joined session rather than from the session that initialised the sharing and it then saved in both sessions üôÇ\nSo that shows how easy it is to install and to use. You can dive deeper using the documentation.\nHappy Collaborating üôÇ\n","date":"2018-05-08T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/07-sign-in.png","permalink":"https://blog.robsewell.com/blog/visual-studio-code-live-sharing-set-up/","title":"Visual Studio Code Live Sharing Set-Up"},{"content":"At the fabulous PowerShell Conference EU I presented about Continuous Delivery to the PowerShell Gallery with VSTS and explained how we use VSTS to enable CD for dbachecks. We even released a new version during the session üôÇ\nSo how do we achieve this?\nWe have a few steps\nCreate a project and link to our GitHub Run unit uests with Pester to make sure that our code is doing what we expect. Update our module version and commit the change to GitHub Sign our code with a code signing certificate Publish to the PowerShell Gallery Create Project and link to GitHub First you need to create a VSTS project by going to¬†https://www.visualstudio.com/ This is free for up to 5 users with 1 concurrent CI/CD queue limited to a maximum of 60 minutes run time which should be more than enough for your PowerShell module.\nClick on Get Started for free under Visual Studio Team Services and fill in the required information. Then on the front page click new project\nFill in the details and click create\nClick on builds and then new definition\nnext you need to link your project to your GitHub (or other source control providers) repository\nYou can either authorise with OAuth or you can provide a PAT token following the instructions here. Once that is complete choose your repo. Save the PAT as you will need it later in the process!\nand choose the branch that you want this build definition to run against.\nI chose to run the Unit Tests when a PR was merged into the development branch. I will then create another build definition for the master branch to sign the code and update module version. This enables us to push several PRs into the development branch and create a single release for the gallery.\nThen I start with an empty process\nand give it a suitable name\ni chose the hosted queue but you can download an agent to your build server if you need to do more or your integration tests require access to other resources not available on the hosted agent.\nRun Unit Tests with Pester We have a number of Unit tests in our tests folder in dbachecks¬†so we want to run them to ensure that everything is as it should be and the new code will not break existing functionality (and for dbachecks the format of the PowerBi)\nYou can use the¬†Pester Test Runner Build Task¬†from the folk at Black Marble¬†by clicking on the + sign next to Phase 1 and searching for Pester\nYou will need to click Get It Free to install it and then click add to add the task to your build definition. You can pretty much leave it as default if you wish and Pester will run all of the *.Tests.ps1 files that it finds in the directory where it downloads the GitHub repo which is referred to using the variable¬†$(Build.SourcesDirectory). It will then output the results to a json file called Test-Pester.XML ready for publishing.\nHowever, as dbachecks has a number of dependent modules, this task was not suitable. I spoke with Chris Gardner¬†b | t¬†from Black Marble at the PowerShell Conference and he says that this can be resolved so look out for the update. Chris is a great guy and always willing to help, you can often find him in the PowerShell Slack channel answering questions and helping people\nBut as you can use PowerShell in VSTS tasks, this is not a problem although you need to write your PowerShell using try catch to make sure that your task fails when your PowerShell errors. This is the code I use to install the modules\n$ErrorActionPreference = \u0026lsquo;Stop\u0026rsquo;\n# Set location to module home path in artifacts directory try { Set-Location $(Build.SourcesDirectory) Get-ChildItem } catch { Write-Error \u0026ldquo;Failed to set location\u0026rdquo;\n}\n# Get the Module versions Install-Module Configuration -Scope CurrentUser -Force $Modules = Get-ManifestValue -Path .\\dbachecks.psd1 -PropertyName RequiredModules\n$PesterVersion = $Modules.Where{$.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;Pester\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;) $PSFrameworkVersion = $Modules.Where{$.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;PSFramework\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;) $dbatoolsVersion = $Modules.Where{$_.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;dbatools\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;)\n# Install Pester try { Write-Output \u0026ldquo;Installing Pester\u0026rdquo; Install-Module Pester -RequiredVersion $PesterVersion -Scope CurrentUser -Force -SkipPublisherCheck Write-Output \u0026ldquo;Installed Pester\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install Pester $($_)\u0026rdquo; } # Install dbatools try { Write-Output \u0026ldquo;Installing PSFramework\u0026rdquo; Install-Module PSFramework -RequiredVersion $PsFrameworkVersion -Scope CurrentUser -Force Write-Output \u0026ldquo;Installed PSFramework\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install PSFramework $($_)\u0026rdquo; } # Install dbachecks try { Write-Output \u0026ldquo;Installing dbatools\u0026rdquo; Install-Module dbatools -RequiredVersion $dbatoolsVersion -Scope CurrentUser -Force Write-Output \u0026ldquo;Installed dbatools\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install dbatools $($_)\u0026rdquo; }\n# Add current folder to PSModulePath try { Write-Output \u0026ldquo;Adding local folder to PSModulePath\u0026rdquo; $ENV:PSModulePath = $ENV:PSModulePath + \u0026ldquo;;$pwd\u0026rdquo; Write-Output \u0026ldquo;Added local folder to PSModulePath\u0026rdquo; $ENV:PSModulePath.Split(\u0026rsquo;;\u0026rsquo;) } catch { Write-Error \u0026ldquo;Failed to add $pwd to PSModulePAth - $_\u0026rdquo; }\nI use the Configuration module from Joel Bennett¬†to get the required module versions for the required modules and then add the path to¬†$ENV:PSModulePath so that the modules will be imported. I think this is because the modules did not import correctly without it.\nOnce I have the modules I can then run Pester as follows\ntry { Write-Output \u0026ldquo;Installing dbachecks\u0026rdquo; Import-Module .\\dbachecks.psd1 Write-Output \u0026ldquo;Installed dbachecks\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install dbachecks $($_)\u0026rdquo; } $TestResults = Invoke-Pester .\\tests -ExcludeTag Integration,IntegrationTests -Show None -OutputFile $(Build.SourcesDirectory)\\Test-Pester.XML -OutputFormat NUnitXml -PassThru\nif ($TestResults.failedCount -ne 0) { Write-Error \u0026ldquo;Pester returned errors\u0026rdquo; }\nAs you can see I import the dbachecks module from the local folder, run Invoke-Pester and output the results to an XML file and check that there are no failing tests.\nWhether you use the task or PowerShell the next step is to Publish the test results so that they are displayed in the build results in VSTS.\nClick on the + sign next to Phase 1 and search for Publish\nChoose the Publish Test Results task and leave everything as default unless you have renamed the xml file. This means that on the summary page you will see some test results\nand on the tests tab you can see more detailed information and drill down into the tests\nTrigger The next step is to trigger a build when a commit is pushed to the development branch. Click on Triggers and tick enable continuous integration\nSaving the Build Definition I would normally save the build definition regularly and ensure that there is a good message in the comment. I always tell clients that this is like a commit message for your build process so that you can see the history of the changes for the build definition.\nYou can see the history on the edit tab of the build definition\nIf you want to compare or revert the build definition this can be done using the hamburger menu as shown below.\nUpdate the Module Version Now we need to create a build definition for the master branch to update the module version and sign the code ready for publishing to the PowerShell Gallery when we commit or merge to master\nCreate a new build definition as above but this time choose the master branch\nAgain choose an empty process and name it sensibly, click the + sign next to Phase 1 and search for PowerShell\nI change the version to 2 and use this code. Note that the commit message has¬†***NO_CI*** in it. Putting this in a commit message tells VSTS not to trigger a build for this commit.\n$manifest = Import-PowerShellDataFile .\\dbachecks.psd1 [version]$version = $Manifest.ModuleVersion Write-Output \u0026ldquo;Old Version - $Version\u0026rdquo; # Add one to the build of the version number [version]$NewVersion = \u0026ldquo;{0}.{1}.{2}\u0026rdquo; -f $Version.Major, $Version.Minor, ($Version.Build + 1) Write-Output \u0026ldquo;New Version - $NewVersion\u0026rdquo; # Update the manifest file try { Write-Output \u0026ldquo;Updating the Module Version to $NewVersion\u0026rdquo; $path = \u0026ldquo;$pwd\\dbachecks.psd1\u0026rdquo; (Get-Content .\\dbachecks.psd1) -replace $version, $NewVersion | Set-Content .\\dbachecks.psd1 -Encoding string Write-Output \u0026ldquo;Updated the Module Version to $NewVersion\u0026rdquo; } catch { Write-Error \u0026ldquo;Failed to update the Module Version - $_\u0026rdquo; }\ntry { Write-Output \u0026ldquo;Updating GitHub\u0026rdquo; git config user.email \u0026ldquo;mrrobsewell@outlook.com\u0026rdquo; git config user.name \u0026ldquo;SQLDBAWithABeard\u0026rdquo; git add .\\dbachecks.psd1 git commit -m \u0026ldquo;Updated Version Number to $NewVersion ***NO_CI***\u0026rdquo;\ngit push https://$(RobsGitHubPAT)@github.com/sqlcollaborative/dbachecks.git HEAD:master Write-Output \u0026ldquo;Updated GitHub \u0026quot;\n} catch { $_ | Fl -Force Write-Output \u0026ldquo;Failed to update GitHub\u0026rdquo; }\nI use Get-Content Set-Content as I had errors with the Update-ModuleManifest but Adam Murray g | t uses this code to update the version using the BuildID from VSTS\n$newVersion = New-Object version -ArgumentList 1, 0, 0, $env:BUILD_BUILDID $Public = @(Get-ChildItem -Path $ModulePath\\Public\\*.ps1) $Functions = $public.basename Update-ModuleManifest -Path $ModulePath\\$ModuleName.psd1 -ModuleVersion $newVersion -FunctionsToExport $Functions\nYou can commit your change by adding your PAT token as a variable under the variables tab. Don‚Äôt forget to tick the padlock to make it a secret so it is not displayed in the logs\nSign the code with a certificate The SQL Collaborative uses a code signing certificate from DigiCert¬†who allow MVPs to use one for free to sign their code for open source projects, Thank You. We had to upload the certificate to the secure files store in the VSTS library. Click on library, secure files and the blue +Secure File button\nYou also need to add the password as a variable under the variables tab as above. Again don‚Äôt forget to tick the padlock to make it a secret so it is not displayed in the logs\nThen you need to add a task to download the secure file. Click on the + sign next to Phase 1 and search for secure\nchoose the file from the drop down\nNext we need to import the certificate and sign the code. I use a PowerShell task for this with the following code\n$ErrorActionPreference = \u0026lsquo;Stop\u0026rsquo; # read in the certificate from a pre-existing PFX file # I have checked this with @IISResetMe and this does not go in the store only memory $cert = [System.Security.Cryptography.X509Certificates.X509Certificate2]::new(\u0026quot;$(Agent.WorkFolder)\\_temp\\dbatools-code-signing-cert.pfx\u0026rdquo;,\u0026quot;$(CertPassword)\u0026quot;)\ntry { Write-Output \u0026ldquo;Signing Files\u0026rdquo; # find all scripts in your module\u0026hellip; Get-ChildItem -Filter *.ps1 -Include *.ps1 -Recurse -ErrorAction SilentlyContinue | # \u0026hellip;that do not have a signature yet\u0026hellip; Where-Object { ($_ | Get-AuthenticodeSignature).Status -eq \u0026lsquo;NotSigned\u0026rsquo; } | # and apply one # (note that we added -WhatIf so no signing occurs. Remove this only if you # really want to add digital signatures!) Set-AuthenticodeSignature -Certificate $cert Write-Output \u0026ldquo;Signed Files\u0026rdquo; } catch { $_ | Format-List -Force Write-Error \u0026ldquo;Failed to sign scripts\u0026rdquo; }\nwhich will import the certificate into memory and sign all of the scripts in the module folder.\nPublish your artifact The last step of the master branch build publishes the artifact (your signed module) to VSTS ready for the release task. Again, click the + sign next to Phase one and choose the Publish Artifact task not the deprecated copy and publish artifact task and give the artifact a useful name\nDon‚Äôt forget to set the trigger for the master build as well following the same steps as the development build above\nPublish to the PowerShell Gallery Next we create a release to trigger when there is an artifact ready and publish to the PowerShell Gallery.\nClick the Releases tab and New Definition\nChoose an empty process and name the release definition appropriately\nNow click on the artifact and choose the master build definition. If you have not run a build you will get an error like below but dont worry click add.\nClick on the lightning bolt next to the artifact to open the continuous deployment trigger\nand turn on Continuous Deployment so that when an artifact has been created with an updated module version and signed code it is published to the gallery\nNext, click on the environment and name it appropriately and then click on the + sign next to Agent Phase and choose a PowerShell step\nYou may wonder why I dont choose the PowerShell Gallery Packager task. There are two reasons. First I need to install the required modules for dbachecks (dbatools, PSFramework, Pester) prior to publishing and second it appears that the API Key is stored in plain text\nI save my API key for the PowerShell Gallery as a variable again making sure to tick the padlock to make it a secret\nand then use the following code to install the required modules and publish the module to the gallery\nInstall-Module dbatools -Scope CurrentUser -Force Install-Module Pester -Scope CurrentUser -SkipPublisherCheck -Force Install-Module PSFramework -Scope CurrentUser -Force\nPublish-Module -Path \u0026ldquo;$(System.DefaultWorkingDirectory)/Master - Version Update, Signing and Publish Artifact/dbachecks\u0026rdquo; -NuGetApiKey \u0026ldquo;$(GalleryApiKey)\u0026rdquo;\nThats it üôÇ\nNow we have a process that will automatically run our Pester tests when we commit or merge to the development branch and then update our module version number and sign our code and publish to the PowerShell Gallery when we commit or merge to the master branch\nAdded Extra ‚Äì Dashboard I like to create dashboards in VSTS to show the progress of the various definitions. You can do this under the dashboard tab. Click edit and choose or search for widgets and add them to the dashboard\nAdded Extra ‚Äì Badges You can also enable badges for displaying on your readme in GitHub (or VSTS). For the build defintions this is under the options tab.\nfor the release definitions, click the environment and then options and integrations\nYou can then copy the URL and use it in your readme like this on dbachecks\nThe SQL Collaborative has joined the preview of enabling public access to VSTS projects as detailed in this blog post¬†So you can see the dbachecks build and release without the need to log in and soon the dbatools process as well\nI hope you found this useful and if you have any questions or comments please feel free to contact me\nHappy Automating!\n","date":"2018-05-01T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/05/32-Dashboard.png","permalink":"https://blog.robsewell.com/blog/version-update-code-signing-and-publishing-to-the-powershell-gallery-with-vsts/","title":"Version Update, Code Signing and publishing to the PowerShell Gallery with VSTS"},{"content":"PASS Summit is the largest conference for technical professionals who leverage the Microsoft Data Platform.¬†PASS Summit 2018 is happening November 5th¬†‚Äì 9th¬†2018 in Seattle. It is an amazing conference. I attended last year and thoroughly enjoyed every minute. It is a whole week of opportunities to learn from and network with people from all over the world involved in Data Platform.\nThe pre-cons have just been announced so go and take a look at the¬†Pre-Con Page¬†to see the wonderful full day learning opportunities you can get this year.\nI am immensely honoured to say that on Tuesday 6th¬†November you can join me for a whole day of PowerShell learning¬†I will pass on as much of the skills and knowledge I have learnt using PowerShell with SQL Server (and other technologies) that I can fit in one day. I want you to leave feeling more confident in using PowerShell to automate away all of the mundane. In particular I want to enable you to have the skills to write professional PowerShell solutions.\nYou can read more and sign up here\nIf¬†you have¬†any questions about this session please feel free to contact me. You can use any of the various social media sites, or via the contact page or in person if you see me.\n","date":"2018-04-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/professional-and-proficient-powershell-from-writing-scripts-to-developing-solutions-at-pass-summit/","title":"Professional and Proficient PowerShell: From Writing Scripts to Developing Solutions at PASS Summit"},{"content":"It‚Äôs been 45 days since we released dbachecks\nSince then there have been 25 releases to the PowerShell Gallery!! Today release 1.1.119 was released üôÇ There have been over 2000 downloads of the module already.\nIn the beginning we had 80 checks and 108 configuration items, today we have 84 checks and 125 configuration items!\nIf you have already installed dbachecks it is important to make sure that you update regularly. You can do this by running\nUpdate-Module dbachecks\nIf you want to try dbachecks, you can install it from the PowerShell Gallery¬†by running\nInstall-Module dbachecks # -Scope CurrentUser # if not running as admin\nYou can read more about installation and read a number of blog posts about using different parts of dbachecks at this link¬†https://dbatools.io/installing-dbachecks/\nHADR Tests Today we updated the HADR tests to add the capability to test multiple availability groups and fix a couple of bugs\nOnce you have installed dbachecks you will need to set some configuration so that you can perform the tests. You can see all of the configuration items and their values using\nGet-DbcConfig | Out-GridView\nYou can set the values with the Set-DbcConfig command. It has intellisense to make things easier üôÇ To set the values for the HADR tests\nSet-DbcConfig -Name app.cluster -Value sql1 Set-DbcConfig -Name app.computername -Value sql0,sql1 Set-DbcConfig -Name app.sqlinstance -Value sql0,sql1 Set-DbcConfig -Name domain.name -Value TheBeard.Local Set-DbcConfig -Name skip.hadr.listener.pingcheck -Value $true\napp.cluster requires one of the nodes of the cluster. app.computername requires the windows computer names of the machines to run operating system checks against app.sqlinstance requires the instance names of the SQL instances that you want to run SQL checks against (These are default instances but it will accept SERVER\\INSTANCE) domain.name requires the domain name the machines are part of skip.hadr.listener.pingcheck is a boolean value which defines whether to skip the listener ping check or not. As this is in Azure I am skipping the check by setting the value to $true policy.hadr.tcpport is set to default to 1433 but you can also set this configuration if your SQL is using a different port NOTE ‚Äì You can find all the configuration items that can skip tests by running\nGet-DbcConfig -Name skip*\nNow we have set the configuration (For the HADR checks ‚Äì There are many more configurations for other checks that you can set) you can run the checks with\nInvoke-DbcCheck -Check HADR\nThis runs the following checks\nEach node on the cluster should be up Each resource on the cluster should be online Each SQL instance should be enabled for Always On Connection check for the listener and each node Should be pingable (unless¬†skip.hadr.listener.pingcheck is set to true) Should be able to run SQL commands Should be the correct domain name Should be using the correct tcpport Each replica should not be in unknown state Each synchronous replica should be synchronised Each asynchronous replica should be synchonising Each database should be¬†synchronised (or¬†synchronising) on each replica Each database should be failover ready on each replica Each database should be joined to the availability group on each replica Each database should not be suspended on each replica Each node should have the AlwaysOn_Health extended event Each node should have the AlwaysOn_Health extended event running Each node should have the AlwaysOn_Health extended event set to auto start (Apologies folk over the pond, I use the Queens English üòâ )\nThis is good for us to be able to run this check at the command line but we can do more.\nWe can export the results and display them with PowerBi. Note we need to add -PassThru so that the results go through the pipeline and that I used -Show Fails so that only the titles of the Describe and Context blocks and any failing tests are displayed to the screen\nInvoke-DbcCheck -Check HADR -Show Fails -PassThru | Update-DbcPowerBiDataSource -Environment HADR-Test Start-DbcPowerBi\nThis will create a file at C:\\Windows\\Temp\\dbachecks and open the PowerBi report. You will need to refresh the data in the report and then you will see\nExcellent, everything passed üôÇ\nSaving Configuration for reuse We can save our configuration using Export-DbcConfig which will export the configuration to a json file\nExport-DbcConfig -Path Git:\\PesterTests\\MyHADRTestsForProd.json\nso that we can run this particular set of tests with this comfiguration by importing the configuration using Import-DbcConfig\nImport-DbcConfig -Path¬†-Path Git:\\PesterTests\\MyHADRTestsForProd.json Invoke-DbcCheck -Check HADR\nIn this way you can set up different check configurations for different use cases. This also enables you to make use of the checks in your CI/CD process. For example, I have a GitHub repository for creating a domain, a cluster and a SQL 2017 availability group using VSTS. I have saved a dbachecks configuration to my repository and as part of my build I can import that configuration, run the checks and output them to XML for consumption by the publish test results task of VSTS\nAfter copying the configuration to the machine, I run\nImport-Dbcconfig -Path C:\\Windows\\Temp\\FirstBuild.json Invoke-DbcCheck-AllChecks -OutputFile PesterTestResultsdbachecks.xml -OutputFormat NUnitXml\nin my build step and then use the publish test results task and VSTS does the rest üôÇ\n","date":"2018-04-08T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/04/VSTS-results.png","permalink":"https://blog.robsewell.com/blog/checking-availability-groups-with-dbachecks/","title":"Checking Availability Groups with dbachecks"},{"content":"So I always like to show splatting PowerShell commands when I am presenting sessions or workshops and realised that I had not really blogged about it. (This blog is for @dbafromthecold who asked me to üôÇ )\nWhat is Splatting? Well you will know that when you call a PowerShell function you can use intellisense to get the parameters and sometimes the parameter values as well. This can leave you with a command that looks like this on the screen\nStart-DbaMigration -Source $Source -Destination $Destination -BackupRestore -NetworkShare $Share -WithReplace -ReuseSourceFolderStructure -IncludeSupportDbs -NoAgentServer -NoAudits -NoResourceGovernor -NoSaRename -NoBackupDevices\rIt goes on and on and on and while it is easy to type once, it is not so easy to see which values have been chosen. It is also not so easy to change the values.\nBy Splatting the parameters it makes it much easier to read and also to alter. So instead of the above you can have\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $startDbaMigrationSplat = @{ Source = $Source NetworkShare = $Share NoAgentServer = $true NoResourceGovernor = $true WithReplace = $true ReuseSourceFolderStructure = $true Destination = $Destination NoAudits = $true BackupRestore = $true NoSaRename = $true IncludeSupportDbs = $true NoBackupDevices = $true } Start-DbaMigration @startDbaMigrationSplat This is much easier on the eye, but if you dont know what the parameters are (and are too lazy to use Get-Help ‚Äì Hint You should always use Get-Help ) or like the convenience and efficiency of using the intellisense, this might feel like a backward step that slows your productivity in the cause of easy on the eye code.\nEnter EditorServicesCommandSuite by SeeminglyScience for VS Code. Amongst the things it makes available to you is easy splatting and people are always impressed when I show it\nYou can install it from the PowerShell Gallery like all good modules using\nInstall-Module EditorServicesCommandSuite -Scope CurrentUser\rand then add it to your VSCode PowerShell profile usually found at C:\\Users\\USERNAME\\Documents\\WindowsPowerShell\\Microsoft.VSCode_profile.ps1\n1 2 3 \\# Place this in your VSCode profile Import-Module EditorServicesCommandSuite Import-EditorCommand -Module EditorServicesCommandSuite and now creating a splat is as easy as this.\nWrite the command, leave the cursor on a parameter, hit F1 ‚Äì Choose PowerShell : Show Additional Commands (or use a keyboard shortcut) type splat press enter. Done üôÇ\nSo very easy üôÇ\nHappy Splatting üôÇ\n","date":"2018-03-11T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2022/dbachecks.jpg","permalink":"https://blog.robsewell.com/blog/easily-splatting-powershell-with-vs-code/","title":"Easily Splatting PowerShell with VS Code"},{"content":"For the last couple of months members of the dbatools¬†team have been working on a new PowerShell module called dbachecks. This open source PowerShell module will enable you to validate your SQL Instances. Today it is released for you all to start to use üôÇ\nValidate Your SQL Instances? What do I mean by validate your SQL Instances? You want to know if your SQL Instances are (still) set up in the way that you want them to be or that you have not missed any configurations when setting them up. With dbachecks you can use any or all of the 80 checks to ensure one or many SQL Instances are as you want them to be. Using Pester, dbachecks will validate your SQL Instance(s) against default settings or ones that you configure yourself.\nInstallation Installation is via the PowerShell Gallery. You will need to open PowerShell on a machine connected to the internet and run\nInstall-Module dbachecks\nIf you are not running your process as admin or you only want (or are able) to install for your own user account you will need to\nInstall-Module -Scope CurrentUser\nThis will also install the PSFramework module used for configuration (and other things beneath the hood) and the latest version (4.2.0 ‚Äì released on Sunday!) of Pester\nOnce you have installed the module you can see the commands available by running\nGet-Command -Module dbachecks\nTo be able to use these (and any PowerShell) commands, your first step should always be Get-Help\nGet-Help Send-DbcMailMessage\n80 Checks At the time of release, dbachecks has 80 checks. You can see all of the checks by running\nGet-DbcCheck\n(Note this has nothing to do with DBCC CheckDb!) Here is the output of\nGet-DbcCheck | Select Group, UniqueTag\nso you can see the current checks\nGroup UniqueTag Agent AgentServiceAccount Agent DbaOperator Agent FailsafeOperator Agent DatabaseMailProfile Agent FailedJob Database DatabaseCollation Database SuspectPage Database TestLastBackup Database TestLastBackupVerifyOnly Database ValidDatabaseOwner Database InvalidDatabaseOwner Database LastGoodCheckDb Database IdentityUsage Database RecoveryModel Database DuplicateIndex Database UnusedIndex Database DisabledIndex Database DatabaseGrowthEvent Database PageVerify Database AutoClose Database AutoShrink Database LastFullBackup Database LastDiffBackup Database LastLogBackup Database VirtualLogFile Database LogfileCount Database LogfileSize Database FileGroupBalanced Database AutoCreateStatistics Database AutoUpdateStatistics Database AutoUpdateStatisticsAsynchronously Database DatafileAutoGrowthType Database Trustworthy Database OrphanedUser Database PseudoSimple Database AdHocWorkloads Domain DomainName Domain OrganizationalUnit HADR ClusterHealth HADR ClusterServerHealth HADR HADR System.Object[] Instance SqlEngineServiceAccount Instance SqlBrowserServiceAccount Instance TempDbConfiguration Instance AdHocWorkload Instance BackupPathAccess Instance DAC Instance NetworkLatency Instance LinkedServerConnection Instance MaxMemory Instance OrphanedFile Instance ServerNameMatch Instance MemoryDump Instance SupportedBuild Instance SaRenamed Instance DefaultBackupCompression Instance XESessionStopped Instance XESessionRunning Instance XESessionRunningAllowed Instance OLEAutomation Instance WhoIsActiveInstalled LogShipping LogShippingPrimary LogShipping LogShippingSecondary Server PowerPlan Server InstanceConnection Server SPN Server DiskCapacity Server PingComputer MaintenancePlan SystemFull MaintenancePlan UserFull MaintenancePlan UserDiff MaintenancePlan UserLog MaintenancePlan CommandLog MaintenancePlan SystemIntegrityCheck MaintenancePlan UserIntegrityCheck MaintenancePlan UserIndexOptimize MaintenancePlan OutputFileCleanup MaintenancePlan DeleteBackupHistory MaintenancePlan PurgeJobHistory 108 Configurations One of the things I have been talking about in my presentation ‚ÄúGreen is Good Red is Bad‚Äù is configuring Pester checks so that you do not have to keep writing new tests for the same thing but with different values.\nFor example, a different user for a database owner. The code to write the test for the database owner is the same but the value might be different for different applications, environments, clients, teams, domains etc. I gave a couple of different methods for achieving this.\nWith dbachecks we have made this much simpler enabling you to set configuration items at run-time or for your session and enabling you to export and import them so you can create different configs for different use cases\nThere are 108 configuration items at present. You can see the current configuration by running\nGet-DbcConfig\nwhich will show you the name of the config, the value it is currently set and the description\nYou can see all of the configs and their descriptions here\nName Description agent.databasemailprofile Name of the Database Mail Profile in SQL Agent agent.dbaoperatoremail Email address of the DBA Operator in SQL Agent agent.dbaoperatorname Name of the DBA Operator in SQL Agent agent.failsafeoperator Email address of the DBA Operator in SQL Agent app.checkrepos Where Pester tests/checks are stored app.computername List of Windows Servers that Windows-based tests will run against app.localapp Persisted files live here app.maildirectory Files for mail are stored here app.sqlcredential The universal SQL credential if Trusted/Windows Authentication is not used app.sqlinstance List of SQL Server instances that SQL-based tests will run against app.wincredential The universal Windows if default Windows Authentication is not used command.invokedbccheck.excludecheck Invoke-DbcCheck: The checks that should be skipped by default. domain.domaincontroller The domain controller to process your requests domain.name The Active Directory domain that your server is a part of domain.organizationalunit The OU that your server should be a part of mail.failurethreshhold Number of errors that must be present to generate an email report mail.from Email address the email reports should come from mail.smtpserver Store the name of the smtp server to send email reports mail.subject Subject line of the email report mail.to Email address to send the report to policy.backup.datadir Destination server data directory policy.backup.defaultbackupcompreesion Default Backup Compression check should be enabled $true or disabled $false policy.backup.diffmaxhours Maxmimum number of hours before Diff Backups are considered outdated policy.backup.fullmaxdays Maxmimum number of days before Full Backups are considered outdated policy.backup.logdir Destination server log directory policy.backup.logmaxminutes Maxmimum number of minutes before Log Backups are considered outdated policy.backup.newdbgraceperiod The number of hours a newly created database is allowed to not have backups policy.backup.testserver Destination server for backuptests policy.build.warningwindow The number of months prior to a build being unsupported that you want warning about policy.connection.authscheme Auth requirement (Kerberos, NTLM, etc) policy.connection.pingcount Number of times to ping a server to establish average response time policy.connection.pingmaxms Maximum response time in ms policy.dacallowed DAC should be allowed $true or disallowed $false policy.database.autoclose Auto Close should be allowed $true or dissalowed $false policy.database.autocreatestatistics Auto Create Statistics should be enabled $true or disabled $false policy.database.autoshrink Auto Shrink should be allowed $true or dissalowed $false policy.database.autoupdatestatistics Auto Update Statistics should be enabled $true or disabled $false policy.database.autoupdatestatisticsasynchronously Auto Update Statistics Asynchronously should be enabled $true or disabled $false policy.database.filebalancetolerance Percentage for Tolerance for checking for balanced files in a filegroups policy.database.filegrowthexcludedb Databases to exclude from the file growth check policy.database.filegrowthtype Growth Type should be \u0026lsquo;kb\u0026rsquo; or \u0026lsquo;percent\u0026rsquo; policy.database.filegrowthvalue The auto growth value (in kb) should be equal or higher than this value. Example: A value of 65535 means at least 64MB. policy.database.logfilecount The number of Log files expected on a database policy.database.logfilesizecomparison How to compare data and log file size, options are maximum or average policy.database.logfilesizepercentage Maximum percentage of Data file Size that logfile is allowed to be. policy.database.maxvlf Max virtual log files policy.dbcc.maxdays Maxmimum number of days before DBCC CHECKDB is considered outdated policy.diskspace.percentfree Percent disk free policy.dump.maxcount Maximum number of expected dumps policy.hadr.tcpport The TCPPort for the HADR check policy.identity.usagepercent Maxmimum percentage of max of identity column policy.invaliddbowner.excludedb Databases to exclude from invalid dbowner checks policy.invaliddbowner.name The database owner account should not be this user policy.network.latencymaxms Max network latency average policy.ola.commandlogenabled Ola\u0026rsquo;s CommandLog Cleanup should be enabled $true or disabled $false policy.ola.commandlogscheduled Ola\u0026rsquo;s CommandLog Cleanup should be scheduled $true or disabled $false policy.ola.database The database where Ola\u0026rsquo;s maintenance solution is installed policy.ola.deletebackuphistoryenabled Ola\u0026rsquo;s Delete Backup History should be enabled $true or disabled $false policy.ola.deletebackuphistoryscheduled Ola\u0026rsquo;s Delete Backup History should be scheduled $true or disabled $false policy.ola.installed Checks to see if Ola Hallengren solution is installed policy.ola.outputfilecleanupenabled Ola\u0026rsquo;s Output File Cleanup should be enabled $true or disabled $false policy.ola.outputfilecleanupscheduled Ola\u0026rsquo;s Output File Cleanup should be scheduled $true or disabled $false policy.ola.purgejobhistoryenabled Ola\u0026rsquo;s Purge Job History should be enabled $true or disabled $false policy.ola.purgejobhistoryscheduled Ola\u0026rsquo;s Purge Job History should be scheduled $true or disabled $false policy.ola.systemfullenabled Ola\u0026rsquo;s Full System Database Backup should be enabled $true or disabled $false policy.ola.systemfullretention Ola\u0026rsquo;s Full System Database Backup retention number of hours policy.ola.systemfullscheduled Ola\u0026rsquo;s Full System Database Backup should be scheduled $true or disabled $false policy.ola.systemintegritycheckenabled Ola\u0026rsquo;s System Database Integrity should be enabled $true or disabled $false policy.ola.systemintegritycheckscheduled Ola\u0026rsquo;s System Database Integrity should be scheduled $true or disabled $false policy.ola.userdiffenabled Ola\u0026rsquo;s Diff User Database Backup should be enabled $true or disabled $false policy.ola.userdiffretention Ola\u0026rsquo;s Diff User Database Backup retention number of hours policy.ola.userdiffscheduled Ola\u0026rsquo;s Diff User Database Backup should be scheduled $true or disabled $false policy.ola.userfullenabled Ola\u0026rsquo;s Full User Database Backup should be enabled $true or disabled $false policy.ola.userfullretention Ola\u0026rsquo;s Full User Database Backup retention number of hours policy.ola.userfullscheduled Ola\u0026rsquo;s Full User Database Backup should be scheduled $true or disabled $false policy.ola.userindexoptimizeenabled Ola\u0026rsquo;s User Index Optimization should be enabled $true or disabled $false policy.ola.userindexoptimizescheduled Ola\u0026rsquo;s User Index Optimization should be scheduled $true or disabled $false policy.ola.userintegritycheckenabled Ola\u0026rsquo;s User Database Integrity should be enabled $true or disabled $false policy.ola.userintegritycheckscheduled Ola\u0026rsquo;s User Database Integrity should be scheduled $true or disabled $false policy.ola.userlogenabled Ola\u0026rsquo;s Log User Database Backup should be enabled $true or disabled $false policy.ola.userlogretention Ola\u0026rsquo;s Log User Database Backup retention number of hours policy.ola.userlogscheduled Ola\u0026rsquo;s Log User Database Backup should be scheduled $true or disabled $false policy.oleautomation OLE Automation should be enabled $true or disabled $false policy.pageverify Page verify option should be set to this value policy.recoverymodel.excludedb Databases to exclude from standard recovery model check policy.recoverymodel.type Standard recovery model policy.storage.backuppath Enables tests to check if servers have access to centralized backup location policy.validdbowner.excludedb Databases to exclude from valid dbowner checks policy.validdbowner.name The database owner account should be this user policy.whoisactive.database Which database should contain the sp_WhoIsActive stored procedure policy.xevent.requiredrunningsession List of XE Sessions that should be running. policy.xevent.requiredstoppedsession List of XE Sessions that should not be running. policy.xevent.validrunningsession List of XE Sessions that can be be running. skip.backup.testing Don\u0026rsquo;t run Test-DbaLastBackup by default (it\u0026rsquo;s not read-only) skip.connection.ping Skip the ping check for connectivity skip.connection.remoting Skip PowerShell remoting check for connectivity skip.database.filegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.database.logfilecounttest Skip the logfilecount test skip.datafilegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.dbcc.datapuritycheck Skip data purity check in last good dbcc command skip.diffbackuptest Skip the Differential backup test skip.logfilecounttest Skip the logfilecount test skip.logshiptesting Skip the logshipping test skip.tempdb1118 Don\u0026rsquo;t run test for Trace Flag 1118 skip.tempdbfilecount Don\u0026rsquo;t run test for Temp Database File Count skip.tempdbfilegrowthpercent Don\u0026rsquo;t run test for Temp Database File Growth in Percent skip.tempdbfilesizemax Don\u0026rsquo;t run test for Temp Database Files Max Size skip.tempdbfilesonc Don\u0026rsquo;t run test for Temp Database Files on C Running A Check You can quickly run a single check by calling Invoke-DbcCheck.\nInvoke-DbcCheck -SqlInstance localhost -Check FailedJob\nExcellent, my agent jobs have not failed üôÇ\nInvoke-DbcCheck -SqlInstance localhost -Check LastGoodCheckDb\nThats good, all of my databases have had a successful DBCC CHECKDB within the last 7 days.\nSetting a Configuration To save me from having to specify the instance I want to run my tests against I can set the¬†app.sqlinstance config to the instances I want to check.\nSet-DbcConfig -Name app.sqlinstance -Value localhost, \u0026rsquo;localhost\\PROD1\u0026rsquo;\nThen whenever I call Invoke-DbcCheck it will run against those instances for the SQL checks\nSo now if I run\nInvoke-DbcCheck -Check LastDiffBackup\nI can see that I dont have a diff backup for the databases on both instances. Better stop writing this and deal with that !!\nThe configurations are stored in the registry but you can export them and then import them for re-use easily. I have written another blog post about that.\nThe Show Parameter Getting the results of the tests on the screen is cool but if you are running a lot of tests against a lot of instances then you might find that you have 3 failed tests out of 15000! This will mean a lot of scrolling through green text looking for the red text and you may find that your PowerShell buffer doesnt hold all of your test results leaving you very frustrated.\ndbachecks supports the Pester Show parameter enabling you to filter the output of the results to the screen. The available values are Summary, None, Fails, Inconclusive, Passed, Pending and Skipped\nin my opinion by far the most useful one is Fails as this will show you only the failed tests with the context to enable you to see which tests have failed\nInvoke-DbcCheck -Check Agent -Show Fails\nIf we check all of the checks tagged as Agent we can easily see that most passed but The Job That Fails (surprisingly) failed. All of the other tests that were run for the agent service, operators, failsafe operator, database mail and all other agent jobs all passed in the example below\nTest Results are for other People as well It is all very well and good being able to run tests and get the results on our screen. It will be very useful for people to be able to validate a new SQL instance for example or run a morning check or the first step of an incident response. But test results are also useful for other people so we need to be able to share them\nWe have created a Power Bi Dashboard that comes with the dbachecks module to enable easy sharing of the test results. You can also send the results via email using¬†Send-DbcMailMessage. we have an open issue for putting them into a database that we would love you to help resolve.\nTo get the results into PowerBi you can run\nInvoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Production\nThis will run all of the dbachecks using your configuration for your Production environment, output only the failed tests to the screen and save the results in your windows\\temp\\dbachecks folder with a suffix of Production\nIf you then used a different configuration for your development environment and ran\nInvoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Development\nit will run all of the dbachecks using your configuration for your Development environment, output only the failed tests to the screen and save the results in your windows\\temp\\dbachecks folder with a suffix of Development and you would end up with two files in the folder\nYou can then simply run\nStart-DbcPowerBi\nand as long as you have the (free) Powerbi Desktop then you will see this. You will need to refresh the data to get your test results\nOf course it is Powerbi so you can publish this report. Here it is so that you can click around and see what it looks like\nIt‚Äôs Open Source ‚Äì We Want Your Ideas, Issues, New Code dbachecks is open-source¬†available on GitHub for anyone to contribute\nWe would love you to contribute. Please open issues for new tests, enhancements, bugs. Please fork the repository and add code to improve the module. please give feedback to make this module even more useful\nYou can also come in the SQL Server Community Slack and join the dbachecks channel and get advice, make comments or just join in the conversation\nFurther Reading There are many more introduction blog posts covering different areas at\ndbachecks.io/install Thank You I want to say thank you to all of the people who have enabled dbachecks to get this far. These wonderful people have used their own time to ensure that you have a useful tool available to you for free\nChrissy Lemaire @cl\nFred¬†Weinmann @FredWeinmann\nCl√°udio Silva @ClaudioESSilva\nStuart Moore @napalmgram\nShawn Melton @wsmelton\nGarry Bargsley @gbargsley\nStephen Bennett¬†@staggerlee011\nSander Stad @SQLStad\nJess Pomfret @jpomfret\nJason Squires @js0505\nShane O‚ÄôNeill @SOZDBA\nTony Wilhelm @TonyWSQL\nand all of the other people who have contributed in the dbachecks Slack channel\n","date":"2018-02-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/02/09-PowerBi.png","permalink":"https://blog.robsewell.com/blog/announcing-dbachecks-configurable-powershell-validation-for-your-sql-instances/","title":"Announcing dbachecks ‚Äì Configurable PowerShell Validation For Your SQL Instances"},{"content":"Today is the day that we have announced dbachecks¬†a PowerShell module enabling you to validate your SQL Instances. You can read more about it here where you can learn how to install it and see some simple use cases\n108 Configurations One of the things I have been talking about in my presentation ‚ÄúGreen is Good Red is Bad‚Äù is configuring Pester checks so that you do not have to keep writing new tests for the same thing but with different values.\nFor example, a different user for a database owner. The code to write the test for the database owner is the same but the value might be different for different applications, environments, clients, teams, domains etc. I gave a couple of different methods for achieving this.\nWith dbachecks we have made this much simpler enabling you to set configuration items at run-time or for your session and enabling you to export and import them so you can create different configs for different use cases\nThere are 108 configuration items at present. You can see the current configuration by running\nGet-DbcConfig\nwhich will show you the name of the config, the value it is currently set and the description You can see all of the configs and their descriptions here\nName Description agent.databasemailprofile Name of the Database Mail Profile in SQL Agent agent.dbaoperatoremail Email address of the DBA Operator in SQL Agent agent.dbaoperatorname Name of the DBA Operator in SQL Agent agent.failsafeoperator Email address of the DBA Operator in SQL Agent app.checkrepos Where Pester tests/checks are stored app.computername List of Windows Servers that Windows-based tests will run against app.localapp Persisted files live here app.maildirectory Files for mail are stored here app.sqlcredential The universal SQL credential if Trusted/Windows Authentication is not used app.sqlinstance List of SQL Server instances that SQL-based tests will run against app.wincredential The universal Windows if default Windows Authentication is not used command.invokedbccheck.excludecheck Invoke-DbcCheck: The checks that should be skipped by default. domain.domaincontroller The domain controller to process your requests domain.name The Active Directory domain that your server is a part of domain.organizationalunit The OU that your server should be a part of mail.failurethreshhold Number of errors that must be present to generate an email report mail.from Email address the email reports should come from mail.smtpserver Store the name of the smtp server to send email reports mail.subject Subject line of the email report mail.to Email address to send the report to policy.backup.datadir Destination server data directory policy.backup.defaultbackupcompreesion Default Backup Compression check should be enabled $true or disabled $false policy.backup.diffmaxhours Maxmimum number of hours before Diff Backups are considered outdated policy.backup.fullmaxdays Maxmimum number of days before Full Backups are considered outdated policy.backup.logdir Destination server log directory policy.backup.logmaxminutes Maxmimum number of minutes before Log Backups are considered outdated policy.backup.newdbgraceperiod The number of hours a newly created database is allowed to not have backups policy.backup.testserver Destination server for backuptests policy.build.warningwindow The number of months prior to a build being unsupported that you want warning about policy.connection.authscheme Auth requirement (Kerberos, NTLM, etc) policy.connection.pingcount Number of times to ping a server to establish average response time policy.connection.pingmaxms Maximum response time in ms policy.dacallowed DAC should be allowed $true or disallowed $false policy.database.autoclose Auto Close should be allowed $true or dissalowed $false policy.database.autocreatestatistics Auto Create Statistics should be enabled $true or disabled $false policy.database.autoshrink Auto Shrink should be allowed $true or dissalowed $false policy.database.autoupdatestatistics Auto Update Statistics should be enabled $true or disabled $false policy.database.autoupdatestatisticsasynchronously Auto Update Statistics Asynchronously should be enabled $true or disabled $false policy.database.filebalancetolerance Percentage for Tolerance for checking for balanced files in a filegroups policy.database.filegrowthexcludedb Databases to exclude from the file growth check policy.database.filegrowthtype Growth Type should be \u0026lsquo;kb\u0026rsquo; or \u0026lsquo;percent\u0026rsquo; policy.database.filegrowthvalue The auto growth value (in kb) should be equal or higher than this value. Example: A value of 65535 means at least 64MB. policy.database.logfilecount The number of Log files expected on a database policy.database.logfilesizecomparison How to compare data and log file size, options are maximum or average policy.database.logfilesizepercentage Maximum percentage of Data file Size that logfile is allowed to be. policy.database.maxvlf Max virtual log files policy.dbcc.maxdays Maxmimum number of days before DBCC CHECKDB is considered outdated policy.diskspace.percentfree Percent disk free policy.dump.maxcount Maximum number of expected dumps policy.hadr.tcpport The TCPPort for the HADR check policy.identity.usagepercent Maxmimum percentage of max of identity column policy.invaliddbowner.excludedb Databases to exclude from invalid dbowner checks policy.invaliddbowner.name The database owner account should not be this user policy.network.latencymaxms Max network latency average policy.ola.commandlogenabled Ola\u0026rsquo;s CommandLog Cleanup should be enabled $true or disabled $false policy.ola.commandlogscheduled Ola\u0026rsquo;s CommandLog Cleanup should be scheduled $true or disabled $false policy.ola.database The database where Ola\u0026rsquo;s maintenance solution is installed policy.ola.deletebackuphistoryenabled Ola\u0026rsquo;s Delete Backup History should be enabled $true or disabled $false policy.ola.deletebackuphistoryscheduled Ola\u0026rsquo;s Delete Backup History should be scheduled $true or disabled $false policy.ola.installed Checks to see if Ola Hallengren solution is installed policy.ola.outputfilecleanupenabled Ola\u0026rsquo;s Output File Cleanup should be enabled $true or disabled $false policy.ola.outputfilecleanupscheduled Ola\u0026rsquo;s Output File Cleanup should be scheduled $true or disabled $false policy.ola.purgejobhistoryenabled Ola\u0026rsquo;s Purge Job History should be enabled $true or disabled $false policy.ola.purgejobhistoryscheduled Ola\u0026rsquo;s Purge Job History should be scheduled $true or disabled $false policy.ola.systemfullenabled Ola\u0026rsquo;s Full System Database Backup should be enabled $true or disabled $false policy.ola.systemfullretention Ola\u0026rsquo;s Full System Database Backup retention number of hours policy.ola.systemfullscheduled Ola\u0026rsquo;s Full System Database Backup should be scheduled $true or disabled $false policy.ola.systemintegritycheckenabled Ola\u0026rsquo;s System Database Integrity should be enabled $true or disabled $false policy.ola.systemintegritycheckscheduled Ola\u0026rsquo;s System Database Integrity should be scheduled $true or disabled $false policy.ola.userdiffenabled Ola\u0026rsquo;s Diff User Database Backup should be enabled $true or disabled $false policy.ola.userdiffretention Ola\u0026rsquo;s Diff User Database Backup retention number of hours policy.ola.userdiffscheduled Ola\u0026rsquo;s Diff User Database Backup should be scheduled $true or disabled $false policy.ola.userfullenabled Ola\u0026rsquo;s Full User Database Backup should be enabled $true or disabled $false policy.ola.userfullretention Ola\u0026rsquo;s Full User Database Backup retention number of hours policy.ola.userfullscheduled Ola\u0026rsquo;s Full User Database Backup should be scheduled $true or disabled $false policy.ola.userindexoptimizeenabled Ola\u0026rsquo;s User Index Optimization should be enabled $true or disabled $false policy.ola.userindexoptimizescheduled Ola\u0026rsquo;s User Index Optimization should be scheduled $true or disabled $false policy.ola.userintegritycheckenabled Ola\u0026rsquo;s User Database Integrity should be enabled $true or disabled $false policy.ola.userintegritycheckscheduled Ola\u0026rsquo;s User Database Integrity should be scheduled $true or disabled $false policy.ola.userlogenabled Ola\u0026rsquo;s Log User Database Backup should be enabled $true or disabled $false policy.ola.userlogretention Ola\u0026rsquo;s Log User Database Backup retention number of hours policy.ola.userlogscheduled Ola\u0026rsquo;s Log User Database Backup should be scheduled $true or disabled $false policy.oleautomation OLE Automation should be enabled $true or disabled $false policy.pageverify Page verify option should be set to this value policy.recoverymodel.excludedb Databases to exclude from standard recovery model check policy.recoverymodel.type Standard recovery model policy.storage.backuppath Enables tests to check if servers have access to centralized backup location policy.validdbowner.excludedb Databases to exclude from valid dbowner checks policy.validdbowner.name The database owner account should be this user policy.whoisactive.database Which database should contain the sp_WhoIsActive stored procedure policy.xevent.requiredrunningsession List of XE Sessions that should be running. policy.xevent.requiredstoppedsession List of XE Sessions that should not be running. policy.xevent.validrunningsession List of XE Sessions that can be be running. skip.backup.testing Don\u0026rsquo;t run Test-DbaLastBackup by default (it\u0026rsquo;s not read-only) skip.connection.ping Skip the ping check for connectivity skip.connection.remoting Skip PowerShell remoting check for connectivity skip.database.filegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.database.logfilecounttest Skip the logfilecount test skip.datafilegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.dbcc.datapuritycheck Skip data purity check in last good dbcc command skip.diffbackuptest Skip the Differential backup test skip.logfilecounttest Skip the logfilecount test skip.logshiptesting Skip the logshipping test skip.tempdb1118 Don\u0026rsquo;t run test for Trace Flag 1118 skip.tempdbfilecount Don\u0026rsquo;t run test for Temp Database File Count skip.tempdbfilegrowthpercent Don\u0026rsquo;t run test for Temp Database File Growth in Percent skip.tempdbfilesizemax Don\u0026rsquo;t run test for Temp Database Files Max Size skip.tempdbfilesonc Don\u0026rsquo;t run test for Temp Database Files on C So there are a lot of configurations that you can use. A lot are already set by default but all of them you can configure for the values that you need for your own estate.\nThe configurations are stored in the registry at¬†HKCU:\\Software\\Microsoft\\WindowsPowerShell\\PSFramework\\\nFirst Configurations First I would run this so that you can see all of the configs in a seperate window (note this does not work on PowerShell v6)\nGet-DbcConfig | Out-GridView\rLets start with the first configurations that you will want to set. This should be the Instances and the Hosts that you want to check\nYou can get the value of the configuration item using\nGet-DbcConfigValue -Name app.sqlinstance\ras you can see in the image, nothing is returned so we have no instances configured at present. We have added tab completion to the name parameter so that you can easily find the right one\nIf you want to look at more information about the configuration item you can use\nGet-DbcConfig -Name app.sqlinstance\rwhich shows you the name, current value and the description\nSo lets set our first configuration for our SQL instance to localhost. I have included a video so you can see the auto-complete in action as well\nSet-DbcConfig -Name app.sqlinstance localhost\rThis configuration will be used for any SQL based checks but not for any windows based ones like Services, PowerPlan, SPN, DiskSpace, Cluster so lets set the app.computername configuration as well\nThis means that when we run invoke-DbcCheck with AllChecks or by specifying a check, it will run against the local machine and default instance unless we specify a sqlinstance when calling Invoke-DbcCheck. So the code below will not use the configuration for app.sqlinstance.\nInvoke-DbcCheck -SqlInstance TheBeard\rExclude a Check You can exclude a check using the -ExcludeCheck parameter of Invoke-DbcConfig. In the example below I am running all of the Server checks but excluding the SPN as we are not on a domain\nInvoke-DbcCheck -Check Server -ExcludeCheck SPN\rThere is a configuration setting to exclude checks as well. (Be careful this will exclude them even if you specifically specify a check using Invoke-DbcCheck but we do give you a warning!)\nSo now I can run\nSet-DbcConfig -Name command.invokedbccheck.excludecheck -Value SPN\rInvoke-DbcCheck -Check Server\rand all of the server checks except the SPN check will run against the local machine and the default instance that I have set in the config\nCreating an environment config and exporting it to use any time we like So lets make this a lot more useful. Lets create a configuration for our production environment and save it to disk (or even source control it!) so that we can use it again and again. We can also then pass it to other members of our team or even embed it in an automated process or our CI/CD system\nLets build up a configuration for a number of tests for my ‚Äúproduction‚Äù environment. I will not explain them all here but let you read through the code and the comments to see what has been set. You will see that some of them are due to me running the test on a single machine with one drive.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # The computername we will be testing Set-DbcConfig -Name app.computername -Value localhost # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value \u0026#39;localhost\u0026#39; ,\u0026#39;localhost\\PROD1\u0026#39;,\u0026#39;localhost\\PROD2\u0026#39;, \u0026#39;localhost\\PROD3\u0026#39; # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;dbachecksdemo\\dbachecks\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;sa\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompreesion -Value $true # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $true # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value FULL # What should our database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;NTLM\u0026#39; # Which Agent Operator should be defined? Set-DbcConfig -Name agent.dbaoperatorname -Value \u0026#39;DBA Team\u0026#39; # Which Agent Operator email should be defined? Set-DbcConfig -Name agent.dbaoperatoremail -Value \u0026#39;DBATeam@TheBeard.Local\u0026#39; # Which failsafe operator shoudl be defined? Set-DbcConfig -Name agent.failsafeoperator -Value \u0026#39;DBA Team\u0026#39; # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value DBAAdmin # What is the maximum time since I took a Full backup? Set-DbcConfig -Name policy.backup.fullmaxdays -Value 7 # What is the maximum time since I took a DIFF backup (in hours) ? Set-DbcConfig -Name policy.backup.diffmaxhours -Value 26 # What is the maximum time since I took a log backup (in minutes)? Set-DbcConfig -Name policy.backup.logmaxminutes -Value 30 # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;WORKGROUP\u0026#39; # Where is my Ola database? Set-DbcConfig -Name policy.ola.database -Value DBAAdmin # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # What is my SQL Credential Set-DbcConfig -Name app.sqlcredential -Value $null # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, HADR, PseudoSimple,spn # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 Get-Dbcconfig | ogv When I run this I get\nI can then export this to disk (to store in source control) using\nExport-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\production_config.json\rand I have a configuration file\nwhich I can use any time to set the configuration for dbachecks using the Import-DbcConfig command (But this doesn‚Äôt work in VS Codes integrated terminal ‚Äì which occasionally does odd things, this appears to be one of them)\nImport-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\production_config.json\rSo I can import this configuration and run my checks with it any time I like. This means that I can create many different test configurations for my many different environment or estate configurations.\nYes, I know ‚Äúgood/best practice‚Äù says we should use the same configuration for all of our instances but we know that isn‚Äôt true. We have instances that were set up 15 years ago that are still in production. We have instances from the companies our organisation has bought over the years that were set up by system administrators. We have instances that were set up by shadow IT and now we have to support but cant change.\nAs well as those though, we also have different environments. Our development or test environment will have different requirements to our production environments.\nIn this hypothetical situation the four instances for four different applications have 4 development containers which are connected to using SQL Authentication. We will need a different configuration.\nSQL Authentication We can set up SQL Authentication for connecting to our SQL Instances using the app.sqlcredential configuration. this is going to hold a PSCredential object for SQL Authenticated connection to your instance. If this is set the checks will always try to use it. Yes this means that the same username and password is being used for each connection. No there is currently no way to choose which instances use it and which don‚Äôt. This may be a limitation but as you will see further down you can still do this with different configurations\nTo set the¬†SQL Authentication run\nSet-DbcConfig -Name app.sqlcredential -Value (Get-Credential)\rThis will give a prompt for you to enter the credential\nDevelopment Environment Configuration So now we know how to set a SQL Authentication configuration we can create our development environment configuration like so. As you can see below the values are different for the checks and more checks have been skipped. I wont explain it all, if it doesn‚Äôt make sense ask a question in the comments or in the dbachecks in SQL Server Community Slack\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #region Dev Config # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value \u0026#39;localhost,1401\u0026#39; ,\u0026#39;localhost,1402\u0026#39;,\u0026#39;localhost,1403\u0026#39;, \u0026#39;localhost,1404\u0026#39; # What is my SQL Credential Set-DbcConfig -Name app.sqlcredential -Value (Get-Credential) # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;sa\u0026#39; # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;SQL\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;dbachecksdemo\\dbachecks\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompreesion -Value $false # What should our database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What should our database growth value be higher than (Mb)? Set-DbcConfig -Name policy.database.filegrowthvalue -Value 64 # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $false # What is the maximum latency (ms)? Set-DbcConfig -Name policy.network.latencymaxms -Value 100 # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value Simple # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value DBAAdmin # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;WORKGROUP\u0026#39; # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, HADR, SaReNamed, PseudoSimple,spn, DiskSpace, DatabaseCollation,Agent,Backup,UnusedIndex,LogfileCount,FileGroupBalanced,LogfileSize,MaintenanceSolution,ServerNameMatch Export-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\development_config.json Using The Different Configurations Now I have two configurations, one for my Production Environment and one for my development environment. I can run my checks whenever I like (perhaps you will automate this in some way)\nImport the production configuration Run my tests with that configuration and create a json file for my Power Bi labelled production Import the development configuration (and enter the SQL authentication credential) Run my tests with that configuration and create a json file for my Power Bi labelled development Start Power Bi to show those results 1 2 3 4 5 6 7 8 9 10 # Import the production config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\production_config.json # Run the tests with the production config and create/update the production json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Production # Import the development config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\development_config.json # Run the tests with the production config and create/update the development json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Development # Open the PowerBi Start-DbcPowerBi I have published the Power Bi so that you can see what it would like and have a click around (maybe you can see improvements you would like to contribute)\nnow we can see how each environment is performing according to our settings for each environment\nCombining Configurations Into One Result Set As you saw above, by using the Environment parameter of Update-DbcPowerBiDataSource you can add different environments to one report. But if I wanted to have a report for my application APP1 showing both production and development environments but they have different configurations how can I do this?\nHere‚Äôs how.\nCreate a configuration for the production environment (I have used the production configuration one from above but only localhost for the instance) Export it using to¬†C:\\Users\\dbachecks\\Desktop\\APP1-Prod_config.json Create a configuration for the development environment (I have used the development configuration one from above but only localhost,1401 for the instance) Export it using to¬†C:\\Users\\dbachecks\\Desktop\\APP1-Dev_config.json Then run\n1 2 3 4 5 6 7 8 9 # Import the production config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\APP1-Prod_config.json # Run the tests with the production config and create/update the production json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment APP1 # Import the development config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\APP1-Dev_config.json # Run the tests with the production config and create/update the development json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment APP1 -Append Start-DbcPowerBi Notice that this time there is an Append on the last Invoke-DbcCheck this creates a single json file for the PowerBi and the results look like this. Now we have the results for our application and both the production environment localhost and the development container localhost,1401\nIt‚Äôs Open Source ‚Äì We Want Your Ideas, Issues, New Code dbachecks is open-source¬†available on GitHub for anyone to contribute\nWe would love you to contribute. Please open issues for new tests, enhancements, bugs. Please fork the repository and add code to improve the module. please give feedback to make this module even more useful\nYou can also come in the SQL Server Community Slack and join the dbachecks channel and get advice, make comments or just join in the conversation\nThank You I want to say thank you to all of the people who have enabled dbachecks to get this far. These wonderful people have used their own time to ensure that you have a useful tool available to you for free\nChrissy Lemaire @cl\nFred¬†Weinmann @FredWeinmann\nCl√°udio Silva @ClaudioESSilva\nStuart Moore @napalmgram\nShawn Melton @wsmelton\nGarry Bargsley @gbargsley\nStephen Bennett¬†@staggerlee011\nSander Stad @SQLStad\nJess Pomfret @jpomfret\nJason Squires @js0505\nShane O‚ÄôNeill @SOZDBA\nand all of the other people who have contributed in the dbachecks Slack channel\n","date":"2018-02-22T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/02/03-autocomplete.png","permalink":"https://blog.robsewell.com/blog/dbachecks-configuration-deep-dive/","title":"dbachecks ‚Äì Configuration Deep Dive"},{"content":"I love VS Code. I love being able to press ALT + SHIFT + F and format my code.\nThe Problem I could reproduce it will. This was very frustrating.\nTurning on Verbose Logging To turn on verbose logging for the PowerShell Editor Services go the Cog in the bottom left, click it and then click User Settings.\nSearch for powershell.developer.editorServicesLogLevel\nIf you hover over the left hand channel a pencil will appear, click it and then click replace in settings\nThis will put the entry in the right hand side where you can change the value. Set it to Verbose and save\na prompt will come up asking if you want to restart PowerShell\nWhen you restart PowerShell, if you click on¬†Output and choose PowerShell Extension Logs you will see the path to the log file\nReproduce the error I then reproduced the error and opened the log file this is what I got\n10/02/2018 09:11:19 [ERROR] ‚Äì Method ‚ÄúOnListenTaskCompleted‚Äù at line 391 of C:\\projects\\powershelleditorservices\\src\\PowerShellEditorServices.Protocol\\MessageProtocol\\ProtocolEndpoint.cs\nProtocolEndpoint message loop terminated due to unhandled exception:\nSystem.AggregateException: One or more errors occurred. ‚Äî\u0026gt; System.Management.Automation.CommandNotFoundException: The term ‚ÄòInvoke-Formatter‚Äô is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nat System.Management.Automation.Runspaces.PipelineBase.Invoke(IEnumerable input)\nat System.Management.Automation.PowerShell.Worker.ConstructPipelineAndDoWork(Runspace rs, Boolean performSyncInvoke)\nat System.Management.Automation.PowerShell.Worker.CreateRunspaceIfNeededAndDoWork(Runspace rsToUse, Boolean isSync)\nat System.Management.Automation.PowerShell.CoreInvokeHelper[TInput,TOutput](PSDataCollection`1 input, PSDataCollection`1 output, PSInvocationSettings settings)\nat System.Management.Automation.PowerShell.CoreInvoke[TInput,TOutput](PSDataCollection`1 input, PSDataCollection`1 output, PSInvocationSettings settings)\nat System.Management.Automation.PowerShell.Invoke(IEnumerable input, PSInvocationSettings settings)\nat Microsoft.PowerShell.EditorServices.AnalysisService.InvokePowerShell(String command, IDictionary2 paramArgMap) at System.Threading.Tasks.Task1.InnerInvoke()\nat System.Threading.Tasks.Task.Execute()\n‚Äî End of stack trace from previous location where exception was thrown ‚Äî\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Microsoft.PowerShell.EditorServices.AnalysisService.d__31.MoveNext()\n‚Äî End of stack trace from previous location where exception was thrown ‚Äî\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nat System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\nat Microsoft.PowerShell.EditorServices.AnalysisService.d__22.MoveNext()\n‚Äî End of stack trace from previous location where exception was thrown ‚Äî\nat System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()\nOpen an issue on GitHub I couldnt quickly see what was happening so I opened an issue on the vscode-powershell repo¬†by going to issues and clicking new issue and following the instructions\nThe Resolution Keith Hill b | t¬†pointed me to the resolution. Thank you Keith.\nFurther up in the log file there is a line where the editor services is loading the PSScriptAnalyzer module and it should have the Invoke-Formatter command exported, but mine was not. It loaded the PsScriptAnalyzer module¬†from my users module directory\n10/02/2018 09:11:01 [NORMAL] ‚Äì Method ‚ÄúFindPSScriptAnalyzerModule‚Äù at line 354 of C:\\projects\\powershelleditorservices\\src\\PowerShellEditorServices\\Analysis\\AnalysisService.cs\nPSScriptAnalyzer found at C:\\Users\\XXXX\\Documents\\WindowsPowerShell\\Modules\\PSScriptAnalyzer\\1.10.0\\PSScriptAnalyzer.psd1\n10/02/2018 09:11:01 [VERBOSE] ‚Äì Method ‚ÄúEnumeratePSScriptAnalyzerCmdlets‚Äù at line 389 of C:\\projects\\powershelleditorservices\\src\\PowerShellEditorServices\\Analysis\\AnalysisService.cs\nThe following cmdlets are available in the imported PSScriptAnalyzer module:\nGet-ScriptAnalyzerRule\nInvoke-ScriptAnalyzer\nI ran\n$Env:PSModulePath.Split(\u0026rsquo;;')\nto see the module paths\nand looked in the .vscode-insiders\\extensions\\ms-vscode.powershell-1.5.1\\modules directory. There was no PsScriptAnalyzer folder\nSo I copied the PSScriptAnalyzer folder from the normal VS Code PowerShell Extension module folder into that folder and restarted PowerShell and I had my formatting back again üôÇ\nI then reset the logging mode in my user settings back to Normal\nThank you Keith\n","date":"2018-02-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/02/formatting.gif","permalink":"https://blog.robsewell.com/blog/vs-code-terminal-crashes-when-formatting-script/","title":"VS Code ‚Äì Terminal crashes when formatting script"},{"content":"Last weekend I was thinking about how to save the tweets for PowerShell Conference Europe. This annual event occurs in Hanover and this year it is on¬†April 17-20, 2018. The agenda has just been released and you can find it on the website¬†http://www.psconf.eu/\nI ended up creating an interactive PowerBi report to which my good friend and Data Platform MVP Paul Andrew b | t¬†added a bit of magic and¬†I published it. The magnificent Tobias Weltner b | t who organises PSConfEU pointed the domain name http://powershell.cool at the link. It looks like this.\nDuring the monthly #PSTweetChat\nReminder that we do this chat the first Friday of every month from 1-2PM Eastern which I think is 6:00PM UTC #pstweetchat\n‚Äî Jeffery Hicks (@JeffHicks) February 2, 2018\nI mentioned that I need to blog about how I created it and Jeff replied\nYes, please. I\u0026rsquo;d love to setup something similiar for the PowerShell+DevOps Summit. #pstweetchat\n‚Äî Jeffery Hicks (@JeffHicks) February 2, 2018\nso here it is! Looking forward to seeing the comparison between the PowerShell and Devops Summit and the PowerShell Conference Europe üôÇ\nThis is an overview of how it works\nA Microsoft Flow looks for tweets with the #PSConfEU hashtag and then gets the information about the tweet Microsoft Cognitive Services Text Analysis API analyses the sentiment of the tweet and provides a score between 0 (negative) and 1 (positive) Details about the tweet and the sentiment are saved in Azure SQL database A PowerBi report uses that data and provides the report You will find all of the resources and the scripts to do all of the below in the GitHub repo. So clone it and navigate to the filepath\nCreate Database First lets create a database. Connect to your Azure subscription\n## Log in to your Azure subscription using the Add-AzureRmAccount command and follow the on-screen directions.\rAdd-AzureRmAccount\r## Select the subscription\rSet-AzureRmContext -SubscriptionId YourSubscriptionIDHere\rThen set some variables\n# The data center and resource name for your resources\r$resourcegroupname = \u0026quot;twitterresource\u0026quot;\r$location = \u0026quot;WestEurope\u0026quot;\r# The logical server name: Use a random value or replace with your own value (do not capitalize)\r$servername = \u0026quot;server-$(Get-Random)\u0026quot;\r# Set an admin login and password for your database\r# The login information for the server You need to set these and uncomment them - Dont use these values\r# $adminlogin = \u0026quot;ServerAdmin\u0026quot; # $password = \u0026quot;ChangeYourAdminPassword1\u0026quot;\r# The ip address range that you want to allow to access your server - change as appropriate\r# $startip = \u0026quot;0.0.0.0\u0026quot;\r# $endip = \u0026quot;0.0.0.0\u0026quot;\r# To just add your own IP Address\r$startip = $endip = (Invoke-WebRequest 'http:// myexternalip.com/raw').Content -replace \u0026quot;`n\u0026quot;\r# The database name\r$databasename = \u0026quot;tweets\u0026quot;\r$AzureSQLServer = \u0026quot;$servername.database. windows.net,1433\u0026quot;\r$Table = \u0026quot;table.sql\u0026quot;\r$Proc = \u0026quot;InsertTweets.sql\u0026quot;\rThey should all make sense, take note that you need to set and uncomment the login and password and choose which IPs to allow through the firewall\nCreate a Resource Group\n## Create a resource group\rNew-AzureRmResourceGroup -Name $resourcegroupname -Location $location\rCreate a SQL Server\n## Create a Server\r$newAzureRmSqlServerSplat = @{\rSqlAdministratorCredentials = $SqlAdministratorCredentials\rResourceGroupName = $resourcegroupname\rServerName = $servername\rLocation = $location\r}\rNew-AzureRmSqlServer @newAzureRmSqlServerSplat\rCreate a firewall rule, I just use my own IP and add the allow azure IPs\n$newAzureRmSqlServerFirewallRuleSplat = @{\rEndIpAddress = $endip\rStartIpAddress = $startip\rServerName = $servername\rResourceGroupName = $resourcegroupname\rFirewallRuleName = \u0026quot;AllowSome\u0026quot;\r}\rNew-AzureRmSqlServerFirewallRule @newAzureRmSqlServerFirewallRuleSplat\r# Allow Azure IPS\r$newAzureRmSqlServerFirewallRuleSplat = @{\rAllowAllAzureIPs = $true\rServerName = $servername\rResourceGroupName = $resourcegroupname\r}\rNew-AzureRmSqlServerFirewallRule @newAzureRmSqlServerFirewallRuleSplat\rCreate a database\n# Create a database\r$newAzureRmSqlDatabaseSplat = @{\rServerName = $servername\rResourceGroupName = $resourcegroupname\rEdition = 'Basic'\rDatabaseName = $databasename\r}\rNew-AzureRmSqlDatabase @newAzureRmSqlDatabaseSplat\rI have used the dbatools module¬†to run the scripts to create the database. You can get it using\nInstall-Module dbatools # -Scope CurrentUser # if not admin process\rRun the scripts\r# Create a credential\r$newObjectSplat = @{\rArgumentList = $adminlogin, $ (ConvertTo-SecureString -String $password -AsPlainText -Force)\rTypeName = 'System.Management.Automation. PSCredential'\r}\r$SqlAdministratorCredentials = New-Object @newObjectSplat\r## Using dbatools module\r$invokeDbaSqlCmdSplat = @{\rSqlCredential = $SqlAdministratorCredentials\rDatabase = $databasename\rFile = $Table,$Proc\rSqlInstance = $AzureSQLServer\r}\rInvoke-DbaSqlCmd @invokeDbaSqlCmdSplat\rThis will have created the following in Azure, you can see it in the portal\nYou can connect to the database in SSMS and you will see\nCreate Cognitive Services Now you can create the Text Analysis Cognitive Services API\nFirst login (if you need to) and set some variables\n## This creates cognitive services for analysing the tweets\r## Log in to your Azure subscription using the Add-AzureRmAccount command and follow the on-screen directions.\rAdd-AzureRmAccount\r## Select the subscription\rSet-AzureRmContext -SubscriptionId YOUR SUBSCRIPTION ID HERE\r#region variables\r# The data center and resource name for your resources\r$resourcegroupname = \u0026quot;twitterresource\u0026quot;\r$location = \u0026quot;WestEurope\u0026quot;\r$APIName = 'TweetAnalysis'\r#endregion\rThen create the API and get the key\r#Create the cognitive services\r$newAzureRmCognitiveServicesAccountSplat = @{\rResourceGroupName = $resourcegroupname\rLocation = $location\rSkuName = 'F0'\rName = $APIName\rType = 'TextAnalytics'\r}\rNew-AzureRmCognitiveServicesAccount @newAzureRmCognitiveServicesAccountSplat\r# Get the Key\r$getAzureRmCognitiveServicesAccountKeySplat = @ {\rName = $APIName\rResourceGroupName = $resourcegroupname\r}\rGet-AzureRmCognitiveServicesAccountKey @getAzureRmCognitiveServicesAccountKeySplat You will need to accept the prompt\nCopy the Endpoint URL as you will need it.Then save one of¬†the keys for the next step!\nCreate the Flow I have exported the Flow to a zip file and also the json for a PowerApp (no details about that in this post). Both are available in the GitHub repo. I have submitted a template but it is not available yet.\nNavigate to¬†https://flow.microsoft.com/¬†and sign in\nCreating Connections You will need to set up your connections. Click New Connection and search for Text\nClick Add and fill in the Account Key and the Site URL from the steps above\nclick new connection and search for SQL Server\nEnter the SQL Server Name (value of $AzureSQLServer) , Database Name , User Name and Password from the steps above\nClick new Connection and search for Twitter and create a connection (the authorisation pop-up may be hidden behind other windows!)\nImport the Flow If you have a premium account you can import the flow, click Import\nand choose the import.zip from the GitHub Repo\nClick on Create as new and choose a name\nClick select during import next to Sentiment and choose the Sentiment connection\nSelect during import for the SQL Server Connection and choose the SQL Server Connection and do the same for the Twitter Connection\nThen click import\nCreate the flow without import If you do not have a premium account you can still create the flow using these steps. I have created a template but it is not available at the moment. Create the connections as above and then click Create from blank.\nChoose the trigger When a New Tweet is posted and add a search term. You may need to choose the connection to twitter by clicking the three dots\nClick Add an action\nsearch for detect and choose the Text Analytics Detect Sentiment\nEnter the name for the connection, the account key and the URL from the creation of the API above. If you forgot to copy them\n#region Forgot the details\r# Copy the URL if you forget to save it\r$getAzureRmCognitiveServicesAccountSplat = @{\rName = $APIName\rResourceGroupName = $resourcegroupname\r}\r(Get-AzureRmCognitiveServicesAccount @getAzureRmCognitiveServicesAccountSplat). Endpoint | Clip\r# Copy the Key if you forgot\r$getAzureRmCognitiveServicesAccountKeySplat = @ {\rName = $APIName\rResourceGroupName = $resourcegroupname\r}\r(Get-AzureRmCognitiveServicesAccountKey @getAzureRmCognitiveServicesAccountKeySplat). Key1 | Clip\r#endregion\rClick in the text box and choose Tweet Text\nClick New Step and add an action. Search for SQL Server and choose SQL Server ‚Äì Execute Stored Procedure\nChoose the stored procedure¬†[dbo].[InsertTweet]\nFill in as follows\n__PowerAppsID__¬†0 Date¬†Created At Sentiment Score Tweet Tweet Text UserLocation¬†Location UserName¬†Tweeted By as shown below\nGive the flow a name at the top and click save flow\nConnect PowerBi Open the¬†PSConfEU Twitter Analysis Direct.pbix from the GitHub repo in PowerBi Desktop. Click the arrow next to Edit Queries and then change data source settings\nClick Change source and enter the server¬†(value of $AzureSQLServer) and the database name. It will alert you to apply changes\nIt will then pop-up with a prompt for the credentials. Choose Database and enter your credentials and click connect\nand your PowerBi will be populated from the Azure SQL Database üôÇ This will fail if there are no records in the table because your flow hasn‚Äôt run yet. If it does just wait until you see some tweets and then click apply changes again.\nYou will probably want to alter the pictures and links etc and then yo can publish the report\nHappy Twitter Analysis\nDont forget to keep an eye on your flow runs to make sure they have succeeded.\n","date":"2018-02-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/how-i-created-powershell.cool-using-flow-azure-sql-db-cognitive-services-powerbi/","title":"How I created PowerShell.cool using Flow, Azure SQL DB, Cognitive Services \u0026 PowerBi"},{"content":"In my last blog post I showed how to run a script with the WhatIf parameter. This assumes that the commands within the script have been written to use the common parameters Confirm, Verbose and WhatIf.\nSomeone asked me how to make sure that any functions that they write will be able to do this.\nit is very easy\nWhen we define our function we are going to add [cmdletbinding(SupportsShouldProcess)] at the top\nfunction Set-FileContent {\r[cmdletbinding(SupportsShouldProcess)]\rParam()\rand every time we perform an action that will change something we put that code inside a code block like this\nif ($PSCmdlet.ShouldProcess(\u0026quot;The Item\u0026quot; , \u0026quot;The Change\u0026quot;)) {\r# place code here\r}\rand alter The Item and The Change as appropriate.\nI have created a snippet for VS Code to make this quicker for me. To add it to your VS Code. Click the settings button bottom right, Click User Snippets, choose the powershell json and add the code below between the last two }‚Äôs (Don‚Äôt forget the comma)\n,\r\u0026quot;IfShouldProcess\u0026quot;: {\r\u0026quot;prefix\u0026quot;: \u0026quot;IfShouldProcess\u0026quot;,\r\u0026quot;body\u0026quot;: [\r\u0026quot;if ($$PSCmdlet.ShouldProcess(\\\u0026quot;The Item\\\u0026quot; , \\\u0026quot;The Change\\\u0026quot;)) {\u0026quot;,\r\u0026quot; # Place Code here\u0026quot;,\r\u0026quot;}\u0026quot;\r],\r\u0026quot;description\u0026quot;: \u0026quot;Shows all the colour indexes for the Excel colours\u0026quot;\r}\rand save the powershell.json file\nThen when you are writing your code you can simply type ‚Äúifs‚Äù and tab and the code will be generated for you\nAs an example I shall create a function wrapped around Set-Content just so that you can see what happens.\nfunction Set-FileContent {\r[cmdletbinding(SupportsShouldProcess)]\rParam(\r[Parameter(Mandatory = $true)]\r[ValidateNotNullOrEmpty()]\r[string]$Content,\r[Parameter(Mandatory = $true)]\r[ValidateScript( {Test-Path $_ })]\r[string]$File\r)\rif ($PSCmdlet.ShouldProcess(\u0026quot;$File\u0026quot; , \u0026quot;Adding $Content to \u0026quot;)) {\rSet-Content -Path $File -Value $Content\r}\r}\rI have done this before because if the file does not exist then Set-Content will create a new file for you, but with this function I can check if the file exists first with the ValidateScript before running the rest of the function.\nAs you can see I add variables from my PowerShell code into the ‚ÄúThe Item‚Äù and ‚ÄúThe Change‚Äù. If I need to add a property of an object I use $($Item.Property).\nSo now, if I want to see what my new function would do if I ran it without actually making any changes I have -WhatIf added to my function automagically.\nSet-FileContent -File C:\\temp\\number1\\TextFile.txt -Content \u0026quot;This is the New Content\u0026quot; -WhatIf\rIf I want to confirm any action I take before it happens I have -Confirm\nSet-FileContent -File C:\\temp\\number1\\TextFile.txt -Content \u0026quot;This is the New Content\u0026quot; -Confirm\rAs you can see it also give the confirm prompts for the Set-Content command\nYou can also see the verbose messages with\nSet-FileContent -File C:\\temp\\number1\\TextFile.txt -Content \u0026quot;This is the New Content\u0026quot; -Verbose\rSo to summarise, it is really very simple to add Confirm, WhatIf and Verbose to your functions by placing¬†[cmdletbinding(SupportsShouldProcess)] at the top of the function and placing any code that makes a change inside\nif ($PSCmdlet.ShouldProcess(\u0026quot;The Item\u0026quot; , \u0026quot;The Change\u0026quot;)) {\rwith some values that explain what the code is doing to the The Item and The Change.\nBonus Number 1 ‚Äì This has added support for other common parameters as well ‚Äì Debug, ErrorAction, ErrorVariable, WarningAction, WarningVariable, OutBuffer, PipelineVariable, and OutVariable.\nBonus Number 2 ‚Äì This has automatically been added to your Help\nBonus Number 3 ‚Äì This has reduced the amount of comments you need to write and improved other peoples understanding of what your code is supposed to do üôÇ People can read your code and read what you have entered for the IfShouldProcess and that will tell them what the code is supposed to do üôÇ\nNow you have seen how easy it is to write more professional PowerShell functions\n","date":"2018-01-25T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/01/03-confirm.png","permalink":"https://blog.robsewell.com/blog/how-to-write-a-powershell-function-to-use-confirm-verbose-and-whatif/","title":"How to write a PowerShell function to use Confirm, Verbose and WhatIf"},{"content":"Before you run a PowerShell command that makes a change to something you should check that it is going to do what you expect. You can do this by using the WhatIf parameter for commands that support it. For example, if you wanted to create a New SQL Agent Job Category you would use the awesome dbatools module and write some code like this\nNew-DbaAgentJobCategory -SqlInstance ROB-XPS -Category 'Backup'\rbefore you run it, you can check what it is going to do using\nNew-DbaAgentJobCategory -SqlInstance ROB-XPS -Category 'Backup' -WhatIf\rwhich gives a result like this\nThis makes it easy to do at the command line but when we get confident with PowerShell we will want to write scripts to perform tasks using more than one command. So how can we ensure that we can check that those will do what we are expecting without actually running the script and see what happens? Of course, there are Unit and integration testing that should be performed using Pester when developing the script but there will still be occasions when we want to see what this script will do this time in this environment.\nLets take an example. We want to place our SQL Agent jobs into specific custom categories depending on their name. We might write a script like this\n\u0026lt;#\r.SYNOPSIS\rAdds SQL Agent Jobs to categories and creates the categories if needed\r.DESCRIPTION\rAdds SQL Agent Jobs to categories and creates the categories if needed. Creates\rBackup', 'Index', 'TroubleShooting','General Info Gathering' categories and adds\rthe agent jobs depending on name to the category\r.PARAMETER Instance\rThe Instance to run the script against\r#\u0026gt;\rParam(\r[string]$Instance\r)\r$Categories = 'Backup', 'Index','DBCC', 'TroubleShooting', 'General Info Gathering'\r$Categories.ForEach{\r## Create Category if it doesnot exist\rIf (-not (Get-DbaAgentJobCategory -SqlInstance $instance -Category $PSItem)) {\rNew-DbaAgentJobCategory -SqlInstance $instance -Category $PSItem -CategoryType LocalJob\r}\r}\r## Get the agent jobs and iterate through them\r(Get-DbaAgentJob -SqlInstance $instance).ForEach{\r## Depending on the name of the Job - Put it in a Job Category\rswitch -Wildcard ($PSItem.Name) {\r'*DatabaseBackup*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'Backup'\r}\r'*Index*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'Index'\r}\r'*DatabaseIntegrity*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'DBCC'\r}\r'*Log SP_*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'TroubleShooting'\r}\r'*Collection*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'General Info Gathering'\r}\r## Otherwise put it in the uncategorised category\rDefault {\rSet-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category '[Uncategorized (Local)]'\r}\r}\r}\rYou can run this script against any SQL instance by calling¬†it and passing an instance parameter from the command line like this\n\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rIf you wanted to see what would happen, you could edit the script and add the WhatIf parameter to every changing command but that‚Äôs not really a viable solution. What you can do is\n$PSDefaultParameterValues['*:WhatIf'] = $true\rthis will set all commands that accept WhatIf to use the WhatIf parameter. This means that if you are using functions that you have written internally you must ensure that you write your functions to use the common parameters\nOnce you have set the default value for WhatIf as above, you can simply call your script and see the WhatIf output\n\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rwhich will show the WhatIf output for the script\nOnce you have checked that everything is as you expected then you can remove the default value for the WhatIf parameter and run the script\n$PSDefaultParameterValues['*:WhatIf'] = $false\r\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rand get the expected output\nIf you wish to see the verbose output or ask for confirmation before any change you can set those default parameters like this\n## To Set Verbose output\r$PSDefaultParameterValues['*:Verbose'] = $true\r## To Set Confirm\r$PSDefaultParameterValues['*:Confirm'] = $true\rand set them back by setting to false\n","date":"2018-01-23T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/01/02-Showing-the-results.png","permalink":"https://blog.robsewell.com/blog/how-to-run-a-powershell-script-file-with-verbose-confirm-or-whatif/","title":"How to run a PowerShell script file with Verbose, Confirm or WhatIf"},{"content":"I was going through my demo for the South Coast User Group meeting tonight and decided to add some details about the Because parameter available in the Pester pre-release version 4.2.0.\nTo install a pre-release version you need to get the latest¬†PowerShellGet¬†module. This is pre-installed with PowerShell v6 but for earlier versions open PowerShell as administrator and run\nInstall-Module PowerShellGet\rYou can try out the Pester pre-release version (once you have the latest PowerShellGet) by installing it from the PowerShell Gallery with\nInstall-Module -Name Pester -AllowPrerelease -Force # -Scope CurrentUser # if not admin\rThere are a number of improvements as you can see in the change log¬†I particularly like the\nAdd -BeTrue to test for truthy values Add -BeFalse to test for falsy values This release adds the Because parameter to the all assertions. This means that you can add a reason why the test has failed. As JAKUB JARE≈† writes here\nReasons force you think more\nReasons document your intent\nReasons make your TestCases clearer\nSo you can do something like this\nDescribe \u0026ldquo;This shows the Because\u0026rdquo;{ It \u0026ldquo;Should be true\u0026rdquo; { $false | Should -BeTrue -Because \u0026ldquo;The Beard said so\u0026rdquo; } }\nWhich gives an error message like this üôÇ\nAs you can see the Expected gives the expected value and then your Because statement and then the actual result. Which means that you could write validation tests like\nDescribe \u0026quot;My System\u0026quot; {\rContext \u0026quot;Server\u0026quot; {\rIt \u0026quot;Should be using XP SP3\u0026quot; {\r(Get-CimInstance -ClassName win32_operatingsystem) .Version | Should -Be '5.1.2600' -Because \u0026quot;We have failed to bother to update the App and it only works on XP\u0026quot;\r}\rIt \u0026quot;Should be running as rob-xps\\\\mrrob\u0026quot; {\rwhoami | Should -Be 'rob-xps\\\\mrrob' -Because \u0026quot;This is the user with the permissions\u0026quot;\r}\rIt \u0026quot;Should have SMB1 enabled\u0026quot; {\r(Get-SmbServerConfiguration).EnableSMB1Protocol | Should -BeTrue -Because \u0026quot;We don't care about the risk\u0026quot;\r}\r}\r}\rand get a result like this\nOr if you were looking to validate your SQL Server you could write something like this\nIt \u0026quot;Backups Should have Succeeeded\u0026quot; {\r$Where = {$\\_IsEnabled -eq $true -and $\\_.Name -like '\\*databasebackup\\*'}\r$Should = @{\rBeTrue = $true\rBecause = \u0026quot;WE NEED BACKUPS - OMG\u0026quot;\r}\r(Get-DbaAgentJob -SqlInstance $instance| Where-Object $where).LastRunOutcome -NotContains 'Failed' | Should @Should\r}\ror maybe your security policies allow Windows Groups as logins on your SQL instances. You could easily link to the documentation and explain why this is important. This way you could build up a set of tests to validate your SQL Server is just so for your environment\nIt \u0026quot;Should only have Windows groups as logins\u0026quot; {\r$Should = @{\rBefalse = $true\rBecause = \u0026quot;Our Security Policies say we must only have Windows groups as logins - See this document\u0026quot;\r}\r(Get-DbaLogin -SqlInstance $instance -WindowsLogins). LoginType -contains 'WindowsUser' | Should @Should\r}\rJust for fun, these would look like this\nand the code looks like\n$Instances = 'Rob-XPS', 'Rob-XPS\\\\Bolton'\rforeach ($instance in $Instances) {\r$Server, $InstanceName = $Instance.Split('/')\rif ($InstanceName.Length -eq 0) {$InstanceName = 'MSSSQLSERVER'}\rDescribe \u0026quot;Testing the instance $instance\u0026quot; {\rContext \u0026quot;SQL Agent Jobs\u0026quot; {\rIt \u0026quot;Backups Should have Succeeeded\u0026quot; {\r$Where = {$\\_IsEnabled -eq $true -and $\\_. Name -like '\\*databasebackup\\*'}\r$Should = @{\rBeTrue = $true\rBecause = \u0026quot;WE NEED BACKUPS - OMG \u0026quot;\r}\r(Get-DbaAgentJob -SqlInstance $instance| Where-Object $where).LastRunOutcome -NotContains 'Failed' | Should @Should\r}\rContext \u0026quot;Logins\u0026quot; {\rIt \u0026quot;Should only have Windows groups as logins\u0026quot; {\r$Should = @{\rBefalse = $true\rBecause = \u0026quot;Our Security Policies say we must only have Windows groups as logins - See this document\u0026quot;\r}\r(Get-DbaLogin -SqlInstance $instance -WindowsLogins).LoginType -contains 'WindowsUser' | Should @Should\r}\r}\r}\r}\r}\rThis will be a useful improvement to Pester when it is released and enable you to write validation checks with explanations.\nCome and Learn Some PowerShell Magic* at #SQLBits with @cl and I\nDetails https://t.co/7OfK75e6Y1\nRegistration https://t.co/RDSkPlfMMx\n*PowerShell is not magic ‚Äì it just might appear that way pic.twitter.com/5czPzYR3QD\n‚Äî Rob Sewell (@sqldbawithbeard) November 27, 2017\nChrissy has written about dbachecks the new up and coming community driven open source PowerShell module for SQL DBAs to validate their SQL Server estate. we have taken some of the ideas that we have presented about a way of using dbatools with Pester to validate that everything is how it should be and placed them into a meta data driven framework to make things easy for anyone to use. It is looking really good and I am really excited about it. It will be released very soon.\nChrissy and I will be doing a pre-con at SQLBits where we will talk in detail about how this works. You can find out more and sign up here\n","date":"2018-01-18T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/01/01-Because-1.png","permalink":"https://blog.robsewell.com/blog/pester-4.2.0-has-a-because-because-/","title":"Pester 4.2.0 has a Because‚Ä¶‚Ä¶ because :-)"},{"content":"TagLine ‚Äì My goal ‚Äì Chrissy will appreciate Unit Tests one day üôÇ\nChrissy has written about dbachecks the new up and coming community driven open source PowerShell module for SQL DBAs to validate their SQL Server estate. we have taken some of the ideas that we have presented about a way of using dbatools with Pester to validate that everything is how it should be and placed them into a meta data driven framework to make things easy for anyone to use. It is looking really good and I am really excited about it. It will be released very soon.\nChrissy and I will be doing a pre-con at SQLBits where we will talk in detail about how this works. You can find out more and sign up here\nCl√°udio Silva has improved my PowerBi For Pester¬†file and made it beautiful and whilst we were discussing this we found that if the Pester Tests were not formatted correctly the Power Bi looked ‚Ä¶ well rubbish to be honest! Chrissy asked if we could enforce some rules for writing our Pester tests.\nThe rules were\nThe Describe title should be in double quotes\nThe Describe should use the plural Tags parameter\nThe Tags should be singular\nThe first Tag should be a unique tag in Get-DbcConfig\nThe context title should end with $psitem\nThe code should use Get-SqlInstance or Get-ComputerName\nThe Code should use the forEach method\nThe code should not use $_\nThe code should contain a Context block\nShe asked me if I could write the Pester Tests for it and this is how I did it. I needed to look at the Tags parameter for the Describe. It occurred to me that this was a job for the Abstract Syntax Tree (AST). I don‚Äôt know very much about the this but I sort of remembered reading a blog post by Francois-Xavier Cat about using it with Pester so I went and read that and found an answer on Stack Overflow as well. These looked just like what I needed so I made use of them. Thank you very much to Francois-Xavier and wOxxOm for sharing.\nThe first thing I did was to get the Pester Tests which we have located in a checks folder and loop through them and get the content of the file with the Raw parameter\nContext \u0026quot;$($_.Name) - Checking Describes titles and tags\u0026quot; {\rThen I decided to look at the Describes using the method that¬†wOxxOm (I know no more about this person!) showed.\n$Describes = \\[Management.Automation.Language.Parser\\] ::ParseInput($check, \\[ref\\]$tokens, \\[ref\\]$errors).\rFindAll(\\[Func\\[Management.Automation.Language.Ast, bool\\]\\] {\rparam($ast)\r$ast.CommandElements -and\r$ast.CommandElements\\[0\\].Value -eq 'describe'\r}, $true) |\rForEach {\r$CE = $_.CommandElements\r$secondString = ($CE |Where { $_.StaticType.name -eq 'string' })\\[1\\]\r$tagIdx = $CE.IndexOf(($CE |Where ParameterName -eq'Tags') ) + 1\r$tags = if ($tagIdx -and $tagIdx -lt $CE.Count) {\r$CE\\[$tagIdx\\].Extent\r}\rNew-Object PSCustomObject -Property @{\rName = $secondString\rTags = $tags\r}\r}\rAs I understand it, this code is using the Parser on the $check (which contains the code from the file) and finding all of the Describe commands and creating an object of the title of the Describe with the StaticType equal to String and values from the Tag parameter.\nWhen I ran this against the database tests file I got the following results\nThen it was a simple case of writing some tests for the values\n@($describes).Foreach{\r$title = $PSItem.Name.ToString().Trim('\u0026quot;').Trim('''')\rIt \u0026quot;$title Should Use a double quote after the Describe\u0026quot; {\r$PSItem.Name.ToString().Startswith('\u0026quot;')| Should be $true\r$PSItem.Name.ToString().Endswith('\u0026quot;')| Should be $true\r}\rIt \u0026quot;$title should use a plural for tags\u0026quot; {\r$PsItem.Tags| Should Not BeNullOrEmpty\r}\r# a simple test for no esses apart from statistics and Access!!\rif ($null -ne $PSItem.Tags) {\r$PSItem.Tags.Text.Split(',').Trim().Where{($_ -ne '$filename') -and ($_ -notlike '\\*statistics\\*') -and ($_ -notlike '\\*BackupPathAccess\\*') }.ForEach{\rIt \u0026quot;$PsItem Should Be Singular\u0026quot; {\r$_.ToString().Endswith('s')| Should Be $False\r}\r}\rIt \u0026quot;The first Tag Should Be in the unique Tags returned from Get-DbcCheck\u0026quot; {\r$UniqueTags -contains $PSItem.Tags.Text.Split(',') \\[0\\].ToString()| Should Be $true\r}\r}\relse {\rIt \u0026quot;You haven't used the Tags Parameter so we can't check the tags\u0026quot; {\r$false| Should be $true\r}\r}\r}\rThe Describes variable is inside @() so that if there is only one the ForEach Method will still work. The unique tags are returned from our command Get-DbcCheck which shows all of the checks. We will have a unique tag for each test so that they can be run individually.\nYes, I have tried to ensure that the tags are singular by ensuring that they do not end with an s (apart from statistics) and so had to not check¬†BackupPathAccess and statistics. Filename is a variable that we add to each Describe Tags so that we can run all of the tests in one file. I added a little if block to the Pester as well so that the error if the Tags parameter was not passed was more obvious\nI did the same with the context blocks as well\nContext \u0026quot;$($_.Name) - Checking Contexts\u0026quot; {\r## Find the Contexts\r$Contexts = \\[Management.Automation.Language.Parser\\] ::ParseInput($check, \\[ref\\]$tokens, \\[ref\\]$errors).\rFindAll(\\[Func\\[Management.Automation.Language.Ast, bool\\] \\] {\rparam($ast)\r$ast.CommandElements -and\r$ast.CommandElements\\[0\\].Value -eq 'Context'\r}, $true) |\rForEach {\r$CE = $_.CommandElements\r$secondString = ($CE |Where { $_.StaticType.name -eq 'string' })\\[1\\]\rNew-Object PSCustomObject -Property @{\rName = $secondString\r}\r}\r@($Contexts).ForEach{\r$title = $PSItem.Name.ToString().Trim('\u0026quot;').Trim('''')\rIt \u0026quot;$Title Should end with `$psitem So that the PowerBi will work correctly\u0026quot; {\r$PSItem.Name.ToString().Endswith('psitem\u0026quot;')| Should Be $true\r}\r}\r}\rThis time we look for the Context command and ensure that the string value ends with psitem as the PowerBi parses the last value when creating columns\nFinally I got all of the code and check if it matches some coding standards\nContext \u0026quot;$($_.Name) - Checking Code\u0026quot; {\r## This just grabs all the code\r$AST = \\[System.Management.Automation.Language.Parser\\] ::ParseInput($Check, \\[ref\\]$null, \\[ref\\]$null)\r$Statements = $AST.EndBlock.statements.Extent\r## Ignore the filename line\r@($Statements.Where{$_.StartLineNumber -ne 1}).ForEach{\r$title = \\[regex\\]::matches($PSItem.text, \u0026quot;Describe(. *)-Tag\u0026quot;).groups\\[1\\].value.Replace('\u0026quot;', '').Replace ('''', '').trim()\rIt \u0026quot;$title Should Use Get-SqlInstance or Get-ComputerName\u0026quot; {\r($PSItem.text -Match 'Get-SqlInstance') -or ($psitem.text -match 'Get-ComputerName')| Should be $true\r}\rIt \u0026quot;$title Should use the ForEach Method\u0026quot; {\r($Psitem.text -match 'Get-SqlInstance\\\\).ForEach {') -or ($Psitem.text -match 'Get-ComputerName\\\\). ForEach{')| Should Be $true# use the \\ to escape the )\r}\rIt \u0026quot;$title Should not use `$_\u0026quot; {\r($Psitem.text -match '$_')| Should Be $false\r}\rIt \u0026quot;$title Should Contain a Context Block\u0026quot; {\r$Psitem.text -match 'Context'| Should Be $True\r}\r}\rI trim the title from the Describe block so that it is easy to see where the failures (or passes) are with some regex and then loop through each statement apart from the first line to ensure that the code is using our internal commands Get-SQLInstance or Get-ComputerName to get information, that we are looping through each of those arrays using the ForEach method rather than ForEach-Object and using $psitem rather than $_ to reference the ‚ÄúThis Item‚Äù in the array and that each Describe block has a context block.\nThis should ensure that any new tests that are added to the module follow the guidance we have set up on the Wiki and ensure that the Power Bi results still look beautiful!\nAnyone can run the tests using\nInvoke-Pester .\\\\tests\\\\Unit.Tests.ps1 -show Fails\rbefore they create a Pull request and it looks like\nif everything is Green then they can submit their Pull Request üôÇ If not they can see quickly that something needs to be fixed. (fail early üôÇ )\n","date":"2018-01-15T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2018/01/02-Pester-results-1.png","permalink":"https://blog.robsewell.com/blog/using-the-ast-in-pester-for-dbachecks/","title":"Using the AST in Pester for dbachecks"},{"content":"This is just a quick post. As is frequent with these they are as much for me to refer to in the future and also because the very act of writing it down will aid me in remembering. I encourage you to do the same. Share what you learn because it will help you as well as helping others.\nAnyway, I was writing some Pester tests for a module that I was writing when I needed some sample data. I have written before about using Json for this purpose¬†This function required some data from a database so I wrote the query to get the data and used dbatools¬†to run the query against the database using Get-DbaDatabase\n1 2 $db = Get-DbaDatabase -SqlInstance $Instance -Database $Database $variable = $db.Query($Query) Simple enough. I wanted to be able to Mock $variable. I wrapped the code above in a function, let‚Äôs call it Run-Query\n1 2 3 4 function Run-Query {(Param $query) $db = Get-DbaDatabase -SqlInstance $Instance -Database $Database $variable = $db.Query($Query) } Which meant that I could easily separate it for mocking in my test. I ran the code and investigated the $variable variable to ensure it had what I wanted for my test and then decided to convert it into JSON using ConvertTo-Json\nLets show what happens with an example using WideWorldImporters and a query I found on Kendra Littles blogpost about deadlocks\n1 2 3 4 5 6 7 8 9 10 11 12 $query = @\u0026#34; SELECT Top 10 CityName, StateProvinceName, sp.LatestRecordedPopulation, CountryName FROM Application.Cities AS city JOIN Application.StateProvinces AS sp on city.StateProvinceID = sp.StateProvinceID JOIN Application.Countries AS ctry on sp.CountryID=ctry.CountryID WHERE sp.StateProvinceName = N\u0026#39;Virginia\u0026#39; ORDER BY CityName \u0026#34;@ $db = Get-DbaDatabase -SqlInstance rob-xps -Database WideWorldImporters $variable = $db.Query($Query) If I investigate the $variable variable I get\nThe results were just what I wanted so I thought I will just convert them to JSON and save them in a file and bingo I have some test data in a mock to ensure my code is doing what I expect. However, when I run\n$variable | ConvertTo-Json\nI get\nand thats just for one row!\nThe way to resolve this is to only select the data that we need. The easiest way to do this is to exclude the properties that we don‚Äôt need\n$variable | Select-Object * -ExcludeProperty ItemArray, Table, RowError, RowState, HasErrors | ConvertTo-Json\nwhich gave me what I needed and a good use case for -ExcludeProperty\n","date":"2017-12-18T00:00:00Z","permalink":"https://blog.robsewell.com/blog/converting-a-datarow-to-a-json-object-with-powershell/","title":"Converting a Datarow to a JSON object with PowerShell"},{"content":"In my previous posts about writing your first Pester Test and looping through instances I described how you can start to validate that your SQL Server is how YOU want it to be.\nUnavailable machines Once you begin to have a number of tests for a number of instances you want to be able to handle any machines that are not available cleanly otherwise you might end up with something like this.\nIn this (made up) example we loop through 3 instances and try to check the DNS Server entry is correct but for one of them we get a massive error and if we had created a large number of tests for each machine we would have a large number of massive errors.\nEmpty Collection If we don‚Äôt successfully create our collection we might have an empty collection which will give us a different issue. No tests\nIf this was in amongst a whole number of tests we would not have tested anything in this Describe block and might be thinking that our tests were OK because we had no failures of our tests. We would be wrong!\nDealing with Empty Collections One way of dealing with empty collections is to test that they have more than 0 members\n1 2 3 4 5 6 if ($instances.count -gt 0) { $instances.ForEach{ ## Tests in here } } else {Write-Warning \u0026#34;Uh-Oh - The Beard is Sad! - The collection is empty. Did you set `$Instances correctly?\u0026#34;} Notice the backtick ` before the $ to escape it in the Write-Warning. An empty collection now looks like\nWhich is much better and provides useful information to the user\nDealing with Unavailable Machines If we want to make sure we dont clutter up our test results with a whole load of failures when a machine is unavailable we can use similar logic.\nFirst we could check if it is responding to a ping (assuming that ICMP is allowed by the firewall and switches) using\nTest-Connection -ComputerName $computer -Count 1 -Quiet -ErrorAction SilentlyContinue\nThis will just try one ping and do it quietly only returning True or False and if there are any errors it shouldn‚Äôt mention it\nIn the example above I am using PSRemoting and we should make sure that that is working too. So whilst I could use\nTest-WSMan -ComputerName $computer\nthis only checks if a WSMAN connection is possible and not other factors that could be affecting the ability to run remote sessions. Having been caught by this before I have always used this function from Lee Holmes (Thank you Lee) and thus can use\n1 2 3 4 5 6 7 8 9 10 11 12 $instances.ForEach{ $computer = $_.Split(\u0026#39;\\\\\u0026#39;)\\[0\\]# To get the computername if there is an instance name # Check if machine responds to ping if (!(Test-Connection-ComputerName $computer-Count 1-Quiet -ErrorAction SilentlyContinue)) {Write-Warning \u0026#34;Uh-Oh - $Computer is not responding to a ping - aborting the tests for this machine\u0026#34;; Return} # Check if PSremoting is possible for this machine # Requires Test-PSRemoting by Lee Holmes¬†http://www.leeholmes.com/blog/2009/11/20/testing-for-powershell-remoting-test-psremoting/ if (!(Test-PsRemoting$computer)) {Write-Warning \u0026#34;Uh-Oh - $Computer is not able to use PSRemoting - aborting the tests for this machine\u0026#34;; Return} Describe \u0026#34;Testing Instance $($_)\u0026#34; { ## Put tests in here } which provides a result like this\nWhich is much better I think üôÇ\nLet dbatools do the error handling for you If your tests are only using the dbatools module then there is built in error handling that you can use. By default dbatools returns useful messages rather than the exceptions from PowerShell (You can enable the exceptions using the -EnableExceptions parameter if you want/need to) so if we run our example from the previous post it will look like\nwhich is fine for a single command but we don‚Äôt really want to waste time and resources repeatedly trying to connect to an instance if we know it is not available if we are running multiple commands against each instance.\ndbatools at the beginning of the loop We can use Test-DbaConnectionto perform a check at the beginning of the loop as we discussed in the previous post\n1 2 3 $instances.ForEach{ if (!((Test-DbaConnection-SqlInstance $_ -WarningAction SilentlyContinue).ConnectSuccess)) {Write-Warning \u0026#34;Uh-Oh - we cannot connect to $_ - aborting the tests for this instance\u0026#34;; Return} Notice that we have used -WarningAction SilentlyContinue to hide the warnings from the command this tiime. Our test now looks like\nTest-DbaConnection performs a number of tests so you can check for ping SQL version, domain name and remoting if you want to exclude tests on those basis\nRound Up In this post we have covered some methods of ensuring that your Pester Tests return what you expect. You don‚Äôt want empty collections of SQL Instances making you think you have no failed tests when you have not actually run any tests.\nYou can do this by checking how many instances are in the collection\nYou also dont want to keep running tests against a machine or instance that is not responding or available.\nYou can do this by checking a ping with Test-Connection or if remoting is required by using the Test-PSRemoting function from Lee Holmes\nIf you want to use dbatools exclusively you can use Test-DbaConnection\nHere is a framework to put your tests inside. You will need to provide the values for the $Instances and place your tests inside the Describe Block\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 if ($instances.count -gt 0) { $instances.ForEach{ $TestConnection = Test-DbaConnection-SqlInstance $_ -WarningAction SilentlyContinue # Check if machine responds to ping if (!($TestConnection.IsPingable)) {Write-Warning \u0026#34;Uh-Oh - The Beard is Sad! - - $_ is not responding to a ping - aborting the tests for this instance\u0026#34;; Return} # Check if we have remote access to the machine if (!($TestConnection.PsRemotingAccessible)) {Write-Warning \u0026#34;Uh-Oh - The Beard is Sad! - - $_ is not able to use PSRemoting - aborting the tests for this instance\u0026#34;; Return} # Check if we have SQL connection to the Instance if (!($TestConnection.ConnectSuccess)) {Write-Warning \u0026#34;Uh-Oh - The Beard is Sad! - - we cannot connect to SQL on $_ - aborting the tests for this instance\u0026#34;; Return} Describe \u0026#34;Testing Instance $($_)\u0026#34; { ## Now put your tests in here - seperate them with context blocks if you want to Context \u0026#34;Networks\u0026#34; { } } } } else ## If the collection is empty { Write-Warning \u0026#34;Uh-Oh - The Beard is Sad! - The collection is empty. Did you set `$Instances correctly?\u0026#34; } ","date":"2017-11-30T00:00:00Z","permalink":"https://blog.robsewell.com/blog/handling-missing-instances-when-looping-with-pester/","title":"Handling Missing Instances when Looping with Pester"},{"content":"In my last post I showed you how to write your first Pester test to validate something. Here‚Äôs a recap\nDecide the information you wish to test Understand how to get it with PowerShell Understand what makes it pass and what makes it fail Write a Pester Test You probably have more than one instance that you want to test, so how do you loop through a collection of instances? There are a couple of ways.\nGetting the Latest Version of the Module Steve Jones wrote about getting the latest version of Pester and the correct way to do it. You can find the important information here\nTest Cases The first way is to use the Test Case parameter of the It command (the test) which I have written about when using TDD for Pester here\nLets write a test first to check if we can successfully connect to a SQL Instance. Running\nFind-DbaCommand connection\nshows us that the Test-DbaConnection command is the one that we want from the dbatools module. We should always run Get-Help to understand how to use any PowerShell command. This shows us that the results will look like this\nSo there is a ConnectSuccess result which returns True or false. Our test can look like this for a single instance\n1 2 3 4 5 Describe \u0026#39;Testing connection to ROB-XPS\u0026#39; { It \u0026#34;Connects successfully to ROB-XPS\u0026#34; { (Test-DbaConnection-SqlInstance ROB-XPS).ConnectSuccess | Should Be $True } } which gives us some test results that look like this\nwhich is fine for one instance but we want to check many.\nWe need to gather the instances into a $Instances variable. In my examples I have hard coded a list of SQL Instances but you can, and probably should, use a more dynamic method, maybe the results of a query to a configuration database. Then we can fill our TestCases variable which can be done like this\n1 2 3 4 5 $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; # Create an empty array $TestCases = @() # Fill the Testcases with the values and a Name of Instance $Instances.ForEach{$TestCases += @{Instance = $_}} Then we can write our test like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Get a list of SQL Servers # Use whichever method suits your situation # Maybe from a configuration database # I\u0026#39;m just using a hard-coded list for example $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; # Create an empty array $TestCases = @() # Fill the Testcases with the values and a Name of Instance $Instances.ForEach{$TestCases += @{Instance = $_}} Describe \u0026#39;Testing connection to SQL Instances\u0026#39; { # Put the TestCases \u0026#39;Name\u0026#39; in \u0026lt;\u0026gt; and add the TestCases parameter It \u0026#34;Connects successfully to \u0026lt;Instance\u0026gt;\u0026#34; -TestCases $TestCases { # Add a Parameter to the test with the same name as the TestCases Name Param($Instance) # Write the test using the TestCases Name (Test-DbaConnection -SqlInstance $Instance).ConnectSuccess | Should Be $True } } Within the title of the test we refer to the instance inside \u0026lt;\u0026gt; and add the parameter TestCases with a value of the $TestCases variable. We also need to add a Param() to the test with the same name and then use that variable in the test.\nThis looks like this\nPester is PowerShell The problem with¬†Test Cases is that we can only easily loop through one collection, but as Pester is just PowerShell we can simply use ForEach if we wanted to loop through multiple ones, like instances and then databases.\nI like to use the ForEach method as it is slightly quicker than other methods. It will only work with PowerShell version 4 and above. Below that version you need to pipe the collection to For-EachObject.\nLets write a test to see if our databases have trustworthy set on. We can do this using the Trustworthy property returned from Get-DbaDatabase\nWe loop through our Instances using the ForEach method and create a Context for each Instance to make the test results easier to read. We then place the call to Get-DbaDatabase inside braces and loop through those and check the Trustworthy property\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Get a list of SQL Servers # Use whichever method suits your situation # Maybe from a configuration database # I\u0026#39;m just using a hard-coded list for example $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; Describe \u0026#39;Testing user databases\u0026#39; { # Loop through the instances $Instances.ForEach{ # Create a Context for each Instance. Context \u0026#34;Testing User Databases on $($_)\u0026#34; { # Loop through the User databases on the instance (Get-DbaDatabase -SqlInstance $_ -ExcludeAllSystemDb).ForEach{ # Refer to the database name and Instance name inside a $() It \u0026#34;Database $($_.Name) on Instance $($_.Parent.Name) should not have TRUSTWORTHY ON\u0026#34; { $_.Trustworthy | Should Be $false } } } } } and it looks like this\nSo there you have two different ways to loop through collections in your Pester tests. Hopefully this can help you to write some good tests to validate your environment.\nHappy Pestering\nSpend a Whole Day With Chrissy \u0026amp; I at SQLBits If you would like to spend a whole day with Chrissy LeMaire and I at SQLBits¬†in London in February ‚Äì we have a pre-con on the Thursday\nYou can find out more about the pre-con¬†sqlps.io/bitsprecon\nand you can register at¬†sqlps.io/bitsreg\n","date":"2017-11-28T00:00:00Z","permalink":"https://blog.robsewell.com/blog/2-ways-to-loop-through-collections-in-pester/","title":"2 Ways to Loop through collections in Pester"},{"content":"In my last post I showed you how to write your first Pester test to validate something. Here‚Äôs a recap\nDecide the information you wish to test Understand how to get it with PowerShell Understand what makes it pass and what makes it fail Write a Pester Test You probably have more than one instance that you want to test, so how do you loop through a collection of instances? There are a couple of ways.\nGetting the Latest Version of the Module Steve Jones wrote about getting the latest version of Pester and the correct way to do it. You can find the important information here\nTest Cases The first way is to use the Test Case parameter of the It command (the test) which I have written about when using TDD for Pester here\nLets write a test first to check if we can successfully connect to a SQL Instance. Running\nFind-DbaCommand connection\nshows us that the Test-DbaConnection command is the one that we want from the dbatools module. We should always run Get-Help to understand how to use any PowerShell command. This shows us that the results will look like this\nSo there is a ConnectSuccess result which returns True or false. Our test can look like this for a single instance\n1 2 3 4 5 Describe \u0026#39;Testing connection to ROB-XPS\u0026#39; { It \u0026#34;Connects successfully to ROB-XPS\u0026#34; { (Test-DbaConnection-SqlInstance ROB-XPS).ConnectSuccess | Should Be $True } } which gives us some test results that look like this\nwhich is fine for one instance but we want to check many.\nWe need to gather the instances into a $Instances variable. In my examples I have hard coded a list of SQL Instances but you can, and probably should, use a more dynamic method, maybe the results of a query to a configuration database. Then we can fill our TestCases variable which can be done like this\n1 2 3 4 5 $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; # Create an empty array $TestCases = @() # Fill the Testcases with the values and a Name of Instance $Instances.ForEach{$TestCases += @{Instance = $_}} Then we can write our test like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Get a list of SQL Servers # Use whichever method suits your situation # Maybe from a configuration database # I\u0026#39;m just using a hard-coded list for example $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; # Create an empty array $TestCases = @() # Fill the Testcases with the values and a Name of Instance $Instances.ForEach{$TestCases += @{Instance = $_}} Describe \u0026#39;Testing connection to SQL Instances\u0026#39; { # Put the TestCases \u0026#39;Name\u0026#39; in \u0026lt;\u0026gt; and add the TestCases parameter It \u0026#34;Connects successfully to \u0026lt;Instance\u0026gt;\u0026#34; -TestCases $TestCases { # Add a Parameter to the test with the same name as the TestCases Name Param($Instance) # Write the test using the TestCases Name (Test-DbaConnection -SqlInstance $Instance).ConnectSuccess | Should Be $True } } Within the title of the test we refer to the instance inside \u0026lt;\u0026gt; and add the parameter TestCases with a value of the $TestCases variable. We also need to add a Param() to the test with the same name and then use that variable in the test.\nThis looks like this\nPester is PowerShell The problem with¬†Test Cases is that we can only easily loop through one collection, but as Pester is just PowerShell we can simply use ForEach if we wanted to loop through multiple ones, like instances and then databases.\nI like to use the ForEach method as it is slightly quicker than other methods. It will only work with PowerShell version 4 and above. Below that version you need to pipe the collection to For-EachObject.\nLets write a test to see if our databases have trustworthy set on. We can do this using the Trustworthy property returned from Get-DbaDatabase\nWe loop through our Instances using the ForEach method and create a Context for each Instance to make the test results easier to read. We then place the call to Get-DbaDatabase inside braces and loop through those and check the Trustworthy property\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Get a list of SQL Servers # Use whichever method suits your situation # Maybe from a configuration database # I\u0026#39;m just using a hard-coded list for example $Instances = \u0026#39;ROB-XPS\u0026#39;,\u0026#39;ROB-XPS\\DAVE\u0026#39;,\u0026#39;ROB-XPS\\BOLTON\u0026#39;,\u0026#39;ROB-XPS\\SQL2016\u0026#39; Describe \u0026#39;Testing user databases\u0026#39; { # Loop through the instances $Instances.ForEach{ # Create a Context for each Instance. Context \u0026#34;Testing User Databases on $($_)\u0026#34; { # Loop through the User databases on the instance (Get-DbaDatabase -SqlInstance $_ -ExcludeAllSystemDb).ForEach{ # Refer to the database name and Instance name inside a $() It \u0026#34;Database $($_.Name) on Instance $($_.Parent.Name) should not have TRUSTWORTHY ON\u0026#34; { $_.Trustworthy | Should Be $false } } } } } and it looks like this\nSo there you have two different ways to loop through collections in your Pester tests. Hopefully this can help you to write some good tests to validate your environment.\nHappy Pestering\nSpend a Whole Day With Chrissy \u0026amp; I at SQLBits If you would like to spend a whole day with Chrissy LeMaire and I at SQLBits¬†in London in February ‚Äì we have a pre-con on the Thursday\nYou can find out more about the pre-con¬†sqlps.io/bitsprecon\nand you can register at¬†sqlps.io/bitsreg\n","date":"2017-11-28T00:00:00Z","permalink":"https://blog.robsewell.com/blog/2-ways-to-loop-through-collections-in-pester/","title":"2 Ways to Loop through collections in Pester"},{"content":"I was in Glasgow this Friday enjoying the fantastic hospitality of the Glasgow SQL User Group @SQLGlasgow and presenting sessions with Andre Kamman, William Durkin, and Chrissy LeMaire.\nI presented ‚ÄúGreen is Good Red is Bad ‚Äì Turning your checklists into Pester Tests‚Äù. I had to make sure I had enough energy beforehand so I treated myself to a fabulous burger.\nAfterwards I was talking to some of the attendees and realised that maybe I could show how easy it was to start writing your first Pester test. Here are the steps to follow so that you can write your first Pester test:\nDecide the information you wish to test Understand how to get it with PowerShell Understand what makes it pass and what makes it fail Write a Pester Test The first bit is up to you. I cannot decide what you need to test for on your servers in your environments. Whatever is the most important. For now, pick one thing.\nLogins ‚Äì Let\u0026rsquo;s pick logins as an example for this post. It is good practice to disable the sa account (advice you‚Äôll read all over the internet and often written into estate documentation), so let‚Äôs write a test for that.\nNow we need the PowerShell command to return the information to test for. We need a command that will get information about logins on a SQL server and if it can return disabled logins then all the better.\nAs always when starting to use PowerShell with SQL Server I would start with dbatools. If we run Find-DbaCommand we can search for commands in the module that support logins. (If you have chosen something non-SQL Server related then you can use Get-Command or the internet to find the command you need.)\nGet-DbaLogin looks like the one that we want. Now we need to understand how to use it. Always, always use Get-Help to do this. If we run\n1 Get-Help Get-DbaLogins -detailed ","date":"2017-11-16T00:00:00Z","permalink":"https://blog.robsewell.com/blog/write-your-first-pester-test-today/","title":"Write Your first Pester Test Today"},{"content":" ","date":"2017-11-14T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-folks-who-have-made-a-difference/","title":"TSQL2sDay ‚Äì Folks Who Have Made a Difference"},{"content":"On the plane home from PASS Summit I was sat next to someone who had also attended and when he saw on my laptop that I was part of the SQL Community we struck up a conversation. He asked me how he could compare SQL Agent Jobs across availability group replicas to ensure that they were the same.\nHe already knew that he could use Copy-DbaAgentJob from dbatools to copy the jobs between replicas and we discussed how to set up an Agent job to accomplish this. The best way to run an Agent Job with a PowerShell script¬†is described here\nCompare-Object I told him about Compare-Object a function available in PowerShell for precisely this task. Take these two SQL instances and their respective Agent Jobs\nSo we can see that some jobs are the same and some are different. How can we quickly and easily spot the differences?\n$Default = Get-DbaAgentJob -SqlInstance rob-xps $bolton = Get-DbaAgentJob -SqlInstance rob-xps\\bolton Compare-Object $Default $bolton\nThose three lines of code will do it. The first two get the agent jobs from each instance and assign them to a variable and the last one compares them. This is the output\nThe arrows show that the first three jobs are only on the Bolton instance and the bottom three jobs are only on the default instance.\nWhat If ? Another option I showed was to use the -WhatIf switch on Copy-DbaAgentJob. This parameter is available on all good PowerShell functions and will describe what the command would do if run WARNING ‚Äì If you are using the old SQLPS module from prior to the SSMS 2016 release -WhatIf will actually run the commands so update your modules.\nWe can run\nCopy-DbaAgentJob -Source rob-xps -Destination rob-xps\\bolton -WhatIf\nand get the following result\nwhich shows us that there are two jobs on Rob-XPS which would be created on the Bolton instance\nAnd if they have been modified? Thats good he said, but what about if the jobs have been modified?\nWell one thing you could do is to compare the jobs DateLastModified property by using the -Property parameter and the passthru switch\n$Default = Get-DbaAgentJob -SqlInstance rob-xps $Dave = Get-DbaAgentJob -SqlInstance rob-xps\\dave $Difference = Compare-Object $Default $dave -Property DateLastModified -PassThru $Difference | Sort-Object Name | Select-Object OriginatingServer,Name,DateLastModified\nThis is going to return the jobs which are the same but were modified at a different time\nso that you can examine when they were changed. Of course the problem with that is that the DateLastModified is a very precise time so it is pretty much always going to be different. We can fix that but now it is a little more complex.\nJust the Date please We need to gather the jobs in the same way but create an array of custom objects with a calculated property like this\n$Dave = Get-DbaAgentJob -SqlInstance rob-xps\\dave ## Create a custom object array with the date instead of the datetime $DaveJobs = @() $Dave.ForEach{ $DaveJobs += [pscustomobject]@{ Server = $.OriginatingServer Name = $.Name Date = $_.DateLastModified.Date } }\nand then we can compare on the Date field. The full code is\n## Get the Agent Jobs $Default = Get-DbaAgentJob -SqlInstance rob-xps $Dave = Get-DbaAgentJob -SqlInstance rob-xps\\dave ## Create a custom object array with the date instead of the datetime $DaveJobs = @() $Dave.ForEach{ $DaveJobs += [pscustomobject]@{ Server = $.OriginatingServer Name = $.Name Date = $.DateLastModified.Date } } ## Create a custom object array with the date instead of the datetime $DefaultJobs = @() $Default.ForEach{ $DefaultJobs += [pscustomobject]@{ Server = $.OriginatingServer Name = $.Name Date = $.DateLastModified.Date } } ## Perform a comparison $Difference = Compare-Object $DefaultJobs $DaveJobs -Property date -PassThru ## Sort by name and display $Difference | Sort-Object Name | Select-Object Server, Name, Date\nThis will look like this\nWhich is much better and hopefully more useful but it only works with 2 instances\nI have more than 2 instances So if we have more than 2 instances it gets a little more complicated as Compare-Object only supports two arrays. I threw together a quick function to compare each instance with the main instance. This is very rough and will work for now but I have also created a feature request issue on the dbatools repository so someone (maybe you ?? ) could go and help create those commands\nFunctionCompare-AgentJobs { Param( $SQLInstances ) ## remove jobs* variables from process Get-Variable jobs*|Remove-Variable ## Get the number of instances $count = $SQLInstances.Count ## Loop through instances $SQLInstances.ForEach{ # Get the jobs and assign to a new dynamic variable $Number = [array]::IndexOf($SQLInstances, $) $Job = Get-DbaAgentJob-SqlInstance $ New-Variable-Name \u0026ldquo;Jobs$Number\u0026rdquo;-Value $Job } $i = $count - 1 $Primary = $SQLInstances[0] While ($i -gt 0) { ## Compare the jobs with Primary $Compare = $SQLInstances[$i] Write-Output\u0026quot;Comparing $Primary with $Compare \u0026quot; Compare-Object(Get-Variable Jobs0).Value (Get-Variable\u0026quot;Jobs$i\u0026quot;).Value $i \u0026ndash; } }\nwhich looks like this. It‚Äôs not perfect but it will do for now until the proper commands are created\n","date":"2017-11-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/comparing-agent-jobs-across-availability-group-replicas-with-powershell/","title":"Comparing Agent Jobs across Availability Group Replicas with PowerShell"},{"content":" ","date":"2017-11-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-plaster-to-create-a-new-powershell-module/","title":"Using Plaster To Create a New PowerShell Module"},{"content":" $srv.Query($Query)\n$srv.Query($Query).column1\n","date":"2017-11-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/dbatools-with-sql-on-docker-and-running-sql-queries/","title":"dbatools with SQL on Docker and running SQL queries"},{"content":" ","date":"2017-10-29T00:00:00Z","permalink":"https://blog.robsewell.com/blog/power-bi/powershell/a-pretty-powerbi-pester-results-template-file/","title":"A Pretty PowerBi Pester Results Template File"},{"content":" ","date":"2017-09-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-get-postroundup/","title":"#TSQL2sDay ‚Äì Get-PostRoundup"},{"content":" ","date":"2017-09-12T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-starting-out-with-powershell/","title":"#TSQL2sDay ‚Äì Starting Out with PowerShell"},{"content":" [version]$Version = [regex]::matches($file, \u0026ldquo;\\sModuleVersion\\s=\\s\u0026rsquo;(\\d.\\d*.\\d*)\u0026rsquo;\\s*\u0026rdquo;).groups[1].value\nUse RegEx to get the Version Number and set it as a version datatype \\s* - between 0 and many whitespace ModuleVersion - literal \\s - 1 whitespace = - literal \\s - 1 whitespace \u0026rsquo; - literal () - capture Group \\d* - between 0 and many digits \u0026rsquo; - literal \\s* between 0 and many whitespace [version]$Version = [regex]::matches($file, \u0026ldquo;\\sModuleVersion\\s=\\s\u0026rsquo;(\\d.\\d*.\\d*)\u0026rsquo;\\s*\u0026rdquo;).groups[1].value Write-Output \u0026ldquo;Old Version - $Version\u0026rdquo;\nAdd one to the build of the version number [version]$NewVersion = \u0026ldquo;{0}.{1}.{2}\u0026rdquo; -f $Version.Major, $Version.Minor, ($Version.Build + 1) Write-Output \u0026ldquo;New Version - $NewVersion\u0026rdquo;\nReplace Old Version Number with New Version number in the file try { (Get-Content .\\BeardAnalysis.psd1) -replace $version, $NewVersion | Out-File .\\BeardAnalysis.psd1 Write-Output \u0026ldquo;Updated Module Version from $Version to $NewVersion\u0026rdquo; } catch { $_ Write-Error \u0026ldquo;failed to set file\u0026rdquo; } ","date":"2017-09-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/automatically-updating-the-version-number-in-a-powershell-module-how-i-do-regex/","title":"Automatically updating the version number in a PowerShell Module ‚Äì How I do regex"},{"content":"Write-Output \u0026ldquo;What are you going to automate today?\u0026rdquo;\nWelcome to T-SQL Tuesday for September 2017!\nT-SQL Tuesday is a chance for you to join in the SQL Server community and write a blog post on a suggested topic. It makes for a great way to find a bunch of blog posts showing the same subject from many different viewpoints. Please join in and write a blog post, maybe it\u0026rsquo;s your first ever, maybe you haven\u0026rsquo;t blogged for a while but even if you blog every day come and join the party and share your knowledge.\nTo participate:\nWrite a post on the topic below Schedule the post to go live on Tuesday, September 12th (between zero am and midnight, UTC) Include the TSQL Tuesday logo in the top of your post Link the post back to this one (it‚Äôs easier if you comment on this post and link it) Optional: Tweet a link to your post using the #tsql2sday hash tag on Twitter Extra credit: if you‚Äôd like to host your own TSQL Tuesday in the future, read the full rules for info on how to sign up. Just like I did but don\u0026rsquo;t forget its your month!!\nThis month‚Äôs topic: Let\u0026rsquo;s get all Posh - What are you going to automate today? It is no surprise to those that know me that I will choose PowerShell as the topic for this month. I am passionate about PowerShell because it has enabled me to have the career I have today and to visit numerous countries all around the world, meet people and talk about PowerShell. By my reckoning searching the TSQL Tuesday website it has been over 3 years since we had a topic specific to PowerShell. So I would like you to blog about PowerShell and SQL Server (or other interesting data platform products)\nIf you don\u0026rsquo;t know or use PowerShell GREAT! That\u0026rsquo;s awesome.\nPlease spend an hour or so with it and tell us how you got on and what and how you learned. Just like Erik and Brent did. You could install one of the community modules like dbatools, dbareports , SQLDiagAPI¬†or the Microsoft ones sqlserver or SSRS and try them out and tell us what you learned.\nIf you want help whilst doing this please make use of the #PowerShellhelp channel in the SQL Server Community Slack\nThis will be of so much benefit to all people who don\u0026rsquo;t use PowerShell and want to start to learn about it.\nIf you do use PowerShell and SQL then either tell the tale of the best thing you have automated or a beginners post to show people how to start using PowerShell. I have heard many stories and am looking forward to tales of\ntesting backups doing migrations resetting log shipping creating things in the cloud and on premises SQL on Linux with PowerShell on Linux using Pester for testing automating manual tasks automating incident knowledge gathering continuous integration and delivery and many more. I will read all of them and do a write up of them later next week.\nInvoke-Coffee\nStart-BlogWriting -Title \u0026lsquo;Cool PowerShell Post\u0026rsquo;\nGet-BlogProofRead\nPost-Blog -Date ‚ÄòSeptember 12th 2017‚Äô -Title \u0026lsquo;Cool PowerShell Post\u0026rsquo;\nWrite-Tweet -Hashtag ‚ÄòTSQL2sday‚Äô -Message \u0026lsquo;This is my cool blogpost\u0026rsquo;\n","date":"2017-09-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-94-lets-get-all-posh/","title":"#TSQL2sday #94 Lets get all Posh!"},{"content":" ","date":"2017-08-13T00:00:00Z","permalink":"https://blog.robsewell.com/blog/psday.uk-tickets-are-on-sale/","title":"PSDay.UK Tickets are on sale"},{"content":" ","date":"2017-08-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-powershell-to-check-if-your-password-has-been-in-a-breach/","title":"Using PowerShell to check if your password has been in a breach"},{"content":"\nThis month‚Äôs¬†T-SQL Tuesday is hosted by Kendra Little and is on the topic of interviews¬†I hate interviews as an interviewee. I have had many memorable experiences with them. Even writing this blog post has been challenging as I relive some of them. I haven‚Äôt shared a lot of the worst ones!\nWhen I was a lad My first interview was for a waiter/washer up at a local country pub aged 15 or so. I stumbled and stammered and stuttered my way through and I think it was only because they needed someone that evening that I got the job.\nWhen I was 17 I wanted a car and whilst doing my A-Levels I got a job at a local private school to achieve this. It was about a 10 minute cycle from my college to the school and I was so nervous that about half-way I was incapable of riding my bike and had to walk. I then was faced with two people interviewing me which I was not expecting or prepared for. I remember nothing of the interview other than leaving it soaking in sweat.\nNerves As a young man I interviewed for numerous jobs and things got progressively worse for me. I would get so incredibly nervous. I could not sleep or eat before an interview. This meant I often was feeling very nauseous and tired during an interview. I would arrive incredibly early and have to waste time wandering around, giving me more time to think about how nervous I was and, of course, making it worse. Obviously I wouldn‚Äôt give a good impression and didn‚Äôt get the jobs which meant more interviews and more nerves.\nI tried many things, I sought advice and information from many sources and approached the situation in a number of different ways without much change to my internal responses.\nPreparation, Practice, Knowledge and Distraction To this day I hate interviews even after 20 years of having to do them. I can still get so overcome by nerves that I forget even the simplest and most obvious things such as what the N in DNS stands for or what the question is that I should be answering.\nTo minimise this, I try my best to prepare as well as I possibly can. I learn and revise the things I think I will need to show that I know by reading the job descriptions and adverts carefully.\nI also split the whole process into separate boxes. Revising and researching a company and a job was one part. Getting ready and travelling and arriving was another and the actual interview was then just the last part consisting of talking to some people. This definitely reduced the overall stress and improved my performance in interviews.\nA previous shop provided interviewer training and needed volunteers to be interviewed for those courses. I volunteered as often as I was able to and treated them as realistically as was feasible. This helped me a lot and also enabled me (sometimes) to view an interview as just a chat. If you suffer with nerves and this is available I would recommend doing so. If not, ask someone who interviews for some practice interviews and treat them as realistically as you feel is necessary.\nA wise person told me to remember that interviewers are people too and also that good interviewers will recognise nerves and assist the interviewee. After all, they are trying to find the right person to fulfil their needs and want to know if you meet their requirements for that position.\nAnother wise person told me, during an interview, that it was ok to ask for clarification about a question. A decade or more of interviews before I knew that. It enables me to pull back from a spiral of nerves making me gabble and to be able to return to the question required. When I find that I am rambling in my answer or that the answer has disappeared from my mind I ask the interviewer for clarification and get some much needed breathing space.\nTo reduce the impact of nervousness before the interview I learnt to distract myself in the couple of hours prior to an interview. I have been known to go and see a film if a cinema is close to the interview or do the weekly shopping. Anything that can occupy my mind without risking me being late. This may be of no use to many people but it works for me.\nThe other side of the table As an interviewer, I hope that I recognise when people are nervous and am able to assist them and also coax out the information that I need to be able to make the best decision about the candidate for the position.\nInterviewing is tiring.\nWhen I worked in secure units we would sometimes spend 2 continuous days interviewing. It is hard work. You need to look after yourself in these situations. It is important to drink plenty of water, to ensure that you eat and at least get up and stretch in between interviews. The very last person you interview might be the perfect candidate don‚Äôt miss that because you have switched off.\nYou are being interviewed too The person that you are interviewing is also interviewing you. They are considering if they want to come and work for your company with the people they meet. That might only be the people in the interview so it is important I think to ensure that you make a good impression as well. During a day of interviewing many candidates try to reset before each person.\nBeing courteous, attentive and professional is important during the interview even if the interviewers recognise that the person is not suitable for that role within 2 minutes. They may be ideal for another position or you may come across them later in your career. Leave a good impression.\nPreparation A shop I worked at employed a new DBA who had impressed the manager in interview with their knowledge as they had passed a lot of exams. The manager was very pleased and looking forward to the new arrival. This changed quite quickly when it became obvious that the new DBA was missing some basic knowledge about installing SQL Server and creating new databases which was a significant part of their role. A lot of time was wasted by the other DBAs in the team re-doing and re-checking the work that this person had done and team dynamics went downhill very quickly (although I did learn a lot about Policy Based Management from this experience!)\nWhen I was working in secure units focusing on people with Autism we knew that communication skills both verbal and non-verbal were vital to all members of our team. We had an excellent set of questions and scenarios early in the interview to establish peoples capabilities in these areas and this allowed us to close off interviews early when we could see that the candidate did not meet our requirements as well as ensuring we employed people with the right skills for a very challenging workplace.\nBefore the interviews for a replacement DBA the manager asked how to avoid a repeat of that situation. I described the situation above and as a team we identified the basic skills, knowledge and approaches that we wanted in our future team members and designed a set of questions and scenarios so that candidates could demonstrate them. This was excellent for ensuring the entire team felt that they had some input into the recruiting process and also added confidence in the new team member. I think it was an excellent piece of team management as well.\nThe biggest take away from this post, I hope, is preparation. For both sides of the table preparation is a vital part of any interview process. Also if you see me all dressed up and in the queue for a film I am probably very nervous and won‚Äôt want to chat!!\nMake sure that you go and visit the round-up post that Kendra posts on her blog to read further posts on the interviewing process from others in the SQL Community. You can also find all the archives at¬†http://tsqltuesday.com/\nAny resemblance to any living people in this post apart from myself is complete co-incidence\n","date":"2017-08-08T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/08/tsql2sday.jpg","permalink":"https://blog.robsewell.com/blog/i-hate-interviews-tsql2sday/","title":"I Hate Interviews ‚Äì TSQL2sDay"},{"content":" ","date":"2017-08-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/presentation-nerves/","title":"Presentation Nerves"},{"content":"On Thursday evening I attended the joint London WinOps and PowerShell User Group. It was an excellent evening with two great sessions by Jaap Brasser and Filip Verloy.\nPSDay.UK There was also an exciting announcement about PSDay.UK¬†https://psday.uk\nLook look\nA whole day of PowerShell\nIn London\nJust after @WinOpsLDN\nIt\u0026rsquo;s going to be awesomehttps://t.co/CAQqoc2cgX\nFollow @psdayuk pic.twitter.com/3RFHpXpRI1\n‚Äî Rob Sewell (@sqldbawithbeard) July 20, 2017\nPSDay.UK is a one day PowerShell event providing the opportunity for you to spend a whole day learning PowerShell from renowned experts from the UK and international speaking community. It will be held at\nSkills Matter | CodeNode, 10 South Place, London, EC2M 7EB, GB\non\nFriday 22nd September 2017¬†.ics\nWe will be running two tracks\nPowerShell Zero to Hero DevOps with PowerShell Register your interest Please go and visit the website and have a look and register your interest to get further notifications about the event.\nFollow the @PSDayUK twitter account and Facebook page https://www.facebook.com/PSDayUK/ and keep yourself informed on this fantastic new event.\nWant to Speak at PSDay.UK ? We already have some fantastic speakers lined up but we would like to invite people to send us submissions for more sessions. If you have a PowerShell talk that will fit into one of the tracks and experience of delivering sessions at events please send us submissions via the website.\nIf you have questions about speaking feel free to contact me via twitter at @sqldbawithbeard\nWhat is a PSDay ? The International PowerShell community has three main global events which run over a number of days with top notch international speakers and Microsoft PowerShell team members, delivering in-depth information about the latest PowerShell trends and technologies, and connecting national communities with another.\npsconf.eu covers the European communities, Hannover, Germany April 16-20 2018 psconf.asia targets the Asian communities, Singapore October 27-28 2017 PowerShell+DevOps Global Summit targets the US communities April 9-12 2018 There are a number of other PowerShell events that have been organised by wonderful volunteers in numerous countries and we feel there is an opportunity to create national events which complement the global events and help PowerShell passionates and professionals to get in touch and learn from another with a similar branding of PSDay.\nWe foresee PSDays to be smaller one day national events promoting speakers from the host country supported by other international speakers with the aim of increasing the exposure of national PowerShell user groups as well as providing excellent PowerShell training.\nThere will be a board of PowerShell community folk set up who will approve requests to use the PSDay name and shield logo providing the event is professionally organized and offer help with technical questions, viral marketing, and experience. We hope that this will enable people to set up their own PSDay in their own country and increase the exposure of the PowerShell community as well as PowerShell knowledge whilst sharing resources, knowledge, experience and skills and ensuring a good standard of PowerShell community national events.\nFurther details of this will be forthcoming and we welcome offers of assistance from people with relevant experience\n","date":"2017-07-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/announcing-psday.uk-whats-a-psday/","title":"Announcing PSDay.UK ‚Äì Whats a PSDay?"},{"content":" ","date":"2017-07-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/writing-dynamic-and-random-tests-cases-for-pester/","title":"Writing Dynamic and Random Tests Cases for Pester"},{"content":"The SQL Server Diagnostics Preview was announced just over a week ago It includes an add-on for SQL Server Management Studio to enable you to analyse SQL Server memory dumps and view information on the latest SQL Server cumulative updates for supported versions of SQL Server. Arun Sirpal has written a good blog post showing how to install it and use it in SSMS to analyse dumps.\nThere is also a developer API available so I thought I would write some PowerShell to consume it as there are no PowerShell code examples available in the documentation!\nIn a previous post I have explained how I created the module and a GitHub repository and used Pester to help me to develop the first command Get-SQLDIagRecommendations. At present the module has 5 commands, all for accessing the Recommendations API.\nThis post is about the command Get-SQLDiagFix which returns the Product Name, Feature Name/Area, KB Number, Title and URL for the Fixes in the Cumulative Updates returned from the SQL Server Diagnostics Recommendations API.\nPowerShell Gallery The module is available on the PowerShell Gallery which means that you can install it using\nInstall-Module SQLDiagAPI\nas long as you have the latest version of the PowerShellGet module. This is already installed in Windows 10 and with WMF 5 but you can install it on the following systems\nWindows 8.1 Pro Windows 8.1 Enterprise Windows 7 SP1 Windows Server 2016 TP5 Windows Server 2012 R2 Windows Server 2008 R2 SP1 following the instructions here.\nIf you are not running your PowerShell using a local administrator account you will need to run\nInstall-Module SQLDiagAPI -Scope CurrentUser\nto install the module.\nIf you can‚Äôt use the PowerShell Gallery you can install it using the instructions on the repository\nAPI Key To use the API you need an API Key. An API Key is a secret token that identifies the application to the API and is used to control access. You can follow the instructions here to get one for the SQL Server Diagnostics API.\nYou will need to store the key to use it. I recommend saving the API Key using the Export-CliXML command as described by Jaap Brasser here .\nGet-Credential | Export-CliXml -Path \u0026quot;${env:\\userprofile}\\SQLDiag.Cred\u0026quot;\nYou need to enter a username even though it is not used and then enter the API Key as the password. It is saved in the root of the user profile folder as hopefully, user accounts will have access there in most shops.\nThis will save you from having to enter the APIKey every time you run the commands as the code is looking for it to be saved in that file.\nThe Commands Once you have installed the module and the APIKey it will be available whenever you start PowerShell. The first time you install you¬†may need to run\nImport-Module SQLDiagAPI\nto load it into your session. Once it is loaded you can view the available commands using\nGet-Command -Module SQLDiagAPI\nYou can find out more about the commands on the GitHub Repository¬†and the Help files are in the documentation.\nGet-Help Always, always when starting with a new module or function in PowerShell you should start with Get-Help. I like to use the -ShowWindow parameter to open the help in a separate window as it has all of the help and a handy search box.\nGet-Help Get-SQLDiagFix\nGood help should always include plenty of examples to show people how to use the command. There are 12 examples in the help for Get-SQLDiagFix. You can view just the examples using\nGet-Help Get-SQLDiagFix -examples\nGet All Of The Fixes The easiest thing to do is to get all of the available fixes from the API. This is done using\nGet-SQLDiagFix\nwhich will return all 123 Fixes currently referenced in the API.\nThat is just a lot of information on the screen. If we want to search through that with PowerShell we can use Out-GridView\nGet-SQLDiagFix | Select Product, Feature, KB, Title | Out-GridView\nOr maybe if you want to put them in a database you could use dbatools\n$Fixes = Get-SQLDiagFix | Out-DbaDataTable\rWrite-DbaDataTable -SqlServer $Server -Database $DB -InputObject $Fixes -Table Fixes -AutoCreateTable\nGet Fixes for a Product If you only want to see the fixes for a particular product you can use the product parameter. To see all of the products available in the API you can run\nGet-SQLDiagProduct\nYou can either specify the product\nGet-SQLDiagFix -Product 'SQL Server 2016 SP1' | Format-Table\nor you can pipe the results of Get-SQLDiagProduct to Get-SQLDiagFix which enables you to search. For example, to search for all fixes for SQL Server 2014 you can do\nGet-SQLDiagProduct 2014 | Get-SQLDiagFix | Format-Table -AutoSize\nWhich will show the fixes available in the API for SQL Server 2014 SP1 and SQL Server 2014 SP2\nGet The Fixes for A Feature The fixes in the API are also categorised by feature area. You can see all of the feature areas using Get-SQLDiagFeature\nGet-SQLDiagFeature\nYou can see the fixes in a particular feature area using the Feature parameter with\nGet-SQLDiagFix -Feature Spatial | Format-Table -AutoSize\nor you can search for a feature with a name like query and show the fixes using\nGet-SQLDiagFix -Feature (Get-SQLDiagFeature query) | Format-Table -AutoSize\nGet Fixes for a Product and a Feature You can combine the two approaches above to search for fixes by product and feature area. If you want to see the fixes for SQL Server 2016¬†to do with backups you can use\nGet-SQLDiagProduct 2016 | Get-SQLDiagFix -Feature (Get-SQLDiagFeature backup) | Format-Table -AutoSize\nNo-one wants to see the words ‚Äú‚Ä¶restore fails when‚Ä¶.‚Äù! This is probably a good time to fix that.\nOpen the KB Article Web-Page As well as getting the title and KB number of the fix, you can open the web-page. This code will open the fixes for all SP1 products in the feature area like al in Out-GridView and enable you to choose one (or more) and open them in your default browser\nGet-SQLDiagProduct SP1 | Get-SQLDiagFix -Feature (Get-SQLDiagFeature -Feature al) | Out-GridView -PassThru | ForEach-Object {Start-Process $_.URL}`\nThere is a YouTube video as well showing how to use the command\nYou can find the GitHub repository at¬†https://github.com/SQLDBAWithABeard/SQLDiagAPI\n","date":"2017-07-04T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-get-sqldiagfix-to-get-information-from-the-sql-server-diagnostic-api-with-powershell/","title":"Using Get-SQLDiagFix to get information from the SQL Server Diagnostic API with PowerShell"},{"content":" ","date":"2017-06-30T00:00:00Z","permalink":"https://blog.robsewell.com/blog/creating-a-powershell-module-and-tdd-for-get-sqldiagrecommendations/","title":"Creating a PowerShell Module and TDD for Get-SQLDiagRecommendations"},{"content":" ","date":"2017-06-29T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershell-module-for-the-sql-server-diagnostics-api-1st-command-get-sqldiagrecommendations/","title":"PowerShell Module for the SQL Server Diagnostics API ‚Äì 1st Command Get-SQLDiagRecommendations"},{"content":" ","date":"2017-06-22T00:00:00Z","permalink":"https://blog.robsewell.com/blog/vscode-powershell-extension-1.4.0-new-command-out-currentfile/","title":"VSCode ‚Äì PowerShell extension 1.4.0 new command Out-CurrentFile"},{"content":"This weekend¬†SQL Saturday Dublin occurred. For those that don‚Äôt know SQL Saturdays are free conferences with local and international speakers providing great sessions in the Data Platform sphere.\nChrissy LeMaire and I presented our session PowerShell SQL Server: Modern Database Administration with dbatools. You can find slides and code here . We were absolutely delighted to be named Best Speaker which was decided from the attendees average evaluation.\nWow, @cl and i won the coveted best speaker award at #SqlSatDublin Thank you so much. We are so pleased pic.twitter.com/f0MPTJf74p\n‚Äî Rob Sewell (@sqldbawithbeard) June 17, 2017\nChrissy also won the Best Lightning talk for her¬†5 minute (technically 4 minutes and 55 seconds)¬†presentation on dbatools as well üôÇ\nLook at how chuffed @cl is at winning the Lightning Talk award as well at #SqlSatDublin üòÜüòÜ pic.twitter.com/KN9CVtYnHa\n‚Äî Rob Sewell (@sqldbawithbeard) June 17, 2017\nWe thoroughly enjoy giving this presentation and I think it shows in the feedback we received.\nHistory We start with a little history of dbatools, how it started as one megalithic script Start-SQLMigration.ps1 and has evolved into (this number grows so often it is probably wrong by the time you read this) over 240 commands from 60 contributors\nRequirements We explain the requirements. You can see them here on the download page.\nThe minimum requirements for the Client are\nPowerShell v3 SSMS / SMO 2008 R2 which we hope will cover a significant majority of peoples workstations.\nThe minimum requirements for the SQL Server are\nSQL Server 2000 No PowerShell for pure SQL commands PowerShell v2 for Windows commands Remote PowerShell enabled for Windows commands As you can see the SQL server does not even need to have PowerShell installed (unless you want to use the Windows commands). We test our commands thoroughly using a test estate that encompasses all versions of SQL from 2000 through to 2017 and whenever there is a vNext available we will test against that too.\nWe recommend though that you are using PowerShell v5.1 with SSMS or SMO for SQL 2016 on the client\nInstallation We love how easy and simple the installation of dbatools is. As long as you have access to the internet (and permission from your companies security team to install 3rd party tools. Please don‚Äôt break your companies policies) you can simply install the module from the PowerShell Gallery using\nInstall-Module dbatools\nIf you are not a local administrator on your machine you can use the -Scope parameter\nInstall-Module dbatools -Scope CurrentUser\nIncidentally, if you or your security team have concerns about the quality or trust of the content in the PowerShell Gallery please read this post which explains the steps that are taken when code is uploaded.\nIf you cannot use the PowerShell Gallery then you can use this line of code to install from GitHub\nInvoke-Expression (Invoke-WebRequest https://dbatools.io/in)\nThere is a video on the¬†download page showing the installation on a Windows 7 machine and also some other methods of installing the module should you need them.\nWebsite Next we visit the website dbatools.io¬†and look at the front page. We have our regular joke about how Chrissy doesn‚Äôt want to present on migrations but I think they are so cool so she makes me perform the commentary on the video. (Don‚Äôt tell anyone but it also helps us to get in as many of the 240+ commands in a one hour session as well üòâ ). You can watch the video on the front page.¬†You definitely should as you have never seen migrations performed so easily.\nThen we talk about the comments we have received from well respected people from both SQL and PowerShell community members so you can trust that its not just some girl with hair and some bloke with a beard saying that its awesome.\nContributors Probably my¬†favourite page on the web-site is the team page showing all of the amazing fabulous wonderful people who have given their own time freely to make such a fantastic free tool. If we have contributors in the audience we do try to point them out. One of our aims with dbatools is to enable people to receive the recognition for the hard work that they put in and we do this via the team page, our LinkedIn company page¬†as well as¬†by linking back to the contributors in the help and the web-page for every command. I wish I could name check each one of you.\nThank You each and every one !!\nFinding Commands We then look at the¬†command page¬†and the new improved search page and demonstrate how you can use them to find information about the commands that you need and the challenges in keeping this all maintained during a period of such rapid expansion.\nDemo Then it is time for me to say this phrase. ‚ÄúStrap yourselves in, do up your seatbelts, now we are going to show 240 commands in the next 40 minutes. Are you ready!!‚Äù\nOf course, I am joking, one of the hardest things about doing a one hour presentation on dbatools¬†is the sheer number of commands that we have that we want to show off. Of course we have already shown some of them in the migration video above but we still have a lot more to show and there are a lot more that we wish we had time to show.\nBackup and Restore We start with a restore of one database¬†and¬†a single backup¬†file using Restore-DbaDatabase¬†showing you the easy to read warning that you get if the database already exists and then how to¬†resolve that warning¬†with the WithReplace switch\nThen how to use it to restore an entire instance worth of backups to the latest available time¬†by pointing¬†Restore-DbaDatabase at a folder on a¬†share\nThen how to use Get-DbaDatabase¬†to get all of the databases on an instance and pass them to Backup-DbaDatabase¬†to back up an entire instance.\nWe look at the Backup history of some databases using Get-DbaBackupHistory¬†and¬†Out-GridView¬†and examine detailed information about a backup file¬†using Read-DbaBackupHeader.\nWe give thanks to Stuart Moore for his amazing work on these and several other backup and restore commands.\nSPN‚Äôs After a quick reminder that you can search for commands at the command line using Find-DbaCommand,¬†we talk about SPNs and try to find someone, anyone, who actually likes working with SQL Server and SPNs and resolving the issues!!\nThen we show Drew‚Äôs SPN commands Get-DbaSpn, Test-DbaSpn, Set-DbaSpn¬†and Remove-DbaSpn¬†Holiday Tasks We then¬†talk about the things we ensure we run before going on holiday to make sure we leave with a warm fuzzy feeling that everything will be ok until we return :-\nGet-DbaLastBackup will show the last time the database had any type of backup.\nGet-DbaLastGoodCheckDb which shows the last time that a database had a successful DBCC CheckDb and how we can gather the information for all the databases on all of your instances in just one line of code\nGet-DbaDiskSpace¬†which will show the disk information for all of the drives including mount points and whether the disk is in use by SQL\nTesting Your Backup Files By Restoring Them We ask how many people test their backup files¬†every single day and Dublin wins marks for a larger percentage than some other places we have given this talk. We show Test-DbaLastBackup¬†in action so that you can see the files being created because we think it looks cool (and you can see the filenames!) Chrissy has written a great post about how you can set up your own dedicated backup file test server\nFree Space We show how to gather the file space information using Get-DbaDatabaseFreespace¬†and then how you can put that (or the results of any PowerShell command) into a SQL database table using Out-DbaDataTable¬†and Write-DbaDataTable\nSQL Community Next we talk about how we love to take community members blog posts and turn them into dbatools commands.\nWe start with Jonathan Kehayias‚Äôs post about SQL Server Max memory (http://bit.ly/sqlmemcalc) and show Get-DbaMaxMemory¬†, Test-DbaMaxMemory¬†and Set-DbaMaxMemory\nThen¬†Paul Randal‚Äôs blog post about Pseudo-Simple Mode¬†which inspired¬†Test-DbaFullRecoveryModel\nWe talked about getting backup history earlier but now we talk about Get-DbaRestoreHistory¬†a command inspired by Kenneth Fishers blog post to show when a database was restored and which file was used.\nNext a command from Thomas LaRock¬†which he gave us for testing linked servers Test-DbaLinkedServerConnection.\nGlenn Berrys diagnostic information queries¬†are available thanks to Andr√© Kamman and the commands Invoke-DbaDiagnosticQuery and Export-DbaDiagnosticQuery. The second one will output all of the results to csv files.\nAdam Mechanic‚Äôs sp_whoisactive is a common tool in SQL DBA‚Äôs toolkit and can now be installed using Install-DbaWhoIsActive and run using Invoke-DbaWhoIsActive.\nAwesome Contributor Commands Then we try to fit in as many commands that we can from our fantastic contributors showing how we can do awesome things with just one line of PowerShell code\nOne line of code! Excellent \u0026lsquo;Poweshell ‚Äì SQL Server: Modern Database Administration\u0026rsquo; presentation by @cl @sqldbawithbeard #SqlSatDublin pic.twitter.com/TOIsi0FgAA\n‚Äî Xavi Arnau (@xavidublin) June 17, 2017\nThe awesome Find-DbaStoredProcedure¬†which you can read more about here¬†which in tests searched 37,545 stored procedures on 9 instances in under 9 seconds for a particular string.\nFind-DbaOrphanedFile which enables you to identify the files left over from detaching databases.\nDon‚Äôt know the SQL Admin password for an instance? Reset-SqlAdmin can help you.\nIt is normally somewhere around here that we finish and even though we have shown 32 commands (and a few more encapsulated in the Start-SqlMigration command)¬†that is less than 15% of the total number of commands in the module!!!\nSomehow, we always manage¬†to fit all of that into 60 minutes and have great fun doing it. Thank you to everyone who has come and seen our sessions in Vienna, Utrecht, PASS PowerShell Virtual Group, Hanover, Antwerp and Dublin.\nMore So you want to know more about dbatools¬†? You can click the link and explore the website\nYou can look at source code on GitHub\nYou can join us in the SQL Community Slack in the #dbatools channel\nYou can watch videos on YouTube\nYou can see a list of all of the presentations¬†and get a lot of the slides and demos\nIf you want to see the slides and demos from our Dublin presentation you can find them here\nVolunteers Lastly and most importantly of all. SQL Saturdays are run by volunteers so massive thanks to Bob, Carmel, Ben and the rest of the team who ensured that¬†SQL Saturday Dublin¬†went so very smoothly\nMassive thanks to the volunteers of #SqlSatDublin pic.twitter.com/veE0UuQxeO\n‚Äî Shane O\u0026rsquo;Neill (@SOZDBA) June 17, 2017\n","date":"2017-06-21T00:00:00Z","permalink":"https://blog.robsewell.com/blog/dbatools-at-%23sqlsatdublin/","title":"dbatools at #SQLSatDublin"},{"content":" ","date":"2017-06-12T00:00:00Z","permalink":"https://blog.robsewell.com/blog/vs-code-automatic-dynamic-powershell-help/","title":"VS Code ‚Äì Automatic Dynamic PowerShell Help"},{"content":"Whilst I was at PSCONFEU I presented a session on writing pester tests instead of using checklists. You can see it here.\nDuring the talk I showed the pester test that I use to make sure that everything is ready for my presentation. A couple of people have asked me about this and wanted to know more so I thought that I would blog about it.\nSome have said that I might be being a little OCD about it üòâ I agree that it could seem like that but there is nothing worse than having things go wrong during your presentation. It makes your heart beat faster and removes the emphasis from the presentation that you give.\nWhen it is things that you as a presenter could have been able to foresee, like a VM not being started or a database not being restored to the pre-demo state or being logged in as the wrong user then it is much worse.\nI use Pester to ensure that my environment for my presentation is as I expect and in fact, in Hanover when I ran through my Pester test for my NUC environment I found that one of my SQL Servers had decided to be in a different time zone and therefore the SQL Service would not authenticate and start. I was able to quickly remove the references to that server and save myself from a sea of red during my demos.\nFor those that don‚Äôt know, Pester is a PowerShell module for Test Driven Development.\nPester provides a framework for running unit tests to execute and validate PowerShell commands from within PowerShell. Pester consists of a simple set of functions that expose a testing domain-specific language (DSL) for isolating, running, evaluating and reporting the results of PowerShell commands.\nIf you have PowerShell version 5 then you will have Pester already installed, although you should update it to the latest version. If not, you can get Pester from the PowerShell Gallery‚Äîfollow the instructions on that page to install it. This is a good post to start learning about Pester.\nWhat can you test? Everything. Well, specifically everything that you can write a PowerShell command to check. So when I am setting up for my presentation I check the following things. I add new things to my tests as I think of them or as I observe things that may break my presentations. Most recently that was ensuring that my Visual Studio Code session was running under the correct user. I did that like this:\n1 2 3 4 5 6 7 Describe \u0026#34;Presentation Test\u0026#34; { Context \u0026#34;VSCode\u0026#34; { It \u0026#34;Should be using the right username\u0026#34; { whoami | Should Be \u0026#39;TheBeard\\Rob\u0026#39; } } } ","date":"2017-05-16T00:00:00Z","permalink":"https://blog.robsewell.com/blog/pester-for-presentations-ensuring-it-goes-ok/","title":"Pester for Presentations ‚Äì Ensuring it goes ok"},{"content":" and whilst we were there we were chatting about running Pester Tests. He wanted to know how he could run a Pester Test and not lose the failed tests as they scrolled past him. In his particular example we were talking about running hundreds of tests on thousands of databases on hundreds of servers\nI guess it looks something like that!!\nI explained about the -Show parameter which allows you to filter the results that you see. Using Get-Help Invoke-Pester you can see this\n-Show Customizes the output Pester writes to the screen. Available options are None, Default, Passed, Failed, Pending, Skipped, Inconclusive, Describe, Context, Summary, Header, All, Fails.\nThe options can be combined to define presets. Common use cases are:\nNone ‚Äì to write no output to the screen. All ‚Äì to write all available information (this is default option). Fails ‚Äì to write everything except Passed (but including Describes etc.).\nA common setting is also Failed, Summary, to write only failed tests and test summary.\nThis parameter does not affect the PassThru custom object or the XML output that is written when you use the Output parameters.\nRequired?¬†false Position?¬†named Default value¬†All Accept pipeline input?¬†false Accept wildcard characters?¬†false\nSo there are numerous options available to you. Lets see what they look like\nI will use a dummy test which creates 10 Context blocks and runs from 1 to 10 and checks if the number has a remainder when divided by 7\n1 2 3 4 5 6 7 8 9 10 11 12 13 Describe \u0026#34;Only the 7s Shall Pass\u0026#34; { $Servers = 0..10 foreach($Server in $servers) { Context \u0026#34;This is the context for $Server\u0026#34; { foreach($A in 1..10){ It \u0026#34;Should Not Pass for the 7s\u0026#34; { $A % 7 | Should Not Be 0 } } } } } Imagine it is 10 servers running 10 different tests\nFor the Show parameter All is the default, which is the output that you are used to\nNone does not write anything out. You could use this with -Passthru which will pass ALL of the test results to a variable and if you added -OutputFile and -OutputFormat then you can save ALL of the results to a file for consumption by another system. The -Show parameter only affects the output from the Invoke-Pester command to the host not the output to the files or the variable.\nHeader only returns the header from the test results and looks like this ( I have included the none so that you can see!)\nSummary, as expected returns only the summary of the results\nYou can use more than one value for the Show parameter so if you chose Header, Summary, Describe you would get this\nYou could use Failed to only show the failed tests which looks like this\nbut Andre explained that he also want to be able to see some progress whilst the test was running. If there were no failures then he would not se anything at all.\nSo Fails might be the answer (or Failed and Summary but that would not show the progress)\nFails shows the Header, Describe, Context¬†and also shows the Summary.\nHowever we carried on talking. PSConfEU is a fantastic place to talk about PowerShell üôÇ and wondered what would happen if you invoked Pester from inside a Pester test. I was pretty sure that it would work as Pester is just PowerShell but I thought it would be fun to have a look and see how we could solve that requirement\nSo I created 3 ‚ÄúInternal Tests‚Äù these are the ones we don‚Äôt want to see the output for. I then wrote an overarching Pester test to call them. In that Pester test script I assigned the results of¬†each¬†test to a variable which. When you examine it you see\nThe custom object that is created shows the counts of all different results of the tests, the time it took and also the test result.\nSo I could create a Pester Test to check the Failed Count property of that Test result\n$InternalTest1.FailedCount | Should Be 0\nTo make sure that we don‚Äôt lose the results of the tests we can output¬†them to a file like this\n$InternalTest1 = Invoke-Pester .\\\\Inside1.Tests.ps1 -Show None -PassThru -OutputFile C:\\\\temp\\\\Internal\\_Test1\\_Results.xml -OutputFormat NUnitXml\nSo now we can run Invoke-Pester and point it at that file and it will show the progress and the final result on the screen.\nYou could make use of this in different ways\nServer 1 Database1 Database2 Database3 Database4 Server 2 Database1 Database2 Database3 Database4 Server 3 Database1 Database2 Database3 Database4 Or by Test Category\nBackup Server1 Server 2 Server 3 Server 4 Agent Jobs Server 1 Server 2 Server 3 Server 4 Indexes Server 1 Server 2 Server 3 Server 4 Your only limitation is your imagination.\nAs we have mentioned PSConfEU you really should check out the videos on the youtube channel All of the videos that were successfully recorded will be on there. You could start with this one and mark your diaries for April 16-20 2018\n","date":"2017-05-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/pester-test-inception-and-the-show-parameter/","title":"Pester Test Inception and the Show Parameter"},{"content":"I was chatting on the SQL Community Slack¬†with my friend Sander Stad b | t¬†about some functions he is writing for the amazing PowerShell SQL Server Community module dbatools. He was asking my opinion as to how to enable user choice or options for Agent Schedules and I said that he should validate the input of the parameters. He said that was difficult as if the parameter was Weekly the frequency values required would be different from if the parameter was Daily or Monthly. That‚Äôs ok, I said, you can still validate the parameter.\nYou can read more about Parameters either online here or here or by running\n1 2 Get-Help¬†About_Parameters Get-Help About_Functions_Parameters You can also find more help¬†information with\nGet-Help About_*Parameters*\nThis is not a post about using Parameters, google for those but this is what I showed him.\nLets create a simple function that accepts 2 parameters Word and Number\n1 2 3 4 5 6 7 8 9 function Test-validation { Param ( [string]$Word, [int]$Number ) Return \u0026#34;$Word and $Number\u0026#34; } We can run it with any parameters\nIf we wanted to restrict the Word parameter to only accept Sun, Moon or Earth we can use the ValidateSetAttribute¬†as follows\n1 2 3 4 5 6 7 8 9 10 function Test-validation { Param ( [ValidateSet(\u0026#34;sun\u0026#34;, \u0026#34;moon\u0026#34;, \u0026#34;earth\u0026#34;)] [string]$Word, [int]$Number ) Return \u0026#34;$Word and $Number\u0026#34; } Now if we try and set a value for the $Word parameter that isn‚Äôt sun moon or earth then we get an error\nand it tells us that the reason for the error is that TheBeard! does not belong to the set sun, moon, earth.\nBut what Sander wanted was to validate the value of the second parameter depending on the value of the first one. So lets say we wanted\nIf word is sun, number must be 1 or 2 If word is moon, number must be 3 or 4 If word is earth, number must be 5 or 6 We can use the ValidateScriptAttribute¬†to do this. This requires a script block which returns True or False. You can access the current parameter with $_ so we can use a script block like this\n1 2 3 4 5 { if($Word -eq \u0026#39;Sun\u0026#39;){$_ -eq 1 -or $_ -eq 2} elseif($Word -eq \u0026#39;Moon\u0026#39;){$_ -eq 3 -or $_ -eq 4} elseif($Word -eq \u0026#39;earth\u0026#39;){$_ -eq 5 -or $_ -eq 6} } The function now looks like\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function Test-validation { Param ( [ValidateSet(\u0026#34;sun\u0026#34;, \u0026#34;moon\u0026#34;, \u0026#34;earth\u0026#34;)] [string]$Word, [ValidateScript({ if($Word -eq \u0026#39;Sun\u0026#39;){$_ -eq 1 -or $_ -eq 2} elseif($Word -eq \u0026#39;Moon\u0026#39;){$_ -eq 3 -or $_ -eq 4} elseif($Word -eq \u0026#39;earth\u0026#39;){$_ -eq 5 -or $_ -eq 6} })] [int]$Number ) Return \u0026#34;$Word and $Number\u0026#34; } It will still fail if we use the wrong ‚ÄúWord‚Äù in the same way but now if we enter earth and 7 we get this\nBut if we enter sun and 1 or moon and 3 or earth and 5 all is well\nI would add one more thing. We should always write PowerShell functions that are easy for our users to self-help. Of course, this means write good help for the function. here is a great place to start from June Blender\nIn this example, the error message\nTest-validation : Cannot validate argument on parameter ‚Äònumber‚Äô. The ‚Äù if($word -eq ‚ÄòSun‚Äô){$_ -eq 1 -or $_ -eq 2} elseif($word -eq ‚ÄòMoon‚Äô){$_ -eq 3 -or $_ -eq 4} elseif($word -eq ‚Äòearth‚Äô){$_ -eq 5 -or $_ -eq 6} ‚Äù validation script for the argument with value ‚Äú7‚Äù did not return a result of True. Determine why the validation script failed, and then try the command again. At line:1 char:39\nTest-validation -Word ‚Äúearth‚Äù -number 007 +¬†~~~ CategoryInfo¬†: InvalidData: (:) [Test-validation], ParameterBindingValidationException FullyQualifiedErrorId : ParameterArgumentValidationError,Test-validation is not obvious to a none-coder so we could make it easier. As we are passing in a script block we can just add a comment like this. I added a spare line above and below to make it stand out a little more\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 function Test-validation { Param ( [ValidateSet(\u0026#34;sun\u0026#34;, \u0026#34;moon\u0026#34;, \u0026#34;earth\u0026#34;)] [string]$Word, [ValidateScript({ # # Sun Accepts 1 or 2 # Moon Accepts 3 or 4 # Earth Accepts 5 or 6 # if($Word -eq \u0026#39;Sun\u0026#39;){$_ -eq 1 -or $_ -eq 2} elseif($Word -eq \u0026#39;Moon\u0026#39;){$_ -eq 3 -or $_ -eq 4} elseif($Word -eq \u0026#39;earth\u0026#39;){$_ -eq 5 -or $_ -eq 6} })] [int]$Number ) Return \u0026#34;$Word and $Number\u0026#34; } Now if you enter the wrong parameter you get this\nwhich I think makes it a little more obvious\n","date":"2017-04-26T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/04/01-more-help.png","permalink":"https://blog.robsewell.com/blog/powershell-function-validating-a-parameter-depending-on-a-previous-parameters-value/","title":"PowerShell Function ‚Äì Validating a Parameter Depending On A Previous Parameter‚Äôs Value"},{"content":"Just a short post today. When you open a new file in VS Code (Using CTRL + N) it opens by default as a plain text file.\nTo change the language for the file¬†use¬†CTRL +K, M.\nThat‚Äôs CTRL and K together and then M afterwards separately.\nthen you can choose the language for the file. It looks like this\nHowever, if you just want your new file to open as a particular language every time you can change this in the settings.\nClick File ‚Äì\u0026gt; Preferences ‚Äì\u0026gt; Settings\nor by clicking CTRL + ,\nThis opens the settings.json file. Search in the bar for default and scroll down until you see file\nIf you hover over the setting that you want to change, you will see a little pencil. Click on that and then Copy to Settings which will copy it to your user settings in the right hand pane.\nNOTE ‚Äì You will need to enter powershell and not PowerShell. For other languages, click on the language in the bottom bar and look at the value in the brackets next to the language name\nOnce you have entered the new settings save the file (CTRL + S) and then any new file you open will be using the language you have chosen\nIt looks like this\nand now every new file that you open will be opened as a PowerShell file (or whichever language you choose)\nYou will still be able to change the language with CTRL K, m\nJust to be clear, because people sometimes get this wrong. That‚Äôs CTRL and K, let go and then M. You will know you are doing correctly when you see\n(CTRL + K) was pressed waiting for second key of chord‚Ä¶‚Ä¶\nIf you get it wrong and Press CTRL + K + M then you will open the Extensions search for keymaps.\nThis is a brilliant feature enabling you to copy key mappings for the programmes you use all the time and save you from learning the Code key mappings. You can find the keymaps in the Extensions Marketplace¬†as well as by pressing CTRL + K + M\n","date":"2017-04-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/setting-the-default-file-type-for-a-new-file-in-vs-code/","title":"Setting the default file type for a new file in VS Code"},{"content":" ","date":"2017-04-19T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-twitter-with-vs-code/","title":"Using Twitter with VS Code"},{"content":"Last week I was showing a co-worker some PowerShell code and he asked what the editor was that I was using. Visual Studio Code I said. Why do you use that? What does it do? This is what I showed him\nRuns on any Operating System Code (as I shall refer to it) is free lightweight open source editor which runs on all the main operating systems. So you have the same experience in Linux as on Windows. So there is less to learn\nExtensions You can add new languages, themes, debuggers and tools from the extensions gallery to reduce the number of programmes you have open and the need to switch between programmesYou can add extensions using CTRL + SHIFT¬†+ X and searching in the bar or by going to the Extensions gallery¬†searching for the extensions and copying the installation command 02 - extensions gallery.PNG\nDebugging There is a rich de-bugging experience built in You can learn about debugging from the official docs and Keith Hill wrote a blog post on Hey Scripting Guys about debugging PowerShell\nIntellisense An absolute must to make life simpler. Code has intellisense for PowerShell and T-SQL which I use the most but also for many more languages . Read more here\nGit integration I love the Git integration, makes it so easy to work with GitHub for me. I can see diffs, commit, undo commits nice and simply. Just open the root folder of the repository and its there This page will give you a good start on using git with Code\nNo distractions With full screen mode (F11) or Zen mode (CTRL +K, Z) I can concentrate on coding and not worry about distractions\nStay in one programme and do it all I have a Markdown document, a PowerShell script and a T-SQL script all in one Git repository and I can work on all of them and version control in one place. The screencast below also shows some of the new capabilities available in the insiders version¬†I managed to leave the screen recording dialogue open as well, apologies and the mistake was deliberate!\nI used the GitLens and SQL beautify extensions as well as the dbatools module in that demo That‚Äôs why I am using Code more and more these days, hope it helps Happy Automating!\n","date":"2017-04-13T00:00:00Z","permalink":"https://blog.robsewell.com/blog/why-vs-code-increases-my-productivity/","title":"Why VS Code Increases my Productivity"},{"content":"WARNING ‚Äì Contains Orange and light-hearted photos üòâ\nI have returned home from SQLBits 2017 The Disco Edition. I am exhausted, my body is pointing out to me in a variety of ways¬†that this is the only week of the year that I spend so much time on my feet. Why would anyone do it?\nMany months of work First though, you need to know that the SQLBits conference is organised and run by volunteers. All of the committee spend many, many long hours, out of their own free time, for many months before and after the event to ensure that the attendees, sponsors, speakers and volunteers experience is trouble free. I think that they do an amazing and fantastic job and repeatedly pull off the best, most enjoyable conference that I have been to.\nThank you Simon, Chris, Darren, Allan, Alex, Jonathan, Annette\nThank you also to their families as well, who undoubtedly miss out on time with them whilst they are organising everything to do with the event, from finding venues, organising dates, speakers, marketing, website, sponsors, printing, audio visual, THE PARTY!! and all the other big and small things that it takes to make an event of that size occur.\nOrange Shirted Wonderful Folk There is another group of volunteers that you will have seen at SQLBits. For the last couple of years¬†we have been the ones in the orange shirts.\nHere¬†is the Class of 2017\nI think this is a brilliant colour as it makes¬†us easy to spot (although a couple of attendees who also had orange tops on did get stopped and asked for directions üôÇ )\nWhat do they do? These folk come in early and get everything set up.¬†Sometimes we have to explain that the event isn‚Äôt ready for you yet\nWe sort out the registration desk and greet every attendee, speaker and sponsor and assist them.\nWe help the speakers get set up\nand ensure they have everything they need.\nAaron Bertrand (above) and John Martin from SentryOne said that it is the best experience for a speaker that they have had¬†anywhere.\nWe direct and assist the attendees to be in the right place, sometimes with some flair!\nWe ensure that any issues are resolved quickly and with as little distraction as possible. The room is too hot, too cold, too noisy or too quiet. The projector isn‚Äôt working or the speakers microphone has a buzz, there is too much light or too little. The water coolers are empty. The rubbish needs picking up. All these and many other minor complications are communicated and passed to the correct people to get resolved.\nSometimes we have to resolve our own issues. We had folks who were called by their work and had to stop helping and go back to their day jobs for a few hours. We all understand what it is like for people working in technology and adapt and manage accordingly. In almost every photo I took, there is someone in an orange shirt to be seen.\nWe answer numerous questions from the 1500 or so attendees (and the odd sheep)¬†who came this year.\nFrom timings and locations to taxi numbers or restaurants. Unfortunately I did not beat last years ‚ÄúBest question I have been asked at SQLBits‚Äù which was\nExcuse me, I have a Dalek in the van . What would you like me to do with it?\nI was even asked questions on the way back to the hotel gone midnight on Saturday!!\nWe stay afterwards and help to get ready for the next day, putting out the new signs for the domes and the required paperwork.\nSo why do we do it? I asked the guys and gals this question and in their own words, this is why they do it\nBeing a volunteer at SQLBits is not easy. I‚Äôm writing this three days after the event and my legs are still sore. Most days are 11 hours long¬†and you will be standing for most of them. Very often the sessions are full, so you‚Äôll be giving up your seat to one of the attendees. Lunches and breaks are shorter as you are either cleaning down the last session or getting ready for the next. When things go wrong, and they do, you‚Äôll need to get them fixed as quickly as possible even¬†if you¬†have not had coffee yet.\nYou do get to attend sessions but you might not always get your first choice. This can be both a good and bad thing. Very often I have filled in on sessions that I normally wouldn‚Äôt attend as they are outside my direct area of work, only to find them the most interesting as I get to see how the other half lives.\nSo why do I keep coming back? Well it‚Äôs fun. We have a laugh, even on reception when it‚Äôs busy you get to joke with the attendees, speakers and other helpers. There is pizza, beer and jokes while bag packing. Odd expresso calls!¬†Working along side some else is a great way to get to know them. I live outside the normal SQL community structures, my nearest user group is a 150 miles away. So I don‚Äôt get to interact with other SQL family members as often as others. But even so, I know as soon as I walk into SQL Bits, there will be a chorus of, ‚ÄúHey Conan, how have you been?‚Äù from people I haven‚Äôt seen in a year. There is also something about wearing a bright orange shirt that seems to attract interactions from the attendees.\nAll because of the of the experience that is being a volunteer.\nConan Farrell\n![WP_20170406_10_33_15_Pro (2).jpg](https://blog.robsewell.com/assets/uploads/2017/04/wp_20170406_10_33_15_pro-2.jpg\nI owe a lot to the SQL Community. It was at SQLBits a few years ago that someone convinced me to start speaking. That encouragement and acceptance from the #SQLFamily put into motion a series of events that lead to me quitting the best job of my life last year in favour of an adventure to set up my own company. It‚Äôs been a wonderful journey and if it had not been for SQLBits (and SQL Relay, and SQL Saturdays, and others) I wouldn‚Äôt have taken it.\nI have a debt to pay. And it‚Äôs wonderful to be able to contribute towards giving other people the same opportunities that the community has given me.\nAlso, for similar reasons, I recently joined the committee for SQL Relay. While I‚Äôve been a sponsor and a speaker a lot before, I am fairly inexperienced at organising events. Helping out with the awesome SQLBits team has been a great learning curve. I hope to take what I have learned and apply it to my role for SQL Relay.\nFinally, frankly, there are so many great people in the SQL Community it is just wonderful to be able to work with them during the day. (And share some beers with them in the evening!)\nAlex Yates\nI volunteer at SQLBits and in the wider Data Platform community for various reasons. The community is the glue in our industry and being part of that glue is both a privilege and an honour. Without volunteers these awesome events couldn‚Äôt function and we couldn‚Äôt share knowledge as freely. There also would not be the same opportunities to network with such great people and experts in the same field. I am proud to be part of the SQL Family and facilitate the learning that being a volunteer offers to professionals just like me. Stronger together.\nPaul Andrew\nWhen I volunteered, I felt I‚Äôd like to give back to the SQL Community a bit of the much I‚Äôve received from them. I wanted to become more engaged. I didn‚Äôt even dream it would be like this, the terrific team I found here was so great that I can barely wait for the next one.\nMiguel Oliveira\nVolunteering gives you the experience of the effort \u0026amp; reward of running an event.. You build an appreciation of generating success, creating networks of colleagues and friends, being quick to react, and helps you walk away with the feeling of ‚ÄúI contributed to this‚Äù. Everyone should volunteer, even if its just once.\nBen Watt\nThis year was my 10th sqlbits (remembering we skipped unlucky 13) and 7th I‚Äôve been on the helping team for. I started helping when I learned my original conference companions were moving on to different technologies. Something I‚Äôve never understood.\nHistorically I‚Äôve worked in multiple companies as a lone dba, something I‚Äôve found many at SQLBits can relate to. Through user groups and SQLBits I‚Äôve met lots of others in the same boat over the years. It can be a frustrating job defending servers, implementing best practice and writing sql when all your application team want to do is add columns and ship the next release!\nYes, there are good networking opportunities and the parties are great but at the core is great quality training. I‚Äôve had a great time playing a small part in helping deliver the experience over the years. Occasionally I have to shout about feedback forms or seating arrangements but on the whole folk are fine with it.¬†If I‚Äôm honest they are long days. A typical day involves a start and end meeting in addition to sessions you are monitoring. Add to that hotel commutes, catching up with the friends you made last year, meals, drinks etc and its 1am. Oh, and tomorrow‚Äôs meeting is 7.30 eek‚Ä¶.\nIt‚Äôs been a great journey watching the progression of events. Each one adds to the last, they run a lot slicker now. The sponsors are first class and always embrace the theme with their stalls and giveaways. I do wish more folk would enter the prize draws and stop me winning though, it‚Äôs getting embarrassing now üôÇ\nRichard Doering¬†The best Data Platform conference in Europe! I couldn‚Äôt miss the opportunity to be much more than just an attendee.\nHubert Kobierzewski\nI volunteer for sqlbits, to get experience from a multi day event that has a lot of attendees. Lots of take aways to make local community events run smoother for the delegates. Thats my main reason\nJens Vestergard\nThere is no way I could not use this next photo celebrate this years Most Valuable Helper.\nThank you Shaun Atkinson you were amazing and thank you for¬†looking after¬†my former intern James as a first time volunteer\nHow can I do it next year? We always welcome new people to our team. If you would like to volunteer your time to help at the next SQLBits please send an email to helpers@sqlbits.com with a subject of I would like to volunteer or something similar. Nearer the time of next years SQLBits (No I don‚Äôt know when or where it is, I will know when you do) the awesome, amazing, fantastic, brilliant, organising amazeballs also known as Annette will get in touch with you with further details\nYou can expect to have a lot of fun, make new friends and help make the best data platform conference the best data platform conference\nOh and sometimes theres free pizza üôÇ\nAnother Thank You Some lovely people, (I am really sorry but I didn‚Äôt get everyones name) brought sweets, biscuits, cakes and other goodies for the crew. Thank you, they were very much appreciated.\nPictures because smiles üôÇ Not all of the volunteers wanted to give feedback publically but I had some cracking photos so I thought I would share them as well. Enjoy\n","date":"2017-04-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/why-volunteer-at-sqlbits/","title":"Why Volunteer at SQLBits ?"},{"content":"NOTE - Updated November 2022 for this site and the correct command name.\nThere are times when DBA‚Äôs are required to export database user permissions to a file. This may be for a number of reasons. Maybe for DR purposes, for auditing, for transfer to another database or instance. Sometimes we need to create a new user with the same permissions as another user or perhaps nearly the same permissions. I was having a conversation with my good friend and MVP Cl√°udio Silva¬†and we were talking about how Export-DbaUser from dbatools could help in these situations and he suggested that I blogged about it so here it is.\nThe dbatools module (for those that don‚Äôt know) is a PowerShell module written by amazing folks in the community designed to make administrating your SQL Server significantly easier using PowerShell. The instructions for installing it are available here It comprises of 182 separate commands at present\nCl√°udio wrote Export-DbaUser¬†to solve a problem. You should always start with Get-Help whenever you are starting to use a new PowerShell command\nGet-Help Export-DbaUser -ShowWindow\nThe command exports users creation and its permissions to a T-SQL file or host. Export includes user, create and add to role(s), database level permissions, object level permissions and also the Create Role statements for any roles, although the script does not create IF NOT EXISTS statements which would be an improvement. It also excludes the system databases so if you are scripting users who need access to those databases then that needs to be considered. Cl√°udio is aware of these and is looking at improving the code to remove those limitations.\nIt takes the following parameters\nSqlInstance\nThe SQL Server instance name. SQL Server 2000 and above supported. User\nExport only the specified database user(s). If not specified will export all users from the database(s) DestinationVersion\nWhich SQL version the script should be generated using. If not specified will use the current database compatibility level FilePath\nThe filepath to write to export the T-SQL. SqlCredential\nAllows you to login to servers using alternative credentials NoClobber\nDo not overwrite the¬†file Append\nAppend to the file Databases\nNot in the help but a dynamic parameter allowing you to specify one or many databases Lets take a look at it in action\nExport-DbaUser -SqlInstance SQL2016N2 -FilePath C:\\temp\\SQL2016N2-Users.sql Notepad C:\\temp\\SQL2016N2-Users.sql\nLets take a look at a single database\n1 2 Export-DbaUser -SqlInstance SQL2016N2 -FilePath C:\\temp\\SQL2016N2-Fadetoblack.sql -Databases Fadetoblack notepad C:\\temp\\SQL2016N2-Fadetoblack.sql This is so cool and so easy. It is possible to do this in T-SQL. I found this script on SQLServerCentral for example which is 262 lines and would then require some mouse action to save to a file\nWe can look at a single user as well. Lets see what Lars Ulrich can see on the FadeToBlack database\n1 2 3 4 5 6 7 8 9 10 11 12 USE [FadetoBlack] GO CREATE USER [UlrichLars] FOR LOGIN [UlrichLars] WITH DEFAULT_SCHEMA=[dbo] GO GRANT CONNECT TO [UlrichLars] GO DENY INSERT ON [dbo].[Finances] TO [UlrichLars] GO DENY SELECT ON [dbo].[RealFinances] TO [UlrichLars] GO GRANT SELECT ON [dbo].[Finances] TO [UlrichLars] GO So he can select data from the Finances table but cannot insert and cannot read the RealFinances data. Now lets suppose a new manager comes in and he wants to be able to look at the data in this database. As the manager though he wants to be able to read the RealFinances table and insert into the Finances table. He requests that we add those permissions to the database. We can create the T-SQL for Lars user¬†and then do a find and replace for UlrichLars with TheManager , DENY INSERT ON [dbo].[Finances] with GRANT INSERT ON [dbo].[Finances]¬†and DENY SELECT ON [dbo].[RealFinances] with GRANT SELECT ON [dbo].[RealFinances] and save to a new file.\n1 2 3 4 5 6 7 8 9 $LarsPermsFile = \u0026#39;C:\\temp\\SQL2016N2-Lars-Fadetoblack.sql\u0026#39; $ManagerPermsFile = \u0026#39;C:\\temp\\SQL2016N2-Manager-Fadetoblack.sql\u0026#39; Export-DbaUser -SqlInstance SQL2016N2 -FilePath $LarsPermsFile -User UlrichLars -Databases Fadetoblack $ManagerPerms = Get-Content $LarsPermsFile ## replace permissions $ManagerPerms = $ManagerPerms.Replace(\u0026#39;DENY INSERT ON [dbo].[Finances]\u0026#39;,\u0026#39;GRANT INSERT ON [dbo].[Finances]\u0026#39;) $ManagerPerms = $ManagerPerms.Replace(\u0026#39;DENY SELECT ON [dbo].[RealFinances]\u0026#39;,\u0026#39;GRANT SELECT ON [dbo].[RealFinances]\u0026#39;) $ManagerPerms = $ManagerPerms.Replace(\u0026#39;UlrichLars\u0026#39;,\u0026#39;TheManager\u0026#39;) Set-Content -path $ManagerPermsFile -Value $ManagerPerms I will open this in Visual Studio Code Insiders using\ncode-insiders $LarsPermsFile , $ManagerPermsFile\nif you are not using the insiders preview remove the ‚Äú-insiders‚Äù\nYou can right click on the Lars file and click select for compare and then right click on the Managers file and select compare with Lars File and get a nice colour coded diff\nPerfect, we can run that code and complete the request. When we¬†impersonate Lars we get\nbut when we run as the manager we get\nExcellent! All is well.\nIt turns out that there is another Fadetoblack database on a SQL2000 instance which for reasons lost in time never had its data imported into¬†the newer database. It is still used for reporting purposes. The manager needs to have the same permissions as on the SQL2016N2 instance. Obviously the T-SQL we have just created will not work as¬†that syntax did not exist for SQL 2000¬†but Cl√°udio has thought of that too. We can use the DestinationVersion parameter to create the SQL2000 (2005,2008/20008R2,2012,2014,2016) code\nWe just run\n1 2 3 Export-DbaUser -SqlInstance SQL2016N2 -Databases FadetoBlack -User TheManager¬†-FilePath C:\\temp\\S QL2016N2-Manager-2000.sql¬†-DestinationVersion SQLServer2000 Notepad C:\\temp\\SQL2016N2-Manager-2000.sql and our SQL2000 compatible code is created\nSimply awesome. Thank you Cl√°udio\nHappy Automating\nNOTE ‚Äì The major 1.0 release of dbatools due in the summer 2017 may have breaking changes which will stop the above code from working. There are also new commands coming which may replace this command. This blog post was written using dbatools version 0.8.942 You can check your version using\nGet-Module dbatools\nand update it using an Administrator PowerShell session with\nUpdate-Module dbatools\nYou may find that you get no output from Update-Module as you have the latest version. If¬†you have not installed the¬†module from the PowerShell Gallery using\nInstall-Module dbatools\nThen you can use\nUpdate-dbatools\n","date":"2017-04-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/export-sql-user-permissions-to-t-sql-script-using-powershell-and-dbatools/","title":"Export SQL User Permissions to T-SQL script using PowerShell and dbatools"},{"content":" ","date":"2017-04-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/testing-sql-server-access-to-a-share-with-powershell-using-dbatools/","title":"Testing SQL Server Access to a share with PowerShell using dbatools"},{"content":" ","date":"2017-04-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/testing-the-identity-column-usage-in-sql-server-with-powershell-and-dbatools/","title":"Testing the Identity Column usage in SQL Server with PowerShell and dbatools"},{"content":" ","date":"2017-04-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-pester-with-get-dbalastgoodcheckdb-from-dbatools/","title":"Using Pester with Get-DbaLastGoodCheckDb from dbatools"},{"content":" ","date":"2017-04-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/getting-sqlservers-last-known-good-dbcc-checkdb-with-powershell-and-dbatools/","title":"Getting SQLServers Last Known Good DBCC Checkdb with PowerShell and dbatools"},{"content":" ","date":"2017-04-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/test-the-sql-server-database-collation-with-powershell-and-dbatools/","title":"Test the SQL Server database collation with PowerShell and dbatools"},{"content":" \u0026ldquo;@ } New-IseSnippet @snippet }\n","date":"2017-04-01T00:00:00Z","permalink":"https://blog.robsewell.com/blog/max-length-of-a-column-in-a-datatable-in-powershell/","title":"Max Length of a column in a DataTable in PowerShell"},{"content":"I read a great blog post about answering the question how big is the database using T-SQL on SQL Buffet and wondered how much I could do with the dbatools module\nThe dbatools module (for those that don‚Äôt know) is a PowerShell module written by amazing folks in the community designed to make administrating your SQL Server significantly easier using PowerShell. The instructions for installing it are available here It comprises of 182 separate commands at present (11 March 2017 Version 0.8.938)\nI know that there is a Get-DBADatabaseFreeSpace Command written by Mike Fal b | t¬†and using Glenn Berry‚Äôs diagnostic queries)\nFirst thing as always is to look at the help\nGet-Help Get-DbaDatabaseFreespace -ShowWindow\nwhich will show you the help for the command and some examples\nLets look at the details for a single instance\nGet-DbaDatabaseFreespace -sqlserver $server\nThis is what it looks like\nand yes it really is that fast, I have not speeded this up. 232 ms to get those details for an instance with 19 databases\nWhat information do you get ? Lets look at the information for a single database, you get an object for each file\nServer¬†: SQL2014SER12R2 Database¬†: DBA-Admin FileName¬†: DBA-Admin_System FileGroup¬†: PRIMARY PhysicalName¬†: F:\\DBA-Admin_System.MDF FileType¬†: ROWS UsedSpaceMB¬†: 3 FreeSpaceMB¬†: 253 FileSizeMB¬†: 256 PercentUsed¬†: 1 AutoGrowth¬†: 0 AutoGrowType¬†: MB SpaceUntilMaxSizeMB¬†: 16777213 AutoGrowthPossibleMB : 0 UnusableSpaceMB¬†: 16777213\nServer¬†: SQL2014SER12R2 Database¬†: DBA-Admin FileName¬†: DBA-Admin_Log FileGroup¬†: PhysicalName¬†: G:\\DBA-Admin_Log.LDF FileType¬†: LOG UsedSpaceMB¬†: 32 FreeSpaceMB¬†: 224 FileSizeMB¬†: 256 PercentUsed¬†: 12 AutoGrowth¬†: 256 AutoGrowType¬†: MB SpaceUntilMaxSizeMB¬†: 2528 AutoGrowthPossibleMB : 2304 UnusableSpaceMB¬†: 0\nServer¬†: SQL2014SER12R2 Database¬†: DBA-Admin FileName¬†: DBA-Admin_User FileGroup¬†: UserFG PhysicalName¬†: F:\\DBA-Admin_User.NDF FileType¬†: ROWS UsedSpaceMB¬†: 1 FreeSpaceMB¬†: 255 FileSizeMB¬†: 256 PercentUsed¬†: 0 AutoGrowth¬†: 256 AutoGrowType¬†: MB SpaceUntilMaxSizeMB¬†: 5119 AutoGrowthPossibleMB : 4864 UnusableSpaceMB¬†: 0\nThere is a lot of useful information¬†returned for each file. Its better if you use Out-GridView as then you can order by columns and filter in the top bar.\nAs always, PowerShell uses the permissions of the account running the sessions to connect to the SQL Server unless you provide a separate credential for SQL Authentication. If you need to connect with a different windows account you will need to hold Shift down and right click on the PowerShell icon and click run as a different user.\nLets get the information for a single database. The command has dynamic parameters which populate the database names to save you time and keystrokes\nBut you may want to gather information about more than one server. lets take a list of servers and place them into a variable. You can add your servers to this variable in a number of ways, maybe by querying your CMDB or using your registered servers or central management server\n$SQLServers = 'SQL2005Ser2003','SQL2012Ser08AG3','SQL2012Ser08AG1','SQL2012Ser08AG2','SQL2014Ser12R2','SQL2016N1','SQL2016N2','SQL2016N3','SQLVnextN1','SQLvNextN2'\nand then\nGet-DbaDatabaseFreespace -SqlInstance $SQLServers | Out-GridView\nAs you can see, you get a warning quite correctly, that the information for the asynchronous secondary node of the availability group databases is not available and I did not have all of my servers running so there are a couple of could not connect warnings as well. You can still filter very quickly. dbatools is tested from SQL2000 to SQL vNext as you can see below¬†(although I don‚Äôt have a SQL2000 instance)\nNot only on Windows, this command will work¬†against SQL running on Linux as well\nSo if we want to know the total size of the¬†files on disk for¬†the database we need to look at the FileSizeMB property\n1 2 3 4 $server = \u0026#39;SQL2014Ser12R2\u0026#39; $dbName = \u0026#39;AdventureWorksDW2014\u0026#39; Get-DbaDatabaseFreespace -SqlServer $server -database $dbName | Select Database,FileName,FileSizeMB Of course that‚Äôs an easy calculation here\nbut if we have numerous files then it may be tougher. we can use the Measure-Object command to sum the properties. We need to do a bit of preparation here and set a couple of calculated properties to make it more readable\n1 2 3 4 5 6 7 8 $server = \u0026#39;SQL2014Ser12R2\u0026#39; $dbName = \u0026#39;AdventureWorksDW2014\u0026#39; $database = @{Name = \u0026#39;Database\u0026#39;; Expression = {$dbname}} $FileSize = @{Name = \u0026#39;FileSize\u0026#39;; Expression = {$_.Sum}} Get-DbaDatabaseFreespace -SqlServer $server -database $dbName | Select Database,FileSizeMB | Measure-Object FileSizeMB -Sum | Select $database ,Property, $filesize Maybe we want to look at all of the databases on an instance. Again, we have to do¬†a little more work here\n1 2 3 4 5 6 7 8 9 10 $server = \u0026#39;SQL2014Ser12R2\u0026#39; $srv = Connect-DbaSqlServer $server $SizeonDisk = @() $srv.Databases |ForEach-Object { $dbName = $_.Name $database = @{Name = \u0026#39;Database\u0026#39;; Expression = {$dbname}} $FileSize = @{Name = \u0026#39;FileSize\u0026#39;; Expression = {$_.Sum}} $SizeOnDisk += Get-DbaDatabaseFreespace -SqlServer $server -database $dbName | Select Database,FileSizeMB |¬†Measure-Object FileSizeMb -Sum | Select $database ,Property, $Filesize } $SizeOnDisk If we wanted the databases ordered by the size of their files we could do this\n$SizeOnDisk |Sort-Object Filesize -Descending\nAs it is PowerShell we have an object and we can use it any way we like. Maybe we want that information in a text file or a csv or an excel file or in an email, PowerShell can do that\n1 2 3 4 5 6 7 8 9 10 11 12 ## In a text file $SizeonDisk | Out-file C:\\\\temp\\\\Sizeondisk.txt Invoke-Item C:\\\\temp\\\\Sizeondisk.txt \\## In a CSV $SizeonDisk | Export-Csv C:\\\\temp\\\\Sizeondisk.csv -NoTypeInformation notepad C:\\\\temp\\\\Sizeondisk.csv \\## Email Send-MailMessage -SmtpServer $smtp -From DBATeam@TheBeard.local -To JuniorDBA-Smurf@TheBeard.Local ` -Subject \u0026#34;Smurf this needs looking At\u0026#34; -Body $SizeonDisk \\## Email as Attachment Send-MailMessage -SmtpServer $smtp -From DBATeam@TheBeard.local -To JuniorDBA-Smurf@TheBeard.Local ` -Subject \u0026#34;Smurf this needs looking At\u0026#34; -Body \u0026#34;Smurf\u0026#34; -Attachments C:\\\\temp\\\\Sizeondisk.csv I had a little play with Boe Prox PoshCharts (you have to use the dev branch) to see if I could get some nice charts and unfortunately the bar charts did not come out so well but luckily the donut and pie charts did. (I‚Äôm a DBA I love donuts!)\n1 2 $SizeonDisk| Out-PieChart -XField Database -YField FileSize -Title \u0026#34;UsedSpaceMB per Database on $Server\u0026#34; -IncludeLegend -Enable3D $SizeonDisk| Out-DoughnutChart -XField Database -YField FileSize -Title \u0026#34;UsedSpaceMB per Database on $Server\u0026#34; -IncludeLegend -Enable3D So the point is, whatever you or your process requires you can pretty much bet that PowerShell can enable it for you to automate.\nYou can make use of all of the properties exposed by the command. If you want to only see the files with less than 20% space free\nGet-DbaDatabaseFreespace -SqlServer $server | Where-Object {$_.PercentUsed -gt 80}\nyou can also use the command to check for file growth settings as well\nGet-DbaDatabaseFreespace -SqlServer $server | Where-Object {$_.AutoGrowType¬†-ne 'Mb'}\nOr maybe you want to know the FileSize, Used and Free Space per database\n1 2 3 4 5 6 7 8 9 10 $server = \u0026#39;SQL2014Ser12R2\u0026#39; $srv = Connect-DbaSqlServer $server $SizeonDisk = @() $srv.Databases |ForEach-Object { $dbName = $_.Name $database = @{Name = \u0026#39;Database\u0026#39;; Expression = {$dbname}} $MB = @{Name = \u0026#39;Mbs\u0026#39;; Expression = {$_.Sum}} $SizeOnDisk += Get-DbaDatabaseFreespace -SqlServer $server -database $dbName | Select Database,FileSizeMB, UsedSpaceMB, FreeSpaceMb |¬†Measure-Object FileSizeMb , UsedSpaceMB, FreeSpaceMb -Sum¬†| Select $database ,Property, $Mb } $SizeOnDisk Hopefully that has given you a quick insight into another one of the fabulous dbatools commands. Any questions, comment below or head over to the SQL Server Community Slack via https://sqlps.io/slack\nHappy Automating\nNOTE ‚Äì The major 1.0 release of dbatools due in the summer 2017 may have breaking changes which will stop the above code from working. There are also new commands coming which may replace this command. This blog post was written using dbatools version 0.8.942 You can check your version using\nGet-Module dbatools\nand update it using an Administrator PowerShell session with\nUpdate-Module dbatools\nYou may find that you get no output from Update-Module as you have the latest version. If¬†you have not installed the¬†module from the PowerShell Gallery using\nInstall-Module dbatools\nThen you can use\nUpdate-dbatools\n","date":"2017-03-29T00:00:00Z","permalink":"https://blog.robsewell.com/blog/getting-sql-server-file-sizes-and-space-used-with-dbatools/","title":"Getting SQL Server File Sizes and Space Used with dbatools"},{"content":" ","date":"2017-03-27T00:00:00Z","permalink":"https://blog.robsewell.com/blog/test-your-sqlserver-backups-on-linux-with-powershell-and-dbatools/","title":"Test your Sqlserver backups on Linux with PowerShell and dbatools"},{"content":" ","date":"2017-03-25T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-pester-with-dbatools-test-dbalastbackup/","title":"Using Pester with dbatools Test-DbaLastBackup"},{"content":" ","date":"2017-03-22T00:00:00Z","permalink":"https://blog.robsewell.com/blog/backing-up-sql-server-on-linux-using-ola-hallengrens-maintenance-solution/","title":"Backing up SQL Server on Linux using Ola Hallengrens Maintenance Solution"},{"content":" ","date":"2017-03-22T00:00:00Z","permalink":"https://blog.robsewell.com/blog/taking-dbatools-test-dbalastbackup-a-little-further/","title":"Taking dbatools Test-DbaLastBackup a little further"},{"content":" ","date":"2017-03-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/testing-your-sql-server-backups-the-easy-way-with-powershell-dbatools/","title":"Testing Your SQL Server Backups the Easy Way with PowerShell \u0026 dbatools"},{"content":" ","date":"2017-03-18T00:00:00Z","permalink":"https://blog.robsewell.com/blog/restoring-an-entire-sql-server-user-databases-with-powershell-using-dbatools/","title":"Restoring an entire SQL Server user databases with PowerShell using dbatools"},{"content":" }\n","date":"2017-03-13T00:00:00Z","permalink":"https://blog.robsewell.com/blog/adding-a-powershell-job-step-to-an-existing-sql-agent-job-step-with-powershell/","title":"Adding a PowerShell Job Step to an existing SQL Agent Job Step with PowerShell"},{"content":" ","date":"2017-03-12T00:00:00Z","permalink":"https://blog.robsewell.com/blog/vs-code-powershell-snippets/","title":"VS Code PowerShell Snippets"},{"content":"One of the most visited posts on my blog is nearly two and half years old now ‚Äì Add User to SQL Server Database Role with PowerShell and Quickly Creating Test Users.¬†I thought it was time to update it and use the latest sqlserver module and the dbatools module.\nYou can get the latest version of the sqlserver module by installing SSMS 2016. The PASS PowerShell Virtual Chapter¬†have created a short link to make this easier for you to remember: https://sqlps.io/dl Once you have downloaded and installed SSMS you can load the module.\n1 Import-Module sqlserver There is one situation where you will get an error loading the sqlserver module into PowerShell. If you have the SQLPS module already imported then you will get the following error:\nImport-Module : The following error occurred while loading the extended type data file:\nIn that case you will need to remove the SQLPS module first.\n1 2 Remove-Module sqlps Import-Module sqlserver The original post dealt with creating a number of test users for a database and assigning them to different roles quickly and easily. First let‚Äôs quickly create a list of Admin users and a list of Service Users and save them in a text file.\n1 2 3 4 5 6 7 8 9 10 11 $i = 0 while ($I -lt 100) { \u0026#34;Beard_Service_User$i\u0026#34; | Out-File \u0026#39;C:\\temp\\Users.txt\u0026#39; -Append $i++ } $i = 0 while ($I -lt 10) { \u0026#34;Beard_Service_Admin_$i\u0026#34; | Out-File \u0026#39;C:\\temp\\Admins.txt\u0026#39; -Append $i++ } Now that we have those users in files we can assign them to a variable by using Get-Content\n$Admins = Get-Content 'C:\\temp\\Admins.txt'\nOf course we can use any source for our users\na database an excel file Active Directory or even just type them in. We can use the Add-SQLLogin command from the sqlserver module to add our users as SQL Logins, but at present we cannot add them as database users and assign them to a role. If we want to add a Windows Group or a Windows User to our SQL Server we can do so using:\n1 Add-SqlLogin -ServerInstance $Server -LoginName $User -LoginType WindowsUser -DefaultDatabase tempdb -Enable -GrantConnectSql Notice that we need to enable and grant connect SQL to the user.\nIf we want to add a SQL login the code is pretty much the same but we either have to enter the password in an authentication box or pass in a PSCredential object holding the username and password. Keeping credentials secure in PowerShell scripts is outside the scope of this post and the requirement is for none-live environments so we will pass in the same password for all users as a string to the script. You may want or be required to achieve this in a different fashion.\n1 2 3 $Pass = ConvertTo-SecureString -String $Password -AsPlainText -Force $Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $User, $Pass Add-SqlLogin -ServerInstance $Server -LoginName $User -LoginType $LoginType -DefaultDatabase tempdb -Enable -GrantConnectSql -LoginPSCredential $Credential We can ensure that we are not trying to add logins that already exist using\n1 2 if(!($srv.Logins.Contains($User))) { The $srv is a SQL Server Management Server Object which you can create using a snippet. I blogged about snippets here and you can find my list of snippets on GitHub here.¬†However, today I am going to use the dbatools module to create a SMO Server Object using the Connect-DbaInstance command and assign the server and the database to a variable:\n1 2 3 # Create a SQL Server SMO Object $srv = Connect-DbaInstance -SqlInstance $server $db = $srv.Databases[$Database] Once we have our Logins we need to create our database users:\n1 2 3 $usr = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.User\u0026#39;) ($db, $User) $usr.Login = $User $usr.Create() and add them to a database role.\n1 2 #Add User to the Role $db.roles[$role].AddMember($User) I created a little function to call in the script and then simply loop through our users and admins and call the function.\n1 2 3 4 5 6 7 foreach ($User in $Users) { Add-UserToRole -Password $Password -User $user -Server $server -Role $Userrole -LoginType SQLLogin } foreach ($User in $Admins) { Add-UserToRole -Password $Password -User $user -Server $server -Role $adminrole -LoginType SQLLogin } To check that they have been added correctly I simply use the Get-DbaRoleMember;command from dbatools and output it to Out-GridView¬†using the alias ogv as I am on the command line:\n1 Get-DbaRoleMember -SqlInstance $server |ogv which looks like this:\nOnce we need to clean up the logins and users we can use the Get-SQLLogin and Remove-SQLLogin commands from the sqlserver module to remove the logins and if we do that first we can then use the dbatools command Remove-SQLOrphanuser to remove the orphaned users üôÇ (I thought that was rather cunning!)\n1 2 (Get-SqlLogin -ServerInstance $server).Where{$_.Name -like \u0026#39;*Beard_Service_*\u0026#39;}|Remove-SqlLogin Remove-SQLOrphanUser -SqlServer $Server -databases $database The Remove-SQLLogin will prompt for confirmation and the result of the Remove-SQLOrphanUser looks like this\nWhen you are looking at doing this type of automation with PowerShell, you should remember always to make use of Get-Command, Get-Help and Get-Member. That will enable you to work out how to do an awful lot. I have a short video on youtube about this:\nand when you get stuck come and ask in the SQL Server Slack at https://sqlps.io/slack. You will find a powershellhelp channel in there. Here is the complete code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 #Requires -module sqlserver #Requires -module dbatools ### Define some variables $server = \u0026#39;\u0026#39; $Password = \u0026#34;Password\u0026#34; $Database = \u0026#39;TheBeardsDatabase\u0026#39; $Admins = Get-Content \u0026#39;C:\\temp\\Admins.txt\u0026#39; $Users = Get-Content \u0026#39;C:\\temp\\Users.txt\u0026#39; $LoginType = \u0026#39;SQLLogin\u0026#39; $userrole = \u0026amp;nbsp; \u0026#39;Users\u0026#39; $adminrole = \u0026#39;Admin\u0026#39; # Create a SQL Server SMO Object $srv = Connect-DbaSqlServer -SqlServer $server $db = $srv.Databases[$Database] function Add-UserToRole { param ( [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true, ValueFromRemainingArguments = $false)] [ValidateNotNullOrEmpty()] [string]$Password, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true, ValueFromRemainingArguments = $false)] [ValidateNotNullOrEmpty()] [string]$User, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true, ValueFromRemainingArguments = $false)] [ValidateNotNullOrEmpty()] [string]$Server, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true, ValueFromRemainingArguments = $false)] [ValidateNotNullOrEmpty()] [string]$Role, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true, ValueFromRemainingArguments = $false)] [ValidateSet(\u0026#34;SQLLogin\u0026#34;, \u0026#34;WindowsGroup\u0026#34;, \u0026#34;WindowsUser\u0026#34;)] [string]$LoginType ) if (!($srv.Logins.Contains($User))) { if ($LoginType -eq \u0026#39;SQLLogin\u0026#39;) { $Pass = ConvertTo-SecureString -String $Password -AsPlainText -Force $Credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $User, $Pass Add-SqlLogin -ServerInstance $Server -LoginName $User -LoginType $LoginType -DefaultDatabase tempdb -Enable -GrantConnectSql -LoginPSCredential $Credential } elseif ($LoginType -eq \u0026#39;WindowsGroup\u0026#39; -or $LoginType -eq \u0026#39;WindowsUser\u0026#39;) { Add-SqlLogin -ServerInstance $Server -LoginName $User -LoginType $LoginType -DefaultDatabase tempdb -Enable -GrantConnectSql } } if (!($db.Users.Contains($User))) { # Add user to database $usr = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.User\u0026#39;) ($db, $User) $usr.Login = $User $usr.Create() } #Add User to the Role $db.roles[$role].AddMember($User) } foreach ($User in $Users) { Add-UserToRole -Password $Password -User $user -Server $server -Role $Userrole -LoginType SQLLogin } foreach ($User in $Admins) { Add-UserToRole -Password $Password -User $user -Server $server -Role $adminrole -LoginType SQLLogin } Get-DbaRoleMember -SqlInstance $server | ogv Happy Automating!\n","date":"2017-03-06T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2017/02/remove-them-all.png","permalink":"https://blog.robsewell.com/blog/quickly-creating-test-users-in-sql-server-with-powershell-using-the-sqlserver-module-and-dbatools/","title":"Quickly Creating Test Users in SQL Server with PowerShell using the sqlserver module and dbatools"},{"content":" ","date":"2017-02-27T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sql-vnext-sp_configure-on-windows-and-linux-with-dbatools/","title":"SQL VNext sp_configure on Windows and Linux with dbatools"},{"content":" ","date":"2017-02-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/adding-a-t-sql-job-step-to-a-sql-agent-job-with-powershell/","title":"Adding a T-SQL Job Step to a SQL Agent Job with PowerShell"},{"content":"I flew to Utrecht last week to present with Chrissy LeMaire and Sander Stad for the joint Dutch SQL and PowerShell User Groups. Whilst I was sat at the airport I got a phone call from my current client.\nThem - We need to change the backup path for all of the servers to a different share, how long will it take you?\nMe - About 5 minutes\n(PowerShell is very powerful ‚Äì be careful when following these examples üòâ )\nThis code was run using PowerShell version 5 and will not work on Powershell version 3 or lower as it uses the where method. Lets grab all of our jobs on the estate. (You will need to fill the $Servers variable with the names of your instances, maybe from a database or CMS or a text file)$Jobs = Get-SQLAgentJob -ServerInstance $Servers Once we have the jobs we need to iterate only through the ones we need to. This step could also have been done in the line above. Lets assume we are using the Ola Hallengren Solution to backup our estateForeach($job in $Jobs.Where{$.Name -like \u0026lsquo;DatabaseBackup\u0026rsquo; -and $.isenabled -eq $true}) Then because I have to target a specific job step I can iterate through those and filter in the same wayforeach ($Step in $Job.jobsteps.Where{$_.Name -like \u0026lsquo;DatabaseBackup\u0026rsquo;}) Now all I need to do is to replace C:\\Backup with C:\\MSSQL\\Backup (in this example I am using my labs backup paths)$Step.Command = $Step.Command.Replace(\u0026ldquo;Directory = N\u0026rsquo;C:\\Backup\u0026rsquo;\u0026rdquo;,\u0026ldquo;Directory = N\u0026rsquo;C:\\MSSQL\\Backup\u0026rsquo;\u0026rdquo;) And then call the Alter method$Step.Alter() And that is all there is to it. Here is the full script I used$Jobs = Get-SQLAgentJob -ServerInstance $Servers\nForeach($job in $Jobs.Where{$.Name -like \u0026lsquo;DatabaseBackup\u0026rsquo; -and $.isenabled -eq $true}) { foreach ($Step in $Job.jobsteps.Where{$_.Name -like \u0026lsquo;DatabaseBackup\u0026rsquo;}) { $Step.Command = $Step.Command.Replace(\u0026ldquo;Directory = N\u0026rsquo;C:\\Backup\u0026rsquo;\u0026rdquo;,\u0026ldquo;Directory = N\u0026rsquo;C:\\MSSQL\\Backup\u0026rsquo;\u0026rdquo;) $Step.Alter() } } In only a few minutes I had altered several hundred instances worth of Ola Hallengren Jobs üôÇ This is one of the many reasons I love PowerShell, it enables me to perform mass changes very quickly and easily. Of course, you need to make sure that you know that what you are changing is what you want to change. I have caused severe issues by altering the SQL alerts frequency to 1 second instead of one hour on an estate!! Although the beauty of PowerShell meant that I was able to change it very quickly once the problem was realisedYou can change a lot of settings. If you look at what is available at a job step levelHappy Automating\n","date":"2017-02-13T00:00:00Z","image":"https://dbatools.io/wp-content/uploads/2016/05/dbatools-logo-1.png","permalink":"https://blog.robsewell.com/blog/altering-a-job-step-on-hundreds-of-sql-servers-with-powershell/","title":"Altering a Job Step on Hundreds of SQL Servers with PowerShell"},{"content":" ","date":"2017-01-12T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-the-powershell-sql-provider-with-sql-authentication/","title":"Using the PowerShell SQL Provider with SQL Authentication"},{"content":" ","date":"2017-01-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/a-whole-day-of-powershell-and-sql-join-chrissy-lemaire-i-at-sqlsatvienna/","title":"A Whole Day of PowerShell and SQL ‚Äì Join Chrissy LeMaire \u0026 I at #sqlsatvienna"},{"content":"Reading this blog post by Shawn Melton Introduction of Visual Studio Code for DBAs reminded me that whilst I use Visual Studio Code (which I shall refer to as Code from here on) for writing PowerShell and Markdown and love how easily it interacts with Github I hadn‚Äôt tried T-SQL. If you are new to Code (or if you are not) go and read Shawns blog post but here are the steps I took to running T-SQL code using Code\nTo download Code go to this link https://code.visualstudio.com/download and choose your operating system. Code works on Windows, Linux and Mac\nOnce you have downloaded and installed hit CTRL SHIFT and P which will open up the command palette\nOnce you start typing the results will filter so type ext and then select Extensions : Install Extension\nWhich will open the Extensions tab ( You could have achieved the same end result just by clicking this icon)\nBut then you would not have learned about the command palette üôÇ\nSo, with the extensions tab open, search for mssql and then click install\nOnce it has installed the button will change to Reload so click it\nAnd you will be prompted to Reload the window\nAccept the prompt and then open a new file (CTRL N) and then change the language for the file.\nYou can do this by clicking CTRL K and then M (Not CTRL K CTRL M) or click the language button\nAnd then choose SQL\nThis will start a download so make sure you are connected (and allowed to connect to the internet)\nOnce it has finished it will show this\nAnd offer you the chance to read the release notes\nWhich you can get for any extension anytime by finding the extension in the extensions tab and clicking on it. This has links to tutorials as well as information about the release\nThe mssql extension enables Intellisence for T-SQL when you open a .sql file or when you change the language to SQL as shown above for a new file\nWrite your T-SQL Query and press CTRL SHIFT and E or Right Click and choose Execute Query. This will ask you to choose a Connection Profile (and display any existing profiles)\nChoose Create Connection Profile and answer the prompts\nThe query will then run\nYou can then output the results to csv or json if you wish\nYou can find a video showing this whole process with some typos and an error here\nUsing SQL with VS Code\n","date":"2017-01-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/running-sql-queries-with-visual-studio-code/","title":"Running SQL Queries with Visual Studio Code"},{"content":"Its¬†the time of year for reflection and I have had the most amazing 2016, I am blessed that I love what I do so much. I thoroughly enjoy writing and talking and sharing and commenting and supporting and cherishing all the SQL and PowerShell things. I wrote about using Power Bi to display my checkins. I only started this¬†in June and this is where I have been :-)\nI¬†learnt about Pester and ended the year incorporating it into dbatools and dbareports. I also started using GitHub¬†It is quite surprising to me how much time I now spend using both. I also had to start learning DSC¬†for the client I was working with because as \u0026rsquo;the PowerShell guy\u0026rsquo; I was the one who could the easiest. I learnt things and then forgot them causing me to find this Pester post via google later in the year!!¬†(That\u0026rsquo;s a big reason for blogging by the way)\nEarly in the year we organised with SQL Saturday Exeter\nhttps://www.facebook.com/mark.pryce.maher/videos/10153333580360863/?pnref=story.unseen-section\nThe Beard says\nWhen you go to an event -¬†Say thank you to the organisers and volunteers\nand a TERRIBLE thing happened - I broke my DBA Team mug\nLuckily the fine folk at redgate sorted me out with a replacement from deep in the stores somewhere and gave it to me at SQL Saturday Exeter :-) Thank you.\nI spoke at the PowerShell Conference Europe and met and made some great friends which lead to me speaking at the PowerShell Monday in Munich and the Dutch PowerShell Usergroup. SQL Saturday Dublin was a blast, its a wonderful city, Manchester had a whole PowerShell Track :-) and Cambridge was memorable for the appalling journey as well as the chance to share a stage with Chrissy. PowerShell Conference Asia in the sovereign city-state of Singapore was such a good event and place. Lastly of course was Slovenia with its fantastic Christmas lights and awesome event organisation. I visited some user groups too. Southampton run by my good friends John Martin¬†and Steph Middleton Congratulations to John on his first MVP award yesterday, Cardiff for the Return of the Battle of the Beards with Terry McCann and Tobiasz Koprowski where the projector threw its toys out of the pram and Birmingham in the school hall which was slightly chilly (theres a joke there for some people)\nAmazing things happened\nAnd that\u0026rsquo;s the biggest and bestest thing about this year. Some amazing new friends and spending¬†time with all my other friends. I started writing out a list but was terrified I would have missed someone out, so to all my friends\nTHANK YOU for a brilliant 2016 and 2017 shall be just as good :-)\nHere are a few of my pics from the year with a lot of my friends\n[gallery type=\u0026ldquo;circle\u0026rdquo; columns=\u0026ldquo;9\u0026rdquo; ids=\u0026ldquo;3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3077,3075,3074,3073,3067,3068,3069,3070,3071,3072,3066,3065,3064,3063,3062,3061,3055,3056,3057,3058,3059,3060,3054,3053,3052,3051,3050,3049,2943,2950,2897,2923,3046,2924,2927,3047,2933,3048\u0026rdquo;]\n","date":"2017-01-02T00:00:00Z","permalink":"https://blog.robsewell.com/blog/2016-that-was-a-year-/","title":"2016 - That was a Year :-)"},{"content":"This weekend (10 December 2016), I went to Slovenia for a SQL Saturday. As always, it was an amazing event well organised by Mladen Prajdic, Dejan Sarka, and Matija Lah in a fabulous setting amongst fabulous scenery. I highly recommend it and, also, Ljubljana is a wonderful place to be in December with all the lights and markets.\nWhilst I was there I was asked by someone if you could deploy data science virtual machines in Azure with PowerShell. I said I was sure that it could be done and agreed I would write a blog post, so here it is.\nAccording to the Azure documentation\nThe Data Science Virtual Machine running on a Windows Server 2012 contains popular tools for data exploration, modeling and development activities. The main tools include Microsoft R Server Developer Edition (An enterprise ready scalable R framework) , Anaconda Python distribution, Jupyter notebooks for Python and R, Visual Studio Community Edition with Python, R and node.js tools, Power BI desktop, SQL Server 2016 Developer edition including support In-Database analytics using Microsoft R Server. It also includes open source deep learning tools like Microsoft Cognitive Toolkit (CNTK 2.0) and mxnet; ML algorithms like xgboost, Vowpal Wabbit. The Azure SDK and libraries on the VM allows you to build your applications using various services in the cloud that are part of the Cortana Analytics Suite which includes Azure Machine Learning, Azure data factory, Stream Analytics and SQL Datawarehouse, Hadoop, Data Lake, Spark and more.\nI have created a function to wrap around the process to make it easier for none PowerShell¬†people to do this. There are a series of steps to follow below and you should be able to create a machine in about 10 minutes once you have completed the pre-requisites.\nEnable Programmatically Deployment First, an annoyance. To be able to deploy Data Science virtual machines in Azure programmatically you first have to login to the portal and click some buttons.\nIn the Portal¬†click new and then marketplace and then search for data science. Choose the Windows Data Science Machine and under the blue Create button you will see a link which says ‚ÄúWant to deploy programmatically? Get started‚Äù Clicking this will lead to the following blade.\nClick Enable and then save and you then move to PowerShell üôÇ\nAzure PowerShell Cmdlets Follow the instructions here¬†to install the Azure PowerShell modules. In the examples you see here I am using Windows 10 and PowerShell version 5.1.14393.479 and I installed the Azure modules using the Install-Module method\nGet the script To install a data science VM, we‚Äôll use the New-WindowsDataScienceVM.ps1 script. In this script, I‚Äôm using version 1.2, but any version of this script published in PowerShell Gallery is fine.\nTo install the New-WindowsDataScienceVM script from the PowerShell gallery, type:\nInstall-Script New-WindowsDataScienceVM\nFor more information about using the PowerShellGet cmdlets to install scripts and modules from PowerShell Gallery, read this page.¬†The PowerShellGet modules is included in PowerShell 5.0 and later on Windows 10, but you can install PowerShellGet for PowerShell 3.0 and 4.0. If you cannot connect to the gallery or prefer not to install the module, you can also find the script on GitHub.\nLogin to Azure You can login to Azure using the command\nLogin-AzureRMAccount\nwhich will pop-up a prompt for you to log into Azure\nEnable Simple Mode The New-WindowsDataScienceVM function comes with a Simple switch parameter.\nIf you use -Simple, the function prompts you only for the admin username and password for the virtual machine. It creates a randomly-named, standard_DS1_v2-sized machine in the ukwest data centre with standard, locally redundant storage in a randomly named Resource Group. All of the required objects have random names, too. If that is not what you want, there is more information at the end of this post. I am considering offering a pop-up to choose location in Simple Mode. Let me know here if that would be something you would like\nTo create a simple data science VM, run:\nNew-WindowsDataScienceVM -Simple\nEnter Local Admin Password When you run the function, it prompts for a local admin username and password to log into the virtual machine. The password must have 3 of the following 1 Upper case, 1 lower case, I special character and 1 number. Don‚Äôt lose it, you will need it.\nGrab a cuppa,¬†creating your VM and its resources¬†will take 5 ‚Äì 10 minutes. (In my testing it reliably took between 7 and 8 minutes) The screen will look like this\nWhen the script has finished running you will have deployed a set of resources like this\nLogin to the Virtual Machine Copy and paste the¬†correct code¬†from the output at the end of the script to launch the RDP session and save the RDP file to your documents folder for later use.\nOr you can find the Virtual machine name in the portal or by running\nGet-AzureRmVM -ResourceGroupName¬†\u0026lt;ResourceGroup\u0026gt; | Where-Object {$_.Name -like 'DSVM*'}\nYou can then use the code below¬†to download a RDP file and log into the virtual machine using this code\nGet-AzureRmRemoteDesktopFile -ResourceGroupName \u0026lt;ResourceGroup\u0026gt; -Name¬†\u0026lt;VMName\u0026gt; -LocalPath C:\\WIP\\DataScienceVM.rdp -Launch\nYou will need to login with the local admin account you set up previously, which means that you will need to click on more choices and then the machinename\\Username. In this case the machine name is DSVMZIAgd\nYou can copy the correct Virtual Machine name and Username from the output at the end of the script.\nIf you have forgotten your password, you can reset it in the Portal.\nEnjoy the Data Science Virtual Machine You are then logged in and can carry on. Once the Azure PowerShell modules and script are installed you would be able to have a machine up and running within 10 minutes.\nCleaning Up To remove the resource group and ALL resources in the resource group, including the data science VM, run:\nRemove-AzureRmResourceGroup -Name \u0026lt;ResourceGroup\u0026gt;¬†-Force\nThis will remove ALL resources in that resource group, so be careful if you have deployed anything else.\nCustomising the Deployment If you want to use different settings for the deployment or want to script the creation of a number of machines, you can run\nGet-Help New-WindowsDataScienceVM -Full\nand see all the options and further examples. Any questions please feel free to comment\n","date":"2016-12-18T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/12/rdp-file.png","permalink":"https://blog.robsewell.com/blog/deploying-a-windows-data-science-virtual-machine-to-azure-with-powershell-easily/","title":"Deploying a Windows Data Science Virtual Machine to Azure with PowerShell easily"},{"content":"Last week at the Birmingham user group I gave a presentation about PowerShell and SQL Server\nIt was a very packed session as I crammed in the new sqlserver module, dbatools and dbareports üôÇ On reflection I think this is a bit too much for a one hour session but at the end of the session I demo‚Äôd live Cortana using the dbareports dataset and returning a Cortana PowerBi page.\nAs always it took a couple of goes to get it right but when it goes correctly it is fantastic. I call it a salary increasing opportunity! Someone afterwards asked me how it was done so I thought that was worth a blog post\nThere is a video below but the steps are quite straightforward.\nAdd Cortana Specific Pages Whilst you can just enable Cortana to access your dataset, as shown later in this post, which¬†enables Cortana to search available datasets and return an appropriate visualisation it is better to provide specific pages for Cortana to use and display. You can do this in PowerBi Desktop\nStart¬†by adding a new page in your report by clicking on the plus button\nand¬†then change the size of the report page by clicking on the paintbrush icon in the visualisation page.\nThis creates a page that is optimised for Cortana to display and also will be the first place that Cortana will look to answer the question\nPower BI first looks for answers in Answer Pages and then searches your datasets and reports for other answers and displays them in the form of visualizations. The highest-scoring results display first as best matches, followed by links to other possible answers and applications. Best matches come from Power BI Answer Pages or Power BI reports.\nRename the page so that it contains the words or phrase you expect to be in the question such as ‚ÄúServers By Version‚Äù You will help Cortana and PowerBi to get your results better if you use some of the column names in your dataset\nThen it is just another report page and you can add visualisations just like any other page\nMake Cortana work for you and your users If your users are likely to use a number of different words in their questions you can assist Cortana to find the right answer by adding alternate names. So maybe if your¬†page is sales by store you might add¬†shop, building, results, amount, orders.¬†This is also useful when Cortana doesn‚Äôt understand the correct words as you will notice in the screenshot below I have added ‚Äúservice‚Äù for ‚Äúservers‚Äù and ‚Äúbuy‚Äù for ‚Äúby‚Äù to help get the right answer.¬†You can add these alternate words by clicking the paintbrush under visualisations and then Page Information\nPublish your PBIX file to PowerBi.com To publish your PowerBi report to PowerBi.com either via the Publish button in PowerBi desktop\nor by using the PowerBiPS module\n1 2 3 4 Install-Module -Name PowerBIPS #Grab the token, will require a sign in $authToken = Get-PBIAuthToken ‚ÄìVerbose Import-PBIFile ‚ÄìauthToken $authToken ‚ÄìfilePath ‚ÄúPath to PBIX file‚Äù ‚Äìverbose Enable Cortana In your browser log into https://powerbi.com and then click on the cog and then settings\nthen click on Datasets\nThen choose the dataset ‚Äì in this case dbareports SQL Information sample and click the tick box to Allow Cortana to access the this dataset and then click apply\nUse Cortana against your PowerBi data You can type into the Cortana search box and it will offer the opportunity for you to choose your PowerBi data\nbut it is so much better when you let it find the answer üôÇ\nand if you want to go to the PowerBi report there is a handy link at the bottom of the Cortana page\nI absolutely love this, I was so pleased when I got it to work and the response when I show people is always one of wonder for both techies and none-techies alike\nThe conditions for Cortana to work You will need to have added your work or school Microsoft ID to the computer or phone that you want to use Cortana on and that account must be able to access the dataset either because it is the dataset owner or because a dashboard using that dataset has been shared with that account.\nFrom this page on PowerBi.com\nWhen a new dataset or custom Cortana Answer Page is added to Power BI and enabled for Cortana it can take up to 30 minutes for results to begin appearing in Cortana. Logging in and out of Windows 10, or otherwise restarting the Cortana process in Windows 10, will allow new content to appear immediately.\nIt‚Äôs not perfect! When you start using Cortana to query your data you will find that at times it is very frustrating. My wife was in fits of giggles listening to me trying to record the video below as Cortana refused to understand that I was saying ‚Äúservers‚Äù and repeatedly searched Bing for ‚Äúservice‚Äù Whilst you can negate the effect by using the alternate names for the Q and A settings it is still a bit hit and miss at times.\nIt is amazing There is something about giving people the ability to just talk to their device in a meeting and for example with dbareports ask\nWhich clients are in Bolton\nor\nWhen was the last backup for client The Eagles\nand get the information they require and a link to the report in PowerBi.com. I am certain that the suits will be absolutely delighted at being able to show off in that way which is why I call it a salary increasing opportunity üôÇ\nhttps://youtu.be/-bPqhDK3WGs\nWe would love YOU to come and join us at the SQL Community Collaborative Help us make dbatools, dbareports and Invoke-SQLCmd2 even better. You can join in by forking the repos in GitHub and writing your code and then performing a PR but we would much rather that you came and discussed new requests in our Trello boards, raised issues in GitHub and generally discussed the modules in the SQL Server Community Slack #dbatools #dbareports. We are also looking for assistance with our wiki pages, Pester tests and appveyor integration for our builds and any comments people want to make\nSQL Server Collaborative GitHub Organisation holding the modules. Go here to raise issues, fork the repositories or download the code\ndbatools Trello for discussion about new cmdlets\nSQL Server Community Slack where you can find #dbatools and #dbareports as well as over 1100 people discussing all aspects of the Data Platform, events, jobs, presenting\nCOME AND JOIN US\n","date":"2016-11-13T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/11/cortana-search-2.png","permalink":"https://blog.robsewell.com/blog/enabling-cortana-for-dbareports-powerbi/","title":"Enabling Cortana for dbareports PowerBi"},{"content":"This is a blog post for¬†this month\u0026rsquo;s T-SQL Tuesday post, hosted by Andy Yun (b|t). T-SQL Tuesday is a monthly blog event started by Adam Machanic (b|t). The T-SQL Tuesday topic this month was about advice for new speakers. Thanks Andy for hosting. I have created a channel in the SQL Server Community Slack for presenting which everyone can make use of to ask and to answer questions\nI think you should share what you know with others. You will be amazing.\nI will give you some great advice I learnt from a blog post\nStart speaking Keep going Listen to feedback That‚Äôs it. Kendra has said it all, you don\u0026rsquo;t need to read any further ;-)\nHowever..\nNot all plain sailing I love giving sessions but I never knew or thought¬†that I would. My journey to speaking started at my SQL user group in Exeter and two fabulous people Jonathan and Annette Allen¬†who encouraged me to share some PowerShell with the group. I was terrified, didn\u0026rsquo;t think I was worthy,¬†my HDMI output wasn\u0026rsquo;t strong enough to power the projector, I had to transfer my slides and demo to Jonathans laptop. It was a fraught and frustrating experience.\nMy second presentation was done on Stuart Moores MacBook Pro¬†using Office Online for presentations and Azure for demos. Again a change right at the last minute and using a machine I didn\u0026rsquo;t know (and a different keyboard set-up).\nStuff will go wrong. Murphy\u0026rsquo;s Law will always show his head somewhere and no matter how often you test and re-test your demos, sometimes an odd thing will make them stop working\nThere will be problems and issues, you can mitigate some of them by following the 6 P\u0026rsquo;s\nProper Preparation Prevents Pretty Poor Performance.\nYou can read some great blog posts in this T-SQL Tuesday¬†Series and also this one from Steve Jones¬†or any of these¬†But also accept that these things happen and you must be prepared to shine on through the darkness if the power runs out or use pen and paper or even plastic cups like John Martin¬†:-)\nYou never know you might enjoy it I found I enjoyed it and wanted to do more and since then I have presented sessions in a wide variety of places. It was very strange to have been sat watching and listening to all of these fantastic presenters thinking I could never do that and then find out that actually it is something that I enjoy doing and find fun. You can do that too.\nEqually, it\u0026rsquo;s ok to not enjoy it, think its not worth the stress and hassle and support the community in a different way but at least give it a go\nYou will be nervous I shared a train across Germany with someone who had attended the PSMonday conference in Munich and they were astonished when I¬†said that I get very nervous before speaking. It\u0026rsquo;s ok to be nervous, the trick is to make use of that nervous energy and turn it into something positive.\nI get very nervous before presentations. My hands shake, I sweat, I either babble or¬†loose my voice. I fret and fidget and check everything a thousandillion times.¬†I find it is better for me if I am sat in the room during the previous presentation as that generally helps me to feel more relaxed as I can listen to their talk¬†and also out of respect for the presenter and the organisation it forces me to sit quietly.\nYou will find your own way to deal with this, maybe listening to music on headphones or just sitting quietly somewhere. Don\u0026rsquo;t worry if it is not immediately obvious, try some different¬†things, talk with others and believe me, it will be ok.\nDon\u0026rsquo;t try to numb it with alcohol\nOnce I get up and its \u0026lsquo;my\u0026rsquo; turn I take a few deep breaths and suddenly presenter turns on and I forget all about being nervous.\nSomething to talk about I have nothing to talk about.\nOr everyone else knows more than I do.\nOr X Y and Z talk about this much better than I do.\nI\u0026rsquo;m scared\nRichard Munn¬†and I gave an impromptu session at SQL Relay in Cardiff where we talked about and hopefully encouraged people to start speaking and these statements came up.\nHeres (a little of) what we said\nNo-one knows everything. Many people know a hell of a lot but not everything. You know a lot more than you realise and you also know things that no-one else does.\nIf you are stuck for things to talk about think about the you of 6 months or a year ago and something that you have learnt in that time and write the session that you wish you could have seen then. There will be other people at a similar stage who will appreciate it.\nDon\u0026rsquo;t be scared, they are only people.\nPractice My dog is the one person who has been present at my presentations the most. He has listened (sometimes intently) to me practicing.\nYou need to practice speaking out loud.\nYou need to understand the timings\nYou need to be comfortable with hearing yourself speaking out aloud\nYou need to practice speaking out loud\nA double reminder because I think it is important. You should practice and practice and practice with an eye on your timings and if you have a good friend who is technical or a small group at work for a lunchtime maybe then ask them if they will listen and give feedback.\nWanna chat? I am very passionate about community involvement and lucky enough to be involved in two fantastic communities - the SQL community and the PowerShell community and have made some great friends along the way. I was amazed and proud when very soon after my second presentation someone told me that I had inspired them to start to present.\nSince then I have gone out of my way to encourage other people to speak and to blog and am really enjoying watching them blossom. If you want to have a chat via email or via slack¬†about speaking or blogging or getting involved in the community please feel free to contact me and I promise you I will get back to you. Better still go to the SQL Community Slack and ask questions in #presentingorspeaking\nGo find out more We are good at sharing and learning technical content but we can share and learn about so much more, about all aspects of our life. Go and read all of the other posts in this T-SQL Tuesday for starters :-) and develop\n","date":"2016-11-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/speaking-you-go-on.-tsql2sday-84/","title":"Speaking? You? Go on. #tsql2sday #84"},{"content":"For my own amusement and also to show my wife where I have been I use the Swarm check-in app on my phone and check-in to places. Also for my own amusement I used PowerBi to visualise the data via the¬†API¬†and allow me to filter it in various ways.\nWhilst at the PowerShell Conference in Asia I was showing the mobile app to a group over some food and saying how easy it was and June Blender,¬†the mother of PowerShell help,¬†said that I ought to blog about it. So I have üôÇ\nFollow these steps and you can create this report.\nand add your own¬†access token¬†to it should you wish. Details at the end of the post\nI am using the swarm API but the principle is the same for any other API that provides you with data. For example, I used the same principles to create the embedded reports on the PASS PowerShell Virtual Chapter page showing the status of the cards suggesting improvements to the sqlserver module for the product team to work on. Hopefully, this post will give you some ideas to work on and show you that it is quite easy to get excellent data visualisation from APIs\nFirst up we need to get the data. I took a look at the Swarm developers page¬†( The Trello is here by the way) I had to register for an app, which gave me a client id and a secret. I then followed the steps here to get my user token¬†I was only interested in my own check ins so I used the steps under Token flow Client applications to get my¬†access token which I used in an URL like this.\nhttps://api.foursquare.com/v2/users/self/checkins?limit=5000\u0026amp;oauth_token=ACCESS_TOKEN\u0026amp;v=YYYYMMDD\nI added the limit 5000 as the default number of checkins returned was too small for my needs and the date was that days date.\nYou can do this in Powershell using code I got from the magnificent Stephen Owen‚Äôs blog post\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ## Enter the details $Clientid =\u0026#39;\u0026#39;¬†## Enter ClientId from foursquare $redirect = \u0026#39;\u0026#39; ## enter redirect url from client app in foursquare ##Create the URL: $URL = \u0026#34;https://foursquare.com/oauth2/authenticate?client_id=$Clientid\u0026amp;response_type=token\u0026amp;redirect_uri=$redirect\u0026#34; ## function from https://foxdeploy.com/2015/11/02/using-powershell-and-oauth/ Function Show-OAuthWindow { Add-Type -AssemblyName System.Windows.Forms $form = New-Object -TypeName System.Windows.Forms.Form -Property @{Width=440;Height=640} $web¬†= New-Object -TypeName System.Windows.Forms.WebBrowser -Property @{Width=420;Height=600;Url=($url -f ($Scope -join \u0026#34;%20\u0026#34;)) } $DocComp¬†= { $Global:uri = $web.Url.AbsoluteUri if ($Global:Uri -match \u0026#34;error=[^\u0026amp;]*|code=[^\u0026amp;]*\u0026#34;) {$form.Close() } } $web.ScriptErrorsSuppressed = $true $web.Add_DocumentCompleted($DocComp) $form.Controls.Add($web) $form.Add_Shown({$form.Activate()}) $form.ShowDialog() | Out-Null } #endregion #login to get an access code then close the redirect window Show-OAuthWindow -URL $URl ## grab the token $regex = \u0026#39;(?\u0026lt;=access_token=)(.*)\u0026#39; $authCode¬†= ($uri | Select-string -pattern $regex).Matches[0].Value $global:AuthToken = $authCode Write-output \u0026#34;Received a token, $AuthToken\u0026#34; Write-Output \u0026#34;So the URL for your PowerBi Data is :-\u0026#34; $PowerBiUrl = \u0026#34;https://api.foursquare.com/v2/users/self/checkins?limit=5000\u0026amp;oauth_token=$AuthToken\u0026amp;v=20160829\u0026#34; $PowerBiUrl | Clip I checked the URL in a browser and confirmed that it returned a json object. Keep that URL safe you will need it in a minute. That code above has placed it in your clipboard. If you want to jump straight to the report using the download stop here and go to the end\nSo now lets move to Power BI. Go to powerbi.com¬†and download the PowerBi Desktop. Its free. You will need to create an account using a school or work email address if you wish to put your reports in powerbi.com\nOnce you have downloaded and installed PowerBi Desktop you will be faced with a window like this\nStart by clicking Get Data\nThen choose Web and paste the URL from above into the filename and press ok which will give you this\nNow we need to put the data into a format that is of more use to us\nI clicked on the record link for response, then converted to table, then the little icon at the top of the column to expand the value.items column¬†and then the value.items column again. It doesn‚Äôt look much yet but we are a step¬†closer.\nNext I looked in the table for the venue column, expanded that and the location column and the formatted address column.\nYou can also expand the categories so that you can look at those too by expanding Value.items.venue.categories and Value.items.venue.categories1\nNow you will see that we have some duplicates in the data so we need to remove those. I did that by deleting the first 3 columns and then clicking remove duplicates under Delete Rows\nThen click close and apply. Then click on the data button¬†as we need to rename and remove some more columns so that our data makes a little sense. I renamed the columns like this\nValue.items.createdAt ‚Äì\u0026gt; CreatedAt Value.items.shout ‚Äì\u0026gt; Comment Value.items.venue.name ‚Äì\u0026gt; VenueName Value.items.venue.location.address ‚Äì\u0026gt; VenueAddress Value.items.timeZoneOffset ‚Äì\u0026gt; TimeZoneOffset Value.items.venue.location.lat ‚Äì\u0026gt; VenueLat Value.items.venue.location.lng ‚Äì\u0026gt; VenueLong Value.items.venue.location.postalCode ‚Äì\u0026gt; VenuePostalCode Value.items.venue.location.cc ‚Äì\u0026gt; CountryCode Value.items.venue.location.city ‚Äì\u0026gt; City Value.items.venue.location.state ‚Äì\u0026gt; State Value.items.venue.location.country ‚Äì\u0026gt; Country Value.items.venue.location.formattedAddress ‚Äì\u0026gt; VenueAddress Value.items.venue.url ‚Äì\u0026gt; VenueURL Value.items.venue.categories.name ‚Äì\u0026gt; Catogory Value.items.venue.categories.pluralName ‚Äì\u0026gt; Categories and remove all of the other columns. You can also do this in the Edit Queries window, I am just showing you that there are multiple ways to do the same thing\nOnce you have done that you should have a window that looks like this. Notice I renamed the query to checkins as well\nNow we need to create a calculated column for the time and a measure for the count of checkins. This is done using this code\nTime = VAR UnixDays = [createdAt]/(60*60*24) RETURN (DATEVALUE(\u0026quot;1/1/1970\u0026quot;)+UnixDays)\nCountCheckins = COUNT(checkins[Time])\nand we can move onto the report side of things. Frist we are going to download a custom visual. Go to the PowerBi Custom Visuals Page and download the Timeline visual\nand then import it into your PowerBi report. I have embedded a YouTube video below showing the steps I took to turn this into the PowerBi report. Its pretty easy, you will be able to click on the visuals and then click on the data columns and alter them until you have the report that you want.\nOnce you have done this, you can upload it to PowerBi if you wish by clicking on the Publish button in PowerBi desktop and signing into PowerBi.com with your work email address.\nand your report is available for you on PowerBi.com üôÇ By clicking on the pins on a visualisation you can add them to a dashboard.\nOnce you have a dashboard you can then use the natural language query to ask questions of your data. Here are some examples\nHow many checkins are in GB\nHow many checkins are in airports\nHow many checkins by month\nHow many checkins by month in GB\nWhich airports\nShow me hotel venuename and time\nHow many hotels by country\nShow me hotel venuename and checkins count\nmetro stations venuename and count checkins as a map\nShow me count checkins in Amsterdam by category as a donut\nIf you want to use the blank report, download it from here¬†open it in PowerBi Desktop, click Edit Queries and Source and add your own URL and click Apply and then Refresh\nHopefully, this has given you some ideas of ways that you can create some reports from many of the data sources available to you via API\n","date":"2016-10-29T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/10/powerbi6.png%29%5D%28/assets/uploads/2016/10/powerbi6.png","permalink":"https://blog.robsewell.com/blog/powerbi-and-api-visualising-my-checkins/","title":"PowerBi and API ‚Äì Visualising my Checkins"},{"content":"I have just got back to the UK from Singapore following the amazing PSConfAsia conference. I must say that Matt, Milton, Sebastian and Ben did a fantastic job organising this conference and were proud that there was a¬†notable increase in attendees from last year.\nThe conference began (unofficially) with a PowerShell User group session in the Microsoft Offices on Wednesday where Ravi Chaganti spoke about DSC\nand then Desmond Lee lead a Q and A session. In the end we decided that all the answers were\nIt Depends and Test in your Environment\nThat evening, I even managed to jump on the PASS PowerShell Virtual Chapter session by Scott Sutherland Hacking SQL Servers on Scale using PowerShell the recording of which is here\nA¬†session organised and managed online¬†in three different time zones by Aaron Chrissy and myself :-).\nOn Thursday the conference proper started with a pre-con day at the Amazon Web Services office. Yes, you read that right. This conference really highlighted the cross-platform¬†direction¬†and adoption of open-source that Microsoft is taking.¬†Jason Yoder spent all day teaching a group \u0026ldquo;PowerShell for Beginners\u0026rdquo; in one room\nwhile The Amazon Web Services Team showed DevOps on AWS with PowerShell in the morning and June Blender gave a SAPIEN Toolmaking Seminar.fter this we went back to the Microsoft Offices for another User Group where Jason Yoder gave a (nother) session with Jaap Brasser on PowerShell Tips and Tricks (Demo)\nFriday started with The PowerShell Team represented by Kenneth Hansen \u0026amp; Angel Calvo talking about PowerShell Past, Present and Future. It was really good that there was such great access to the product team at the conference and I saw lots of interaction around the conference as well, in addition to the sessions they provided.\nNext up for me was another¬†session from the PowerShell Team, this time Hemant Mahawar \u0026amp; Jason Shirk taking us on¬†a Journey Through the Ages of PowerShell Security\nExecution Policy is not a security feature\nThat took us to lunch, we were treated to excellent lunches at this conference\nAfter lunch I sat in the PowerShell Teams Ask Us Anything session although I was mainly preparing for my own session PowerShell Profile Prepares Perfect Production Purlieu which followed. There were excellent sessions on JEA, Nano Server, Chef and DSC, Containers, ETS and securing PowerShell against malware whilst I attended Flynn Bundy\u0026rsquo;s session about Windows Containers and Building GUIs with XAML with David Das Neves\nThat evening, organisers, speakers and attendees all went to the Penny Black pub on Marina Bay and enjoyed some food, refreshments and networking\nSaturday started slowly after the rain (another impressive \u0026lsquo;feature\u0026rsquo; of Singapore)¬†but the first session was a brilliant one with Hemant Mahawar \u0026amp; Jason Shirk talking Pragmatic PowerShell and answering questions. I am glad Jason used Carnac¬†to show what he was typing so that people could (just about¬†:-) ) keep up. I then attended the excellent session about contribution with Microsoft.\nThe rest of the day had amazing sessions on Azure Automation, IoT, AWS Cloud Formation, Centralised Repository Server, Chef, Puppet, Professional Help, Nano Server, Docker, DSC, Release Pipeline and of course some bearded fella talking about Installing SQL Scripts and creating Pester Tests for them and combining PowerShell, SQL, SSRS, PowerBi and Cortana :-)\nMy takeaways from the conference were that Microsoft is very open to all members of the open source community, DevOps is a very important topic¬†and also the following points from the PowerShell team\nPowerShell Team want YOU to contribute. Interact with them File bugs Feature Requests Documentation Tests Code\nand\nFixing is better than complaining :-) @HemanMahawar #psconfasia You can help fix the documentation. Use the contribute button on the doc\nand\nIf you are thinking of starting or run¬†a PowerShell usergroup Microsoft would like help. Tag¬†1 of the team such as @ANGELCALVOS #psconfasia\nSpecial thanks and congratulations must go to Matt, Milton, Sebastian and Ben for their excellent organisation and for creating an awesome event. I am looking forward to seeing how they can better it next year and also hoping that seeing all the fabulous speakers and sessions will inspire some attendees from this years event to share their own knowledge and experience at local user groups and even next years conference.\n","date":"2016-10-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/psconfasia-2016/","title":"PSConfAsia 2016"},{"content":"If you are a SQL DBA you will have heard of Ola Hallengrens Maintenance solution If you haven‚Äôt go and click the link and look at the easiest way to ensure that all of your essential database maintenance is performed. You can also watch a video from Ola at SQL Bits Recently I was thinking about how I could validate that this solution was installed in the way that I wanted it to be so I turned to Pester¬†You can find a great how to get started here¬†which will show you how to get Pester and how to get started with TDD. This isn‚Äôt TDD though this is Environment Validation and this is how I went about creating my test. First I thought about what I would look for in SSMS when I had installed the maintenance solution and made a list of the things that I would check which looked something like this. This would be the checklist you would create (or have already created) for yourself or a junior following this install. This is how easy you can turn that checklist into a Pester Test and remove the human element and open your install for automated testing\nSQL Server Agent is running ‚Äì Otherwise the jobs won‚Äôt run üôÇ We should have 4 backup jobs with a name of DatabaseBackup ‚Äì SYSTEM_DATABASES ‚Äì FULL DatabaseBackup ‚Äì USER_DATABASES ‚Äì FULL DatabaseBackup ‚Äì USER_DATABASES ‚Äì DIFF DatabaseBackup ‚Äì USER_DATABASES ‚Äì LOG We should have Integrity Check and Index Optimisation Jobs We should have the clean up jobs All jobs should be scheduled All jobs should be enabled The jobs should have succeeded I can certainly say that I have run through that check in my head and also written it down in an installation guide in the past. If I was being more careful I would have checked if there were the correct folders in the folder I was backing up to.\nOla‚Äôs script uses a default naming convention so this makes it easy. There should be a SERVERNAME or SERVERNAME$INSTANCENAME folder or if there is an Availability Group a CLUSTERNAME$AGNAME and in each of those a FULL DIFF and LOG folder which I can add to my checklist\nSo now we have our checklist we just need to turn in into a Pester Environmental Validation script\nIt would be useful to be able to pass in a number of instances so we will start with a foreach loop and then a Describe Block then split the server name and instance name, get the agent jobs and set the backup folder name\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ServerName = $Server.Split(\u0026#39;\\\u0026#39;)[0] $InstanceName = $Server.Split(\u0026#39;\\\u0026#39;)[1] $ServerName = $ServerName.ToUpper() Describe \u0026#39;Testing $Server Backup solution\u0026#39;{ BeforeAll {$Jobs = Get-SqlAgentJob -ServerInstance $Server $srv = New-Object Microsoft.SQLServer.Management.SMO.Server $Server $dbs = $Srv.Databases.Where{$_.status -eq \u0026#39;Normal\u0026#39;}.name if($InstanceName) { $DisplayName = \u0026#39;SQL Server Agent ($InstanceName)\u0026#39; $Folder = $ServerName + \u0026#39;$\u0026#39; + $InstanceName } else { $DisplayName = \u0026#39;SQL Server Agent (MSSQLSERVER)\u0026#39; $Folder = $ServerName } } if($CheckForBackups -eq $true) { $CheckForDBFolders -eq $true } $Root = $Share + \u0026#39;\\\u0026#39; + $Folder I also set the Agent service display name so I can get its status. I split the jobs up using a Context block, one each for Backups, Database maintenance and solution clean up but they all follow the same pattern. .First get the jobs\n1 $Jobs = $Jobs.Where{($_.Name -like \u0026#39;DatabaseBackup - SYSTEM_DATABASES - FULL*\u0026#39; + $JobSuffix + \u0026#39;*\u0026#39;) -or ($_.Name -like \u0026#39;DatabaseBackup - USER_DATABASES - FULL*\u0026#39; + $JobSuffix + \u0026#39;*\u0026#39;) -or ($_.Name -like \u0026#39;DatabaseBackup - USER_DATABASES - DIFF*\u0026#39; + $JobSuffix + \u0026#39;*\u0026#39;) -or ($_.Name -like \u0026#39;DatabaseBackup - USER_DATABASES - LOG*\u0026#39; + $JobSuffix + \u0026#39;*\u0026#39;)} Then we can iterate through them and check them but first lets test the Agent Service. You do this with an It Block and in it put a single test like this\nactual-value | Should Be expected-value\nSo to check the Agent Job is running we can do this\n1 (Get-service -ComputerName $ServerName -DisplayName $DisplayName).Status | Should Be \u0026#39;Running\u0026#39; To find out how to get the right values for any test I check using get member so to see what is available for a job I gathered the Agent Jobs into a variable using the Get-SQLAgentJob command in the new sqlserver module (which you can get by installing the latest SSMS from here)¬†and then explored their properties using Get-Member and the values using Select Object\n1 2 3 $jobs = Get-SqlAgentJob -ServerInstance $server ($Jobs | Get-Member -MemberType Property).name $Jobs[0] | Select-Object * then using a foreach to loop through them I can check that the jobs, exists, is enabled, has a schedule and succeeded last time it ran like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $Jobs = $Jobs.Where{($_.Name -eq \u0026#39;DatabaseIntegrityCheck - SYSTEM_DATABASES\u0026#39;) -or ($_.Name -eq \u0026#39;DatabaseIntegrityCheck - USER_DATABASES\u0026#39;) -or ($_.Name -eq \u0026#39;IndexOptimize - USER_DATABASES\u0026#39;)} foreach($job in $Jobs) { $JobName = $Job.Name It \u0026#39;$JobName Job Exists\u0026#39;{ $Job | Should Not BeNullOrEmpty } It \u0026#39;$JobName Job is enabled\u0026#39; { $job.IsEnabled | Should Be \u0026#39;True\u0026#39; } It \u0026#39;$JobName Job has schedule\u0026#39; { $Job.HasSchedule | Should Be \u0026#39;True\u0026#39; } if($DontCheckJobOutcome -eq $false) { It \u0026#39;$JobName Job succeeded\u0026#39; { $Job.LastRunOutCome | Should Be \u0026#39;Succeeded\u0026#39; } } So I have checked the agent and the jobs and now I want to check the folders exist. First for the¬†instance using Test-Path so the user running the PowerShell session must have privileges and access to list the files and folders\n1 2 3 4 Context \u0026#39;$Share Share For $Server\u0026#39; { It \u0026#39;Should have the root folder $Root\u0026#39; { Test-Path $Root | Should Be $true } The for every database we need to set some variables for the Folder path. We don‚Äôt back up tempdb so we ignore that and then check if the server is SQL2012 or above and if it is check if the database is a member of an availability group and set the folder name appropriately\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 foreach($db in $dbs.Where{$_ -ne \u0026#39;tempdb\u0026#39;}) { if($Srv.VersionMajor -ge 11) { If($srv.Databases[$db].AvailabilityGroupName) { $AG = $srv.Databases[$db].AvailabilityGroupName $Cluster = $srv.ClusterName $OLAAg = $Cluster + \u0026#39;$\u0026#39; + $AG if($Share.StartsWith(\u0026#39;\\\\\u0026#39;) -eq $False) { $UNC = $Share.Replace(\u0026#39;:\u0026#39;,\u0026#39;$\u0026#39;) $Root = \u0026#39;\\\\\u0026#39; + $ServerName + \u0026#39;\\\u0026#39; + $UNC + \u0026#39;\\\u0026#39; + $OlaAG } else { $Root = \u0026#39;\\\\\u0026#39; + $ServerName + \u0026#39;\\\u0026#39; + $UNC + \u0026#39;\\\u0026#39; + $Folder } } else { if($Share.StartsWith(\u0026#39;\\\\\u0026#39;) -eq $False) { $UNC = $Share.Replace(\u0026#39;:\u0026#39;,\u0026#39;$\u0026#39;) $Root = \u0026#39;\\\\\u0026#39; + $ServerName + \u0026#39;\\\u0026#39; + $UNC + \u0026#39;\\\u0026#39; + $Folder } else { $Root = $Share + \u0026#39;\\\u0026#39; + $Folder } } } $db = $db.Replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;) $Dbfolder = $Root + \u0026amp;amp;quot;\\$db\u0026amp;amp;quot; $Full = $Dbfolder + \u0026#39;\\FULL\u0026#39; $Diff = $Dbfolder + \u0026#39;\\DIFF\u0026#39; $Log¬†= $Dbfolder + \u0026#39;\\LOG\u0026#39; If($CheckForDBFolders -eq $True) { Context \u0026amp;amp;quot;Folder Check for $db on $Server on $Share\u0026amp;amp;quot; { It \u0026amp;amp;quot;Should have a folder for $db database\u0026amp;amp;quot; { Test-Path $Dbfolder |Should Be $true } But we need some logic for checking for folders because Ola is smart and checks for Log Shipping databases so as not to break the LSN chain and system databases only have full¬†folders and simple recovery databases only have full and diff folders. I used the System.IO.Directory Exists method as I found it slightly quicker for UNC Shares\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 If($CheckForDBFolders -eq $True) { Context \u0026#39;Folder Check for $db on $Server on $Share\u0026#39; { It \u0026#39;Should have a folder for $db database\u0026#39; { Test-Path $Dbfolder |Should Be $true } if($Db -notin (\u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;model\u0026#39;) -and ($Srv.Databases[$db].RecoveryModel -ne \u0026#39;Simple\u0026#39;) -and ( $LSDatabases -notcontains $db)) { It \u0026#39;Has a Full Folder\u0026#39; { [System.IO.Directory]::Exists($Full) | Should Be $True } It \u0026#39;Has a Diff Folder\u0026#39; { [System.IO.Directory]::Exists($Diff) | Should Be $True } It \u0026#39;Has a Log Folder\u0026#39; { [System.IO.Directory]::Exists($Log) | Should Be $True } } # elseif(($Srv.Databases[$db].RecoveryModel -eq \u0026#39;Simple\u0026#39;) -and $Db -notin (\u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;model\u0026#39;) -or ( $LSDatabases -contains $db) ) { It \u0026#39;Has a Full Folder\u0026#39; { [System.IO.Directory]::Exists($Full) | Should Be $True } It \u0026#39;Has a Diff Folder\u0026#39; { [System.IO.Directory]::Exists($Diff) | Should Be $True } } # else { It \u0026#39;Has a Full Folder\u0026#39; { [System.IO.Directory]::Exists($Full) | Should Be $True } }# } # End Check for db folders } and a similar thing for the files in the folders although this caused me some more issues with performance. I first used Get-ChildItem but in folders where a log backup is running every 15 minutes it soon became very slow. So I then decided to compare the create time of the folder with the last write time which was significantly quicker for directories with a number of files but then fell down when there was a single file in the directory so if the times match I revert back to Get-ChildItem.\nIf anyone has a better more performant option I would be interested in knowing. I used √òyvind Kallstad PowerShell Conference session Chasing the seconds Slides and Video\nand tried the methods in there with Measure-Command but this was the best I came up with\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 If($CheckForBackups -eq $true) { Context \u0026#39; File Check For $db on $Server on $Share\u0026#39; { $Fullcreate = [System.IO.Directory]::GetCreationTime($Full) $FullWrite = [System.IO.Directory]::GetLastWriteTime($Full) if($Fullcreate -eq $FullWrite) { It \u0026#39;Has Files in the FULL folder for $db\u0026#39; { Get-ChildItem $Full*.bak | Should Not BeNullOrEmpty } } else { It \u0026#39;Has Files in the FULL folder for $db\u0026#39; { $FullCreate | Should BeLessThan $FullWrite } } It \u0026#39;Full File Folder was written to within the last 7 days\u0026#39; { $Fullwrite |Should BeGreaterThan (Get-Date).AddDays(-7) } if($Db -notin (\u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;model\u0026#39;)) { $Diffcreate = [System.IO.Directory]::GetCreationTime($Diff) $DiffWrite = [System.IO.Directory]::GetLastWriteTime($Diff) if($Diffcreate -eq $DiffWrite) { It \u0026#39;Has Files in the DIFF folder for $db\u0026#39; { Get-ChildItem $Diff*.bak | Should Not BeNullOrEmpty } } else { It \u0026#39;Has Files in the DIFF folder for $db\u0026#39; { $DiffCreate | Should BeLessThan $DiffWrite } }\u0026amp;amp;amp;amp;amp;lt;/div\u0026amp;amp;amp;amp;amp;gt;\u0026amp;amp;amp;amp;amp;lt;div\u0026amp;amp;amp;amp;amp;gt;It \u0026#39;Diff File Folder was written to within the last 24 Hours\u0026#39; { $Diffwrite |Should BeGreaterThan (Get-Date).AddHours(-24) } } if($Db -notin (\u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;model\u0026#39;) -and ($Srv.Databases[$db].RecoveryModel -ne \u0026#39;Simple\u0026#39;) -and ( $LSDatabases -notcontains $db)) { $Logcreate = [System.IO.Directory]::GetCreationTime($Log) $LogWrite = [System.IO.Directory]::GetLastWriteTime($Log) if($Logcreate -eq $LogWrite) { It \u0026#39;Has Files in the LOG folder for $db\u0026#39; { Get-ChildItem $Log*.trn | Should Not BeNullOrEmpty } } else { It \u0026#39;Has Files in the LOG folder for $db\u0026#39; { $LogCreate | Should BeLessThan $LogWrite } } It \u0026#39;Log File Folder was written to within the last 30 minutes\u0026#39; { $Logwrite |Should BeGreaterThan (Get-Date).AddMinutes(-30) } }# Simple Recovery } }# Check for backups You could just run the script you have just created from your check-list,¬†hopefully this¬†blog post can help you see that you¬†can do so.\nBut I like the message¬†showing¬†number of tests and successes and failures at the bottom¬†and I want to use parameters in my script. I can do this like this\n1 2 3 4 5 6 7 8 9 10 11 [CmdletBinding()] ## Pester Test to check OLA Param( $Instance, $CheckForBackups, $CheckForDBFolders, $JobSuffix , $Share , [switch]$NoDatabaseRestoreCheck, [switch]$DontCheckJobOutcome ) and then call it using Invoke-Pester with the parameters like this\n1 2 3 4 5 6 7 8 9 10 11 $Script = @{ Path = $Path; Parameters = @{ Instance = Instance; CheckForBackups = $true; CheckForDBFolders = $true; JobSuffix = \u0026#39;BackupShare1\u0026#39;; Share = \u0026#39;\\\\Server1\\BackupShare1\u0026#39;; NoDatabaseRestoreCheck= $true; DontCheckJobOutcome = $true} } Invoke-Pester -Script $Script but that‚Äôs a bit messy, hard to remember and won‚Äôt encourage people newer to Powershell to use it so I wrapped it in a function with some help and examples and put it in GitHub Test-OlaInstance.ps1¬†and Test-Ola. There is one thing to remember. You will need to add the path to Test-Ola.ps1 on Line 90 of Test-OlaInstance so that the script can find it\nOnce you have that you can call it for a single instance or a number of instances like so. Here I check for Folders and Backup files\n1 2 $Servers =¬†\u0026#39;SQL2008Ser2008\u0026#39;,\u0026#39;SQL2012Ser08AG1\u0026#39;,\u0026#39;SQL2012Ser08AG2\u0026#39;,\u0026#39;SQL2014Ser12R2\u0026#39; Test-OLAInstance -Instance $Servers -Share \u0026#39;H:\\\u0026#39; -CheckForBackups and get¬†a nice result like this. In a little under 20 seconds I completed my checklist for 4 servers including checking if the files and folders exist for 61 databases üôÇ (The three failures were my Integrity Check jobs holding some test corrupt databases)\nThis gives me a nice and simple automated method of checking if Ola‚Äôs maintenance script has been correctly installed. I can use this for one server or many by passing in an array of servers (although they must use the same folder for backing up whether that is UNC or local) I can also add this to an automated build process to ensure that everything has been deployed correctly.\nYou can find the two scripts on GitHub here\nI hope you find it useful\n","date":"2016-09-24T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/09/pester-ola-check.png","permalink":"https://blog.robsewell.com/blog/powershell-pester-and-ola-hallengrens-maintenance-solution/","title":"PowerShell, Pester and Ola Hallengrens Maintenance Solution"},{"content":"My wonderful friend Chrissy LeMaire and I are the creators of two GitHub repositories for SQL Server and PowerShell called dbatools and dbareports\nIf you are working with SQL Server I highly recommend that you take a look at the vast number of commands available to you at dbatools which will help you complete tasks within SQL Server especially for Instance migrations and also a growing number of best practice implementations\nBoth of these modules are not just the work of one person any more. We have over 20 people who have collaborated on the modules THANK YOU ALL and more that have provided guidance and comments via the Slack Channels in the SQL Server Community Slack https://sqlps.io/slack¬†and via the Trello boards https://dbatools.io/trello and https://dbareports/trello\nAt SQL Saturday Cambridge this weekend I was proud to join Chrissy in her presentation as we talked about both modules. Heres a fabulous picture of us with Buck Woody\nWe had discussed previously that it didn‚Äôt feel quite right that these community tools were under our own personal accounts and it also caused some administration issues with allowing access. So with that in mind after a naming discussion in the slack channel we created an organisation to hold them both\nSQL Server Community Collaborative is born at https://github.com/sqlcollaborative\nNothing much changes except the name. we have even found that all the old links work and GitHub desktop updated. We will continue to make great commands with all of our fantastic collaborators. Discussions will happen in Slack and organisation in Trello and we will continue to grow and learn and teach and¬†share and create together.\nWe would love you to come and join us\n","date":"2016-09-14T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/09/wp_20160910_10_14_58_pro.jpg","permalink":"https://blog.robsewell.com/blog/the-sql-server-community-collaborative-github-organisation-is-born/","title":"The SQL Server Community Collaborative GitHub Organisation is born"},{"content":"When you look in msdb for the SQL Agent Job duration you will find that it is an int.\nThis is also the same when you look at Get-SQLAgentJobHistory from the sqlserver module. (You can get this by downloading the latest SSMS release from here)\nThis means that when you look at the various duration of the Agent Jobs you get something like this\nThe first job took 15 hours 41¬†minutes¬†53 seconds, the second 1 minute 25 seconds, the third 21 seconds. This makes it quite tricky to calculate the duration in a suitable datatype. In T-SQL people use scripts like the following from MSSQLTips.com\n1 ((run_duration/10000*3600 + (run_duration/100)%100*60 + run_duration%100 + 31 ) / 60)¬†as \u0026#39;RunDurationMinutes\u0026#39; I needed more information than the number of minutes so I have this which will convert the Run Duration to a timespan\n1 $FormattedDuration = @{Name = \u0026#39;FormattedDuration\u0026#39; ; Expression = {[timespan]$_.RunDuration.ToString().PadLeft(6,\u0026#39;0\u0026#39;).insert(4,\u0026#39;:\u0026#39;).insert(2,\u0026#39;:\u0026#39;)}} So how did I get to there?\nFirst I tried to just convert it. In PowerShell you can define a datatype in square brackets and PowerShell will try to convert it\nIt did its best but it converted it to ticks! So we need to convince PowerShell that this is a proper timespan. First we need to convert the run duration to a standard length, you can use the PadLeft method of a string to do this which will ensure that a string has a length and precede the current string with a value you choose until the string is that length.\nLets have a length of 6 and preceding zeros PadLeft(6,‚Äô0‚Ä≤)\nBut this works only if it is a string!! Remember red text is useful, it will often contain the information you need to resolve your error. Luckily there is a method to turn an int to a string. I am using the foreach method to demonstrate\nNow every string is 6 characters long starting with zeros. So all that is left is to format this with colons to separate the hours and minutes and the minutes and seconds. We can do this with the insert method. You can find out the methods using Get-Member or its¬†alias gm\nSo the insert method takes an int for the startindex and a string value to enter\nThere we go now we have some proper formatted timespans however they are still strings. We can then convert them using [timespan] Now we can format the results within the select by using an expression as shown below\nand as you can see it is a timespan now\nOn a slight side note. I needed the durations for Agent Jobs with a certain name within the last 6 days.\nI did this by passing an array of servers (which I got from my dbareports database) to Get-SQLAgentJobHistory. I then used the Where method to filter for JobName and the Job Outcome step of the history. I compared the RunDate property¬†to Get-Date (today) adding -6 days using the AddDays method üôÇ\nHopefully this will be of use to people and also I have it recorded for the next time I need to do it üôÇ\n","date":"2016-09-12T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/09/timespan.png","permalink":"https://blog.robsewell.com/blog/converting-sql-agent-job-duration-to-timespan-using-powershell/","title":"Converting SQL Agent Job Duration to TimeSpan using PowerShell"},{"content":"I love to speak about PowerShell. I really enjoy giving presentations and when I saw Start-Demo being used at the PowerShell Conference in Hanover I started to make use of it in my presentations.\nStart-Demo was written in 2007 by a fella who knows PowerShell pretty well üôÇ¬†https://blogs.msdn.microsoft.com/powershell/2007/03/03/start-demo-help-doing-demos-using-powershell/\nIt was then updated in 2012 by Max Trinidad http://www.maxtblog.com/2012/02/powershell-start-demo-now-allows-multi-lines-onliners/\nThis enabled support for multi-line code using backticks at the end of each line. This works well but I dislike having to use the backticks in foreach loops, it confuses people who think that they need to be included and to my mind looks a bit messy\nThis didn‚Äôt bother me enough to look at the code but I did mention it to my friend Luke t | g¬†who decided to use it as a challenge for his Friday lunch-time codeathon and updated the function so that it works without needing a backtick\nIt also works with nested loops\njust a little improvement but one I think that works well and looks good\nYou can find it at\nhttps://github.com/SQLDBAWithABeard/Presentations/blob/master/Start-Demo.ps1\nand a little demo showing what it can and cant do\nhttps://github.com/SQLDBAWithABeard/Presentations/blob/master/start-demotest.ps1\nLoad the Start-Demo.ps1 file and then run\nStart-Demo PATHTO\\start-demotest.ps1\nEnjoy!\n","date":"2016-08-29T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/08/start-demo2.png","permalink":"https://blog.robsewell.com/blog/making-start-demo-work-with-multi-line-commands-without-a-backtick/","title":"Making Start-Demo work with multi-line commands without a backtick"},{"content":"What is DBA Tools?\nA collection of modules for SQL Server DBAs. It initially started out as ‚Äòsqlmigration‚Äô, but has now grown into a collection of various commands that help automate DBA tasks and encourage best practices.\nYou can read more about here¬†and it is freely available for download on GitHub¬†I thoroughly recommend that [you watch this quick video]\nto see just how easy it is to migrate an entire SQL instance in one command Longer session here\nInstalling it is as easy as\nInstall-Module dbatools\nwhich will get you over 80 commands . Visit https://dbatools.io/functions/¬†to find out more information about them\nThe journey to Remove-SQLDatabaseSafely started with William Durkin b¬†| t¬†who presented to the SQL South West User Group (You can get his slides here)\nFollowing that session I wrote a Powershell Script to gather information about the last used date for databases which I blogged about here and then a T-SQL script to take a final backup and create a SQL Agent Job to restore from that back up which I blogged about here¬†The team have used this solution (updated to load the DBA Database and a¬†report instead of using Excel) ever since and it proved invaluable when a read-only database was dropped and could quickly and easily be restored with no fuss.\nI was chatting with Chrissy LeMaire who founded¬†DBATools¬†b | t¬†about this process and when she asked for contributions in the SQL Server Community Slack¬†I offered my help and she suggested I write this command. I have learnt so much. I thoroughly enjoyed and highly recommend working on projects collaboratively to improve your skills. It is amazing to work with such incredible professional PowerShell people.\nI went back to the basics and thought about what was required and watched one of my favourite videos again. Grant Fritcheys Backup Rant\nI decided that the process should be as follows\nPerforms a DBCC CHECKDB Database is backed up WITH CHECKSUM Database is restored with VERIFY ONLY on the source An Agent Job is created to easily restore from that backup The database is dropped The Agent Job restores the database performs a DBCC CHECKDB and drops the database for a final time This (hopefully) passes all of Grants checks. This is how I created the command\nI check that the SQL Agent is running otherwise we wont be able to run the job. I use a while loop with a timeout like this\n1 2 3 4 5 6 7 8 9 10 $agentservice = Get-Service -ComputerName $ipaddr -Name $serviceName if ($agentservice.Status -ne \u0026#39;Running\u0026#39;) { $agentservice.Start() $timeout = new-timespan -seconds 60 $sw = [diagnostics.stopwatch]::StartNew() $agentstatus = (Get-Service -ComputerName $ipaddr -Name $serviceName).Status while ($dbStatus -ne \u0026#39;Running\u0026#39; -and $sw.elapsed -lt $timeout) { $dbStatus = (Get-Service -ComputerName $ipaddr -Name $serviceName).Status } } There are a lot more checks and logic than I will describe here to make sure that the process is as robust as possible. For example, the script can exit after errors are found using DBCC CHECKDB or continue and label the database backup file and restore job appropriately. Unless the force option is used it will exit if the job name already exists. We have tried to think of everything but if¬†something has been¬†missed or you have suggestions let us know (details at end of post)\nThe only thing I didn‚Äôt add was a LARGE RED POP UP SAYING ARE YOU SURE YOU WANT TO DROP THIS DATABASE but I considered it!!\nPerforms a DBCC CHECKDB Running DBCC CHECKDB with Powershell is as easy as this\n1 2 3 $sourceserver = New-Object Microsoft.SQLServer.Management.Smo.Server \u0026#34;ServerName\u0026#34; $db = $sourceserver.databases[$dbname] $null = $db.CheckTables(\u0026#39;None\u0026#39;) you can read more on MSDN\nDatabase is backed up WITH CHECKSUM Stuart Moore is my go to for doing backups and restores with SMO\nI ensured that the backup was performed with checksum like this\n1 2 3 4 5 $backup = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Backup $backup.Action = [Microsoft.SqlServer.Management.SMO.BackupActionType]::Database $backup.BackupSetDescription = \u0026#34;Final Full Backup of $dbname Prior to Dropping\u0026#34; $backup.Database = $dbname $backup.Checksum = $True Database is restored with VERIFY ONLY on the source I used SMO all the way through this command and performed the restore verify only like this\n1 2 3 4 $restoreverify = New-Object \u0026#39;Microsoft.SqlServer.Management.Smo.Restore\u0026#39; $restoreverify.Database = $dbname $restoreverify.Devices.AddDevice($filename, $devicetype) $result = $restoreverify.SqlVerify($sourceserver) An Agent Job is created to easily restore from that backup First I created a category for the Agent Job\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Function New-SqlAgentJobCategory { param ([string]$categoryname, [object]$jobServer) if (!$jobServer.JobCategories[$categoryname]) { if ($Pscmdlet.ShouldProcess($sourceserver, \u0026#34;Creating Agent Job Category $categoryname\u0026#34;) { try { Write-Output \u0026#34;Creating Agent Job Category $categoryname\u0026#34; $category = New-Object Microsoft.SqlServer.Management.Smo.Agent.JobCategory $category.Parent = $jobServer $category.Name = $categoryname $category.Create() Write-Output \u0026#34;Created Agent Job Category $categoryname\u0026#34; } catch { Write-Exception $_ throw \u0026#34;FAILED : To Create Agent Job Category $categoryname - Aborting\u0026#34; } } } } } and then generated the TSQL for the restore step by using the script method on the Restore SMO object\nThis is how to create an Agent Job\n1 2 3 4 $job = New-Object Microsoft.SqlServer.Management.Smo.Agent.Job $jobServer, $jobname $job.Name = $jobname $job.OwnerLoginName = $jobowner $job.Description = \u0026#34;This job will restore the $dbname database using the final backup located at $filename\u0026#34; and then to add a job step to run the restore command\n1 2 3 4 5 6 7 8 9 10 11 12 $jobStep = new-object Microsoft.SqlServer.Management.Smo.Agent.JobStep $job, $jobStepName $jobStep.SubSystem = \u0026#39;TransactSql\u0026#39; # \u0026#39;PowerShell\u0026#39; $jobStep.DatabaseName = \u0026#39;master\u0026#39; $jobStep.Command = $jobStepCommmand $jobStep.OnSuccessAction = \u0026#39;QuitWithSuccess\u0026#39; $jobStep.OnFailAction = \u0026#39;QuitWithFailure\u0026#39; if ($Pscmdlet.ShouldProcess($destination, \u0026#34;Creating Agent JobStep on $destination\u0026#34;) { $null = $jobStep.Create() } $job.ApplyToTargetServer($destination) $job.StartStepID = $jobStartStepid $job.Alter() The database is dropped We try 3 different methods to drop the database\n1 2 3 $server.KillDatabase($dbname) $server.databases[$dbname].Drop() $null = $server.ConnectionContext.ExecuteNonQuery(\u0026#34;DROP DATABASE \u0026#34;) The Agent Job restores the database To run the Agent Job I call the start method of the Job SMO Object\n1 2 3 4 5 6 7 8 9 $job = $destserver.JobServer.Jobs[$jobname] $job.Start() $status = $job.CurrentRunStatus while ($status -ne \u0026#39;Idle\u0026#39;) { Write-Output \u0026amp;quot; Restore Job for $dbname on $destination is $status\u0026amp;quot; $job.Refresh() $status = $job.CurrentRunStatus Start-Sleep -Seconds 5 } Then we drop the database for the final time with the confidence that we have a safe backup and an easy one click¬†method to restore it from that backup (as long as the backup is in the same location)\nThere are further details on the functions page on dbatools\nSome videos of it in action are on YouTube http://dbatools.io/video\nYou can take a look at the code on GitHub here\nYou can install it with\nInstall-Module dbatools\nYou can provide feedback via the Trello Board or discuss it in the #dbatools channel in the Sqlserver Community Slack\nYou too can also become a contributor https://dbatools.io/join-us/¬†Come and write a command to make it easy for DBAs to (this bit is up to your imagination).\n","date":"2016-07-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/remove-sqldatabasesafely-my-first-contribution-to-dbatools/","title":"Remove-SQLDatabaseSafely My First Contribution to DBATools"},{"content":"So with the July Release of SSMS everything changed for using PowerShell with SQL. You can read the details here As I mentioned in my previous post the name of the module has changed to sqlserver\nThis means that if you have a PowerShell script doing Import-Module SQLPS_, it will need to be changed to be_ Import-Module SqlServer in order to take advantage of the new provider functionality and new CMDLETs. The new module will be installed to ‚Äú%Program Files\\WindowsPowerShell\\Modules\\SqlServer_‚Äù and hence no update to $env:PSModulePath is required._\nYou can download the latest SSMS release here Once you have installed and rebooted you can start to look at the new Powershell CMDlets\nImport-module sqlserver\nTake a look at cmdlets\nGet-command -module sqlserver\nToday I want to look at agent jobs\nGet-command *sqlagent*\nSo I decided to see how to gather the information I gather for the DBADatabase as described here\nThis is the query I use to insert the data for the server level agent job information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $Query = @\u0026#34; INSERT INTO [Info].[AgentJobServer] ([Date] ,[InstanceID] ,[NumberOfJobs] ,[SuccessfulJobs] ,[FailedJobs] ,[DisabledJobs] ,[UnknownJobs]) VALUES (GetDate() ,(SELECT [InstanceID] FROM [DBADatabase].[dbo].[InstanceList] WHERE [ServerName] = \u0026#39;$ServerName\u0026#39; AND [InstanceName] = \u0026#39;$InstanceName\u0026#39; AND [Port] = \u0026#39;$Port\u0026#39;) ,\u0026#39;$JobCount\u0026#39; ,\u0026#39;$successCount\u0026#39; ,\u0026#39;$failedCount\u0026#39; ,\u0026#39;$JobsDisabled\u0026#39; ,\u0026#39;$UnknownCount\u0026#39;) \u0026#34;@ So Get-SQLAgentJob looks like the one I need. Lets take a look at the help. This should be the starting point whenever you use a new cmdlet\nGet-Help Get-SqlAgentJob -Full\nWhich states\nReturns a SQL Agent Job object for each job that is present in the target instance of SQL Agent.\nThat sounds like it will meet my needs. Lets take a look\nGet-SqlAgentJob -ServerInstance $Connection|ft -AutoSize\nI can get the information I require like this\n1 2 3 4 5 $JobCount = (Get-SqlAgentJob -ServerInstance $Connection ).Count $successCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Succeeded\u0026#39;}.Count $failedCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Failed\u0026#39;}.Count $JobsDisabled = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.IsEnabled -eq $false}.Count $UnknownCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Unknown\u0026#39;}.Count NOTE ‚Äì That code is for PowerShell V4 and V5, if you are using earlier versions of PowerShell you would need to use\n1 2 3 4 5 $JobCount = (Get-SqlAgentJob -ServerInstance $Connection ).Count $successCount = (Get-SqlAgentJob -ServerInstance $Connection|Where-Object {$_.LastRunOutcome -eq \u0026#39;Succeeded\u0026#39;}).Count $failedCount = (Get-SqlAgentJob -ServerInstance $Connection |Where-Object {$_.LastRunOutcome -eq \u0026#39;Failed\u0026#39;}).Count $JobsDisabled = (Get-SqlAgentJob -ServerInstance $Connection |Where-Object{$_.IsEnabled -eq $false}).Count $UnknownCount = (Get-SqlAgentJob -ServerInstance $Connection |Where-Object{$_.LastRunOutcome -eq \u0026#39;Unknown\u0026#39;}).Count But to make the code more performant it is better to do this\n1 2 3 4 5 6 7 [pscustomobject]$Jobs= @{} $Jobs.JobCount = (Get-SqlAgentJob -ServerInstance $Connection ).Count $Jobs.successCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Succeeded\u0026#39;}.Count $Jobs.failedCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Failed\u0026#39;}.Count $Jobs.JobsDisabled = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.IsEnabled -eq $false}.Count $Jobs.UnknownCount = (Get-SqlAgentJob -ServerInstance $Connection ).where{$_.LastRunOutcome -eq \u0026#39;Unknown\u0026#39;}.Count $Jobs Using Measure-Command showed that this completed in\nTotalSeconds : 0.9889336\nRather than\nTotalSeconds : 2.9045701\nNote that\n(Get-SqlAgentJob -ServerInstance $Connection ).where{$_.Enabled -eq $false}.Count\nDoes not work. I had to check the properties using\n1 Get-SqlAgentJob -ServerInstance $Connection |Get-Member -Type Properties Which showed me\nIsEnabled Property bool IsEnabled {get;set;}\nSo I tested this against the various SQL versions I had in my lab using this code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $Table = $null $Table = New-Object System.Data.DataTable \u0026#34;Jobs\u0026#34; $Col1 = New-Object System.Data.DataColumn ServerName,([string]) $Col2 = New-Object System.Data.DataColumn JobCount,([int]) $Col3 = New-Object System.Data.DataColumn SuccessCount,([int]) $Col4 = New-Object System.Data.DataColumn FailedCount,([int]) $Col5 = New-Object System.Data.DataColumn DisabledCount,([int]) $Col6 = New-Object System.Data.DataColumn UnknownCount,([int]) $Table.Columns.Add($Col1) $Table.Columns.Add($Col2) $Table.Columns.Add($Col3) $Table.Columns.Add($Col4) $Table.Columns.Add($Col5) $Table.Columns.Add($Col6) foreach ($ServerName in $DemoServers) { ## $ServerName $InstanceName =¬†$ServerName|Select-Object InstanceName -ExpandProperty InstanceName $Port = $ServerName| Select-Object Port -ExpandProperty Port $ServerName = $ServerName|Select-Object ServerName -ExpandProperty ServerName $Connection = $ServerName + \u0026#39;\\\u0026#39; + $InstanceName + \u0026#39;,\u0026#39; + $Port try { $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection } catch { \u0026#34;Failed to connect to $Connection\u0026#34; } if (!( $srv.version)){ \u0026#34;Failed to Connect to $Connection\u0026#34; continue } [pscustomobject]$Jobs= @{} $JobHistory = Get-SqlAgentJob -ServerInstance $Connection $Row = $Table.NewRow() $Row.ServerName = $ServerName $Row.JobCount = $JobHistory.Count $Row.SuccessCount = $JobHistory.where{$_.LastRunOutcome -eq \u0026#39;Succeeded\u0026#39;}.Count $Row.FailedCount = $JobHistory.where{$_.LastRunOutcome -eq \u0026#39;Failed\u0026#39;}.Count $Row.DisabledCount = $JobHistory.where{$_.IsEnabled -eq $false}.Count $Row.UnknownCount = $JobHistory.where{$_.LastRunOutcome -eq \u0026#39;Unknown\u0026#39;}.Count $Table.Rows.Add($row) } $Table|ft Here are the results\nI also had a look at Get-SQLAgentJobHistory Lets take a look at the help\nGet-help get-SQLAgentJobHistory -showwindow\nDESCRIPTION\nReturns the JobHistory present in the target instance of SQL Agent.\nThis cmdlet supports the following modes of operation to return the JobHistory:\nBy specifying the Path of the SQL Agent instance. By passing the instance of the SQL Agent in the input. By invoking the cmdlet in a valid context. So I ran\nGet-SqlAgentJobHistory -ServerInstance sql2014ser12r2\nAnd got back a whole load of information. Every job history available on the server. Too much to look it immediately to work out what to do\nSo I looked at just one job\nGet-SqlAgentJobHistory -ServerInstance SQL2014Ser12R2 -JobName 'DatabaseBackup - SYSTEM_DATABASES - FULL - Local G Drive'\nAnd got back the last months worth of history for that one job as that is the schedule used to purge the job history for this server So then I added -Since Yesterday to only get the last 24 hours history\nGet-SqlAgentJobHistory -ServerInstance SQL2014Ser12R2 -JobName 'DatabaseBackup - SYSTEM_DATABASES - FULL - Local G Drive' -Since Yesterday\nThe Since Parameter is described as\n-Since A convenient abbreviation to avoid using the -StartRunDate parameter.\nIt can be specified with the -EndRunDate parameter.\nDo not specify a -StartRunDate parameter, if you want to use it.\nAccepted values are:\n‚Äì Midnight (gets all the job history information generated after midnight)\n‚Äì Yesterday (gets all the job history information generated in the last 24 hours)\n‚Äì LastWeek (gets all the job history information generated in the last week)\n‚Äì LastMonth (gets all the job history information generated in the last month)\nWhen I run\nGet-SqlAgentJobHistory -ServerInstance SQL2014Ser12R2 -JobName 'DatabaseBackup - SYSTEM_DATABASES - FULL - Local G Drive' -Since Yesterday |Measure-Object\nI get\nCount : 3\nAnd if I run\nGet-SqlAgentJobHistory -ServerInstance SQL2014Ser12R2 -JobName 'DatabaseBackup - SYSTEM_DATABASES - FULL - Local G Drive' -Since Yesterday |select RunDate,StepID,Server,JobName,StepName,Message|Out-GridView\nI get\nWhich matches the view I see in SSMS Agent Job History\nSo Get-SqlAgentJobHistory will enable you to use PowerShell to gather information about the Job history for each step of the Agent Jobs and also the message which I can see being very useful.\nCome and join us in the SQL Community Slack to discuss these CMDLets and all things SQL Community https://sqlps.io/slack\nCALL TO ACTION Microsoft are engaging with the community to improve the tools we all use in our day to day work. There is are two Trello boards set up for YOU to use to contribute\nhttps://sqlps.io/vote for SQLPS sqlserver PowerShell module\nhttps://sqlps.io/ssms for SSMS\nGo and join them and upvote YOUR preferred choice of the next lot of CMDlets\nWe have also set up a SQL Community Slack for anyone in the community to discuss all things related to SQL including the Trello board items and already it seems a good place for people to get help with 150+ members in a few days. You can get an invite here https://sqlps.io/slack\nCome and join us\n","date":"2016-07-03T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/07/getcomand-sqlagent.png","permalink":"https://blog.robsewell.com/blog/using-the-new-sqlserver-powershell-module-to-get-sql-agent-job-information/","title":"Using the new SQLServer Powershell module to get SQL Agent Job Information"},{"content":"The post on the SQLServer blog at TechNet by the SQL Server Tools Team today made me jump out of my seat.\nThe July update for SSMS includes the first substantial improvement in SQL PowerShell in many years. We owe a lot of thanks for this effort to the great collaboration with our community. We have several new CMDLETs to share with you\nIn one release there are twenty-five new CMDLets for the new sqlserver module\nThis means that if you have a PowerShell script doing Import-Module SQLPS, it will need to be changed to be Import-Module SqlServer in order to take advantage of the new provider functionality and new CMDLETs. The new module will be installed to ‚Äú%Program Files\\WindowsPowerShell\\Modules\\SqlServer‚Äù and hence no update to $env:PSModulePath is required.\nSo SQLPS will still continue to work but will not be updated and will not contain the new CMDlets or the future new CMDlets.\nSo what new things do we have? This month we introduce CMDLETs for the following areas:\nAlways Encrypted SQL Agent SQL Error Logs Chrissy LeMaire has written about the new SQL Agent cmdlets\nAaron Nelson has written about the new Get-SqlErrorLog cmdlet\nLaerte Junior has written about Invoke-SQLCmd\nAll four of us will be presenting a webinar on the new CMDlets via the PowerShell Virtual Chapter¬†Wed, Jul 06 2016 12:00 Eastern Daylight Time If you cant make it¬†a recording¬†will be made available on YouTube on the VC Channel https://sqlps.io/video\nAlways Encrypted CMDlets That leaves the Always Encrypted CMDLets and there are 17 of those!\nThat seems to cover setting up Always Encrypted with PowerShell , removing it and getting information about it. When the new SSMS update is dropped you will be able to start using all of this new functionality.\nJust remember Import-Module sqlserver\nCALL TO ACTION Microsoft are engaging with the community to improve the tools we all use in our day to day work. There is are two Trello boards set up for YOU to use to contribute\nhttps://sqlps.io/vote for SQLPS¬†sqlserver PowerShell module\nhttps://sqlps.io/ssms for SSMS\nGo and join them and upvote YOUR preferred choice of the next lot of CMDlets\nWe have also set up a SQL Community Slack for anyone in the community to discuss all things related to SQL including the Trello board items and already it seems a good place for people to get help with 150+ members in a few days. You can get an invite here https://sqlps.io/slack\nCome and join us\n","date":"2016-06-30T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershell-cmdlets-added-for-sql2016-always-encrypted/","title":"PowerShell CMDLets added for SQL2016 Always Encrypted"},{"content":"When I was at PowerShell Conference EU in Hannover last month (The videos are available now ‚Äì click here and the slides and code here) I found out about Irwin Strachans Active Directory Operations Test which got me thinking.\nI decided to do the same for my usual SQL Set-up. Treating all of your servers to the same defaults makes it even easier to manage at scale remotely.\nI am comfortable with using SMO to gather and change properties on SQL Instances so I started by doing this\n1 2 3 4 5 6 7 It \u0026#39;Should have a default Backup Directory of F:\\SQLBACKUP\\BACKUPS\u0026#39; { $Scriptblock = { [void][reflection.assembly]::LoadWithPartialName(\u0026#39;Microsoft.SqlServer.Smo\u0026#39;); $srv = New-Object Microsoft.SqlServer.Management.Smo.Server . return $srv.BackupDirectory} $State = Invoke-Command -ComputerName ROB-SURFACEBOOK -ScriptBlock $Scriptblock $State |Should Be \u0026#39;F:\\SQLBACKUP\\BACKUPS\u0026#39; This is the how to find the properties that you want\n1 2 3 4 5 6 7 8 9 10 11 12 ## Load the Assemblies [void][reflection.assembly]::LoadWithPartialName(\u0026#39;Microsoft.SqlServer.Smo\u0026#39;); ## Create a Server SMO object $srv = New-Object Microsoft.SqlServer.Management.Smo.Server SERVERNAME ## Explore it $srv|gm ## If you find an array pick the first one and expand and then explore that $srv.Databases[0] | select * $srv.Databases[0] | gm I quickly found as I added more tests that it was taking a long time to perform the tests (about 5 seconds each test) and that it took an age to fail each of the tests if the server name was incorrect or the server unavailable.\nI fixed the first one by testing with a ping before running the tests\n1 2 3 4 5 6 ## Check for connectivity if((Test-Connection $Server -count 1 -Quiet) -eq $false){ Write-Error \u0026#39;Could not connect to $Server\u0026#39; $_ continue } The continue is there because I wanted to loop through an array of servers\nI improved the performance using a remote session and a custom object\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Describe \u0026#34;$Server\u0026#34; { BeforeAll { $Scriptblock = { [pscustomobject]$Return = @{} $srv = \u0026#39;\u0026#39; $SQLAdmins = $Using:SQLAdmins [void][reflection.assembly]::LoadWithPartialName(\u0026#39;Microsoft.SqlServer.Smo\u0026#39;); $srv = New-Object Microsoft.SQLServer.Management.SMO.Server $Server $Return.DBAAdminDb = $Srv.Databases.Name.Contains(\u0026#39;DBA-Admin\u0026#39;) $Logins = $srv.Logins.Where{$_.IsSystemObject -eq $false}.Name $Return.SQLAdmins = @(Compare-Object $Logins $SQLAdmins -SyncWindow 0).Length - $Logins.count -eq $SQLAdmins.Count $SysAdmins = $Srv.Roles[\u0026#39;sysadmin\u0026#39;].EnumMemberNames() $Return.SQLAdmin = @(Compare-Object $SysAdmins $SQLAdmins -SyncWindow 0).Length - $SysAdmins.count -eq $SQLAdmins.Count $Return.BackupDirectory = $srv.BackupDirectory $Return.DataDirectory = $srv.DefaultFile The BeforeAll script block is run, as it sounds like it should, once before all of the tests, BeforeEach would run once before each of the tests. I define an empty custom object and then create an SMO object and add the properties I am interested in testing to it. I then return the custom object at the end\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $Return.Alerts82345Exist = ($srv.JobServer.Alerts |Where {$_.Messageid -eq 823 -or $_.Messageid -eq 824 -or $_.Messageid -eq 825}).Count $Return.Alerts82345Enabled = ($srv.JobServer.Alerts |Where {$_.Messageid -eq 823 -or $_.Messageid -eq 824 -or $_.Messageid -eq 825 -and $_.IsEnabled -eq $true}).Count $Return.SysDatabasesFullBackupToday = $srv.Databases.Where{$_.IsSystemObject -eq $true -and $_.Name -ne \u0026#39;tempdb\u0026#39; -and $_.LastBackupDate -lt (Get-Date).AddDays(-1)}.Count Return $Return } try { $Return = Invoke-Command -ScriptBlock $Scriptblock -ComputerName $Server -ErrorAction Stop } catch { Write-Error \u0026#34;Unable to Connect to $Server\u0026#34; $Error continue I was then able to test against the property of the custom object It \u0026#39;Should have Alerts for Severity 20 and above\u0026#39; { $Return.Alerts20SeverityPlusExist | Should Be 6 } It \u0026#39;Severity 20 and above Alerts should be enabled\u0026#39; { $Return.Alerts20SeverityPlusEnabled | Should Be 6 } It \u0026#39;Should have alerts for 823,824 and 825\u0026#39; { $Return.Alerts82345Exist |Should Be 3 } It \u0026#39;Alerts for 823,824 and 825 should be enebled\u0026#39; { $Return.Alerts82345Enabled |Should Be 3 } Occasionally, for reasons I haven‚Äôt explored I had to test against the value property of the returned object It \u0026#34;The Full User Database Backup should be scheduled Weekly $OlaUserFullSchedule\u0026#34; { $Return.OlaUserFullSchedule.value | Should Be $OlaUserFullSchedule } I wanted to be able to run the tests against environments or groups of servers with different default values so I parameterised the Test Results as well and then the logical step was to turn it into a function and then I could do some parameter splatting. This also gives me the opportunity to show all of the things that I am currently giving parameters to the test for\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $Parms = @{ Servers = \u0026#39;SQLServer1\u0026#39;,\u0026#39;SQLServer2\u0026#39;,\u0026#39;SQLServer3\u0026#39;; SQLAdmins = \u0026#39;THEBEARD\\Rob\u0026#39;,\u0026#39;THEBEARD\\SQLDBAsAlsoWithBeards\u0026#39;; BackupDirectory = \u0026#39;C:\\MSSQL\\Backup\u0026#39;; DataDirectory = \u0026#39;C:\\MSSQL\\Data\\\u0026#39;; LogDirectory = \u0026#39;C:\\MSSQL\\Logs\\\u0026#39;; MaxMemMb = \u0026#39;4096\u0026#39;; Collation = \u0026#39;Latin1_General_CI_AS\u0026#39;; TempFiles = 4 ; OlaSysFullFrequency = \u0026#39;Daily\u0026#39;; OlaSysFullStartTime = \u0026#39;21:00:00\u0026#39;; OlaUserFullSchedule = \u0026#39;Weekly\u0026#39;; OlaUserFullFrequency = 1 ;## 1 for Sunday OlaUserFullStartTime = \u0026#39;22:00:00\u0026#39;; OlaUserDiffSchedule = \u0026#39;Weekly\u0026#39;; OlaUserDiffFrequency = 126; ## 126 for every day except Sunday OlaUserDiffStartTime = \u0026#39;22:00:00\u0026#39;; OlaUserLogSubDayInterval = 15; OlaUserLoginterval = \u0026#39;Minute\u0026#39;; HasSPBlitz = $true; HasSPBlitzCache = $True; HasSPBlitzIndex = $True; HasSPAskBrent = $true; HASSPBlitzTrace = $true; HasSPWhoisActive = $true; LogWhoIsActiveToTable = $true; LogSPBlitzToTable = $true; LogSPBlitzToTableEnabled = $true; LogSPBlitzToTableScheduled = $true; LogSPBlitzToTableSchedule = \u0026#39;Weekly\u0026#39;; LogSPBlitzToTableFrequency = 2 ; # 2 means Monday LogSPBlitzToTableStartTime = \u0026#39;03:00:00\u0026#39;} Test-SQLDefault @Parms I have some other tests which always return what I want, particularly the firewall rules which you will have to modify to suit your own environment\nTo be able to run this you will need to have the Pester Module. If you are using Windows 10 then it is installed by default, if not\nFind-Module ‚ÄìName 'Pester' | Install-Module\nYou can find more about Pester here and here and also these videos from the conference\nYou can find the tests on GitHub here and I will continue to add to the defaults that I check.\nThis is not a replacement for other SQL configuration tools such as PBM but it is a nice simple way of giving a report on the current status of a SQL installation either at a particular point in time when something is wrong or after an installation prior to passing the server over to another team or into service\n","date":"2016-05-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/some-pester-tests-for-sql-defaults/","title":"Some Pester Tests for SQL Defaults"},{"content":"It started with a tweet from Dusty\nThe second session I presented at the fantastic PowerShell Conference Europe¬†was about using the DBA Database to automatically install DBA scripts like sp_Blitz, sp_AskBrent, sp_Blitzindex from Brent Ozar¬†, Ola Hallengrens Maintenance Solution¬†, Adam Mechanics sp_whoisactive¬†, This fantastic script for logging the results from sp_whoisactive to a table¬†, Extended events sessions and other goodies for the sanity of the DBA.\nBy making use of the dbo.InstanceList in my DBA database I am able to target instances, by SQL Version, OS Version, Environment, Data Centre, System, Client or any other variable I choose. An agent job that runs every night will automatically pick up the instances and the scripts that are marked as needing installing. This is great when people release updates to the above scripts allowing you to target the development environment and test before they get put onto live.\nI talked to a lot of people in Hannover and they all suggested that I placed the scripts onto GitHub and after some how-to instructions from a few people (Thank you Luke) I spent the weekend updating and cleaning up the code and you can now find it on GitHub here\nI have added the DBA Database project, the Powershell scripts and Agent Job creation scripts to call those scripts and everything else I use. Some of the DBA Scripts I use (and links to those you need to go and get yourself for licensing reasons) and the Power Bi files as well. I will be adding some more jobs that I use to gather other information soon.\nPlease go and have a look and see if it is of use to you. It is massively customisable and I have spoken to various people who have extended it in interesting ways so I look forward to hearing about what you do with it.\nAs always, questions and comments welcome\n","date":"2016-05-16T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/05/tweets.png%29%5D%28/assets/uploads/2016/05/tweets.png","permalink":"https://blog.robsewell.com/blog/dba-database-scripts-are-on-github/","title":"DBA Database scripts are on Github"},{"content":"\nSo SQLBits has finished for another year. About 1500 people registered for the largest SQL Conference in Europe held over 4 days in Liverpool last week. 2 days of full day sessions and 2 days of shorter sessions, a pub quiz, a party (literally) out of this world.\nIt is organised and run by volunteers\nYes, you‚Äôre right.\nWow.\nOf course we must pay special thanks to Simon, Darren, Chris, Jonathan, Annette, Allan and Alex without forgetting all that JRJ has done. They ensured that the venue, sessions, speakers, sponsors, audio visual and not forgetting the amazing party by Sneaky Experience (Thank you Julia), all occurred at the right time in the right place with the right people and the work and effort they have put in cannot be over-estimated but this post isn‚Äôt about those generous, inspiring, amazing folk Several people asked me what a volunteer does and how they can become one and hopefully this post will answer both of those questions. More than 40 people volunteered to help at SQL Bits\nSome volunteers arrived on Tuesday to help with the set up. Making sure that we knew where everything was and helping to lift and carry all of the things that needed moving. We wanted to begin the process of packing the swag bags so that all the attendees on Wednesday could have them as they arrived but unfortunately we were let down by a delivery firm and that was not possible. That job was completed by some very dedicated helpers during all of Wednesday and finished off by all helpers after Wednesdays sessions had finished. Many, many thanks and respect particularly to Bob and Conan for their dedication to the bag-packing cause, although I did hear ‚ÄúIf I ever see another bag again ‚Ä¶‚Ä¶..‚Äù\nAll of the registration desks with badges, lanyards and booklets were set up ready for the next day, the covers were put on the tables, the bean bags put out. (They were needed later!!)\nWe started every morning with a helpers briefing at 7am\nwhere we ensured that everyone knew what was happening that day. Most helpers took some part in registration whether it was being at the desks handing out badges, assisting speakers and sponsors on arrival with questions and some the other jobs that needed doing such as putting the table cloths on the tables in the domes and getting out the bags.\nDuring every session there were two room monitors. Their responsibility was to ensure that everything went smoothly during the day and during each of the sessions on Friday and Saturday. They assisted the speaker with drinks, timings and the other foibles that technology can throw, they would have known what to do if there were issues with the domes.\nThey also made sure that everyone was in the correct session and answered lots of questions. The bit that hopefully most attendees missed was that in the case of any issues they called in one of the 5 or 6 super helpers who were patrolling the arena, who could go and get the AV guys or the venue staff to iron out the wrinkles, making sure that the water coolers got re-filled etc They took all of your feedback forms and took them to the main desk. There were always people at the front desk helping, although Alex, Annette and Terri spent almost all of their time there.\nOf course it doesn‚Äôt stop there. During the breaks and particularly at lunchtime the helpers were available and indeed noticeable. They assisted with the queues at lunchtime and for the key-note\nanswered questions and they helped attendees with session choices. There were always people and questions that they were willing to answer or to find someone who could. Once the sessions had finished, the helpers didn‚Äôt. They manned and womanned the registration desks in the evening. Returned all of the feedback forms and helped with sorting out the venue ready for the next day. There was also a de-brief for any lessons learned.\nThey even gave out the free drinks tokens at the entrance to the party!\nSo why do they do it? They are all part of this amazing community dedicated to providing great events for people to learn. Many of them are volunteers at other events and/or user group leaders and attendees and want to give something back to the community.) . They were able to choose the full day and hour sessions that they attended so were still able to learn from the amazing speakers at the brilliant sessions available. Everyone was able to choose which days they helped it doesn‚Äôt have to be for every day you are at the conference. They are a wonderful group of people and I have made some fantastic friendships by volunteering and get great hugs üôÇ\nYou also get asked the most brilliant questions like ‚ÄúExcuse me, I have a Dalek in my van, what would you like me to do with it?‚Äù If that sounds like something you would like to do, you just need to email helpers@sqlbits.com with ‚ÄòI would like to be a helper next year at SQLBits‚Äô in the subject. You can also offer to help at your local user group or at other events, speak to your user group leaders for more details on that If it‚Äôs not for you, that‚Äôs cool but please, for me, the next time you see them at an event just say thank you. It really means a lot and they thoroughly deserve every word of thanks that we give them.\n","date":"2016-05-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/05/dscn0286-3.jpg","permalink":"https://blog.robsewell.com/blog/a-day-in-the-life-of-a-sqlbits-volunteer/","title":"A Day In The Life of a SQLBits Volunteer"},{"content":"When you look after more than a few SQL Servers you will need to perform the same actions against a number of¬†them and that is where PowerShell will be of great benefit. Recently I needed to ensure that all SQL Servers had a certain Extended Event Session set to auto-start and that it was running. I have used the¬†Always On¬†health session in the example below but you could use the same code below and do this for any Extended Event session. Just note that the code below checks for the existence of an Availability Group which may not be what you require.\nAs always when I started to look at Powershell for a solution I turned to MSDN and found this page¬†and also a quick search found Mike Fals blogpost which showed me how to get going.\nI used my DBA Database as described in my previous posts¬†and created a query to check for all of the servers that were active and contactable\n1 2 3 4 5 6 7 8 9 10 11 12 13 SELECT IL.ServerName FROM [dbo].[InstanceList] IL WHERE NotContactable = 0 AND Inactive = 0 and used Invoke-SQLCMD to gather the Server Names $Results = (Invoke-Sqlcmd -ServerInstance $DBADatabaseServer -Database DBADatabase -Query $query -ErrorAction Stop).ServerName Then it was a case of looping through the servers and connecting to the XEvent Store and checking if the required extended event was started and set to auto-start and if not altering those settings\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## Can we connect to the XEStore? if(Test-Path SQLSERVER:\\XEvent\\$Server) { $XEStore = get-childitem -path SQLSERVER:\\XEvent\\$Server -ErrorAction SilentlyContinue¬†| where {$_.DisplayName -ieq \u0026#39;default\u0026#39;} $AutoStart = $XEStore.Sessions[$XEName].AutoStart $Running = $XEStore.Sessions[$XEName].IsRunning Write-Output \u0026#34;$server for $AGNames --- $XEName -- $AutoStart -- $Running\u0026#34; if($AutoStart -eq $false) { $XEStore.Sessions[$XEName].AutoStart = $true $XEStore.Sessions[$XEName].Alter() } if($Running -eq $false) { $XEStore.Sessions[$XEName].Start() } } Very quick and simple and hopefully of use to people, this could easily be turned into a function.¬†The full script is below and also available¬†here on the Powershell gallery or by running¬†Save-Script -Name Set-ExtendedEventsSessionstoAutoStart -Path \u0026lt;path\u0026gt;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 \u0026lt;# .Synopsis Connects to the servers in the DBA Database and for Servers above 2012 sets alwayson_health Extended Events Sessions to Auto-Start and starts it if it is not running .DESCRIPTION Sets Extended Events Sessions to Auto-Start and starts it if it is not running .EXAMPLE Alter the XEvent name and DBADatabase name or add own server list and run .NOTES AUTHOR - Rob Sewell BLOG - DATE - 20/03/2016 #\u0026gt; $DBADatabaseServer $XEName = \u0026#39;AlwaysOn_health\u0026#39; ## Query to gather the servers required $Query = @\u0026#34; SELECT IL.ServerName FROM [dbo].[InstanceList] IL WHERE NotContactable = 0 AND Inactive = 0 \u0026#34;@ Try { $Results = (Invoke-Sqlcmd -ServerInstance $DBADatabaseServer -Database DBADatabase -Query $query -ErrorAction Stop).ServerName } catch { Write-Error \u0026#34;Unable to Connect to the DBADatabase - Please Check\u0026#34; } foreach($Server in $Results) { try { $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Server } catch { Write-Output \u0026#34; Failed to connect to $Server\u0026#34; continue } # To ensure we have a connection to the server if (!( $srv.version)){ Write-Output \u0026#34; Failed to Connect to $Server\u0026#34; continue } if($srv.versionmajor -ge \u0026#39;11\u0026#39;) { ## NOTE this checks if there are Availability Groups - you may need to change this if ($srv.AvailabilityGroups.Name) { $AGNames = $srv.AvailabilityGroups.Name ## Can we connect to the XEStore? if(Test-Path SQLSERVER:\\XEvent\\$Server) { $XEStore = get-childitem -path SQLSERVER:\\XEvent\\$Server -ErrorAction SilentlyContinue | where {$_.DisplayName -ieq \u0026#39;default\u0026#39;} $AutoStart = $XEStore.Sessions[$XEName].AutoStart $Running = $XEStore.Sessions[$XEName].IsRunning Write-Output \u0026#34;$server for $AGNames --- $XEName -- $AutoStart -- $Running\u0026#34; if($AutoStart -eq $false) { $XEStore.Sessions[$XEName].AutoStart = $true $XEStore.Sessions[$XEName].Alter() } if($Running -eq $false) { $XEStore.Sessions[$XEName].Start() } } else { Write-Output \u0026#34;Failed to connect to XEvent on $Server\u0026#34; } } else { ## Write-Output \u0026#34;No AGs on $Server\u0026#34; } } else { ## Write-Output \u0026#34;$server not 2012 or above\u0026#34; } } ","date":"2016-03-28T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-powershell-to-set-extended-events-sessions-to-autostart/","title":"Using PowerShell to set Extended Events Sessions to AutoStart"},{"content":"If you use SQL Backup to URL to backup your databases to Azure blob storage remember that for¬†the container name case is important\nSo\n1 2 3 BACKUP LOG [DatabaseName] TO URL = N\u0026#39;https://storageaccountname.blob.core.windows.net/containername/databasename_log_dmmyyhhss.trn\u0026#39; WITH CHECKSUM, NO_COMPRESSION, CREDENTIAL = N\u0026#39;credential\u0026#39; will work but\n1 2 3 BACKUP LOG [DatabaseName] TO URL = N\u0026#39;https://storageaccountname.blob.core.windows.net/CONTAINERNAME/databasename_log_dmmyyhhss.trn\u0026#39; WITH CHECKSUM, NO_COMPRESSION, CREDENTIAL = N\u0026#39;credential\u0026#39; will give an (400) Bad Request Error which may not be easy to diagnose\nMsg 3271, Level 16, State 1, Line 1 A nonrecoverable I/O error occurred on file \u0026ldquo;https://storageacccountname.blob.core.windows.net/CONTAINERNAME/databasename_log_dmmyyhhss.trn':\" Backup to URL received an exception from the remote endpoint. Exception Message: The remote server returned an error: (400) Bad Request.. Msg 3013, Level 16, State 1, Line 1 BACKUP LOG is terminating abnormally.\nIf you are using Ola Hallengrens jobs to perform your backup then your job step will look like this\n1 sqlcmd -E -S $(ESCAPE_SQUOTE(SRVR)) -d DBA-Admin -Q \u0026#34;EXECUTE [dbo].[DatabaseBackup] @Databases = \u0026#39;USER_DATABASES\u0026#39;,\u0026amp;nbsp; @URL = \u0026#39;https://storageaccountname.blob.core.windows.net/containername\u0026#39;, @Credential = \u0026#39;credential\u0026#39;, @BackupType = \u0026#39;LOG\u0026#39;, @ChangeBackupType = \u0026#39;Y\u0026#39;, @Verify = \u0026#39;Y\u0026#39;, @CheckSum = \u0026#39;Y\u0026#39;, @LogToTable = \u0026#39;Y\u0026#39;\u0026#34; -b Note the @ChangeBackupType = ‚ÄòY‚Äô parameter which is not created by default but I think is very useful. If you have just created a database and take log backups every 15 minutes but differential (or full) every night the log backup will fail until a full backup has been taken. This parameter will check if a log backup is possible and if not take a full backup meaning that you still can keep to your RTO/RPO requirements even for newly created databases\n","date":"2016-03-03T00:00:00Z","permalink":"https://blog.robsewell.com/blog/backing-up-to-url-container-name-case-is-important/","title":"Backing up to URL container name ‚Äì case is important"},{"content":"This error caught me out. I am putting this post here firstly to remind me if I do it again and also to help others who may hit the same issue.\nI also have been looking at Pester which is a framework for running unit tests within PowerShell\nYou will find some good blog posts about starting with Pester here\nSo I created a function script file Create-HyperVFromBase.ps1 and a tests script file Create-HyperVFromBase.tests.ps1 as shown.\nThe tests contained this code\n1 2 3 4 5 6 7 8 9 10 $here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026#34;.Tests.\u0026#34;, \u0026#34;.\u0026#34;) . {\u0026#39;$here\\$sut\u0026#39;} Describe \u0026#34;Create Hyper V from Base Tests\u0026#34; { Context \u0026#34;Parameter Values,Validations and Errors\u0026#34; { It exists { test-path function:\\create-hypervmfrombase | should be $true } } } When I ran the test I got the following error\nor\nGoogling pester ‚ÄúThe script failed due to call depth overflow.‚Äù returned only 7 results but the Reddit link contained the information I needed\n.Replace() is case sensitive. It didn‚Äôt remove the .tests. keyword from your file name. So it calls your test script again and repeats the same mistake over and over.\nand so I renamed the tests script file to Create-HyperVFromBase.Tests.ps1 With a Capital T! and bingo\nDon‚Äôt forget to name your Pester Tests scripts with a capital T when loading the script in this way and remember that Replace() is case sensitive.\n1 2 3 $here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026#34;.Tests.\u0026#34;, \u0026#34;.\u0026#34;) . \u0026#34;$here\\$sut\u0026#34; ","date":"2016-01-31T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2016/01/pester-success_thumb1.jpg","permalink":"https://blog.robsewell.com/blog/powershell-pester-the-script-failed-due-to-call-depth-overflow./","title":"PowerShell Pester ‚Äì The script failed due to call depth overflow."},{"content":"This error caught me out. I am putting this post here firstly to remind me if I do it again adn also to help others who may hit the same issue.\nToday I am rewriting a function to create a Hyper-V VM so that I can properly script the creation of my labs for demos and other things. I am doing this because I want to use DSC to create an availability group and want to be able to tear down and recreate the machines (but thats for another day)\nI also have been looking at Pester which is a framework for running unit tests within PowerShell\nYou will find some good blog posts about starting with Pester here\nHere is the start of the function. I validate the VMName parameter to ensure that there a VM with that¬†name does not already exist\nfunction Create-HyperVMFromBase { [cmdletbinding()] param ( [Parameter(Mandatory = $true,HelpMessage=\u0026ldquo;Enter a VMName for the VM that does not exist\u0026rdquo;)] [ValidateScript({(!(Get-VM -Name $_))})] [string]$VMName,\nand my Pester test looks like this\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026ldquo;.\u0026rdquo;) . {\u0026rsquo;$here$sut'}\nDescribe \u0026ldquo;Create Hyper V from Base Tests\u0026rdquo; { Context \u0026ldquo;Parameter Values,Validations and Errors\u0026rdquo; { It exists { test-path function:\\create-hypervmfrombase | should be $true } It \u0026ldquo;Should error when VMName exists\u0026rdquo; { $VMName = (Get-VM|Select -First 1 Name).Name create-hypervmfrombase -VMName $VMName |should throw }\nI thought that what I was testing was that the function threw an error when an incorrect parameter was passed. The should throw should be true but what I got was\nSo I was getting the correct error but not passing the test. It was a simple fix. Simply adding curly braces around the call to the function\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026ldquo;.\u0026rdquo;) . \u0026ldquo;$here$sut\u0026rdquo; Describe \u0026ldquo;Create Hyper V from Base Tests\u0026rdquo; { Context \u0026ldquo;Parameter Values,Validations and Errors\u0026rdquo; { It exists { test-path function:\\create-hypervmfrombase | should be $true } It \u0026ldquo;Should error when VMName exists\u0026rdquo; { $VMName = (Get-VM|Select -First 1 Name).Name {create-hypervmfrombase -VMName $VMName} |should throw } } }\nand we pass the test.\n","date":"2016-01-31T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershell-pester-testing-for-parameter-validation/","title":"PowerShell Pester Testing for Parameter Validation"},{"content":"Continuing my series on using Power Bi with my DBA Database I am going to show in this post how I create the most useful daily report for DBAs - The SQL Agent Job report. You can get the scripts and reports here\nPlease note this project became dbareports.io\nThis gives a quick overview of the status of the Agent Jobs across the estate and also quickly identifies recent failed jobs enabling the DBA to understand their focus and prioritise their morning efforts.\nI gather the information into 2 tables AgentJobDetail\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 CREATE TABLE [Info].[AgentJobDetail]( [AgetnJobDetailID] [int] IDENTITY(1,1) NOT NULL, [Date] [datetime] NOT NULL, [InstanceID] [int] NOT NULL, [Category] [nvarchar](50) NOT NULL, [JobName] [nvarchar](250) NOT NULL, [Description] [nvarchar](750) NOT NULL, [IsEnabled] [bit] NOT NULL, [Status] [nvarchar](50) NOT NULL, [LastRunTime] [datetime] NOT NULL, [Outcome] [nvarchar](50) NOT NULL, CONSTRAINT [PK_info.AgentJobDetail] PRIMARY KEY CLUSTERED ( [AgetnJobDetailID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO and AgentJobServer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE [Info].[AgentJobServer]( [AgentJobServerID] [int] IDENTITY(1,1) NOT NULL, [Date] [datetime] NOT NULL, [InstanceID] [int] NOT NULL, [NumberOfJobs] [int] NOT NULL, [SuccessfulJobs] [int] NOT NULL, [FailedJobs] [int] NOT NULL, [DisabledJobs] [int] NOT NULL, [UnknownJobs] [int] NOT NULL, CONSTRAINT [PK_Info.AgentJobServer] PRIMARY KEY CLUSTERED ( [AgentJobServerID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO The Detail table holds the results of every Agent Job and the Server table holds a roll up for each server. The script to gather this information is based on the script I used to put the information into an Excel Sheet as described in my post How I Check Hundreds of Agent Jobs in 60 Seconds with PowerShell which I also altered to send an HTML email to the DBA team each morning. This however is a much better solution and allows for better monitoring and trending.\nAs I have explained in my previous posts I use an Instance List table to hold the information about each instance in the estate and a series of PowerShell scripts which run via Agent Jobs to gather the information into various tables. These posts describe the use of the Write-Log function and the methodology of gathering the required information and looping through each instance so I wont repeat that here. There is an extra check I do however for Express Edition as this does not contain the Agent service\n1 2 3 4 5 $edition = $srv.Edition if ($Edition -eq \u0026#39;Express\u0026#39;) { Write-Log -Path $LogFile -Message \u0026#34;No Information gathered as this Connection $Connection is Express\u0026#34; continue } The Agent Job information can be found in SMO by exploring the $srv.JobServer.Jobs object and I gather the information by iterating through each job and setting the values we require to variables\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 try { $JobCount = $srv.JobServer.jobs.Count $successCount = 0 $failedCount = 0 $UnknownCount = 0 $JobsDisabled = 0 #For each job on the server foreach ($jobin$srv.JobServer.Jobs) { $jobName = $job.Name; $jobEnabled = $job.IsEnabled; $jobLastRunOutcome = $job.LastRunOutcome; $Category = $Job.Category; $RunStatus = $Job.CurrentRunStatus; $Time = $job.LastRunDate; if ($Time -eq \u0026#39;01/01/000100:00:00\u0026#39;) {$Time = \u0026#39;\u0026#39;} $Description = $Job.Description; #Counts for jobs Outcome if ($jobEnabled -eq $False) {$JobsDisabled += 1} elseif ($jobLastRunOutcome -eq \u0026#34;Failed\u0026#34;) {$failedCount += 1; } elseif ($jobLastRunOutcome -eq \u0026#34;Succeeded\u0026#34;) {$successCount += 1; } elseif ($jobLastRunOutcome -eq \u0026#34;Unknown\u0026#34;) {$UnknownCount += 1; } } } I found that some Jobs had names and descriptions that had \u0026rsquo; in them which would cause the SQL update or insert statement to fail so I use the replace¬†method to replace the \u0026rsquo; with ''\n1 2 3 4 5 6 7 8 if ($Description -eq $null) { $Description = \u0026#39; \u0026#39; } $Description = $Description.replace(\u0026#39;\u0026#39;\u0026#39;\u0026#39;, \u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;) if ($jobName -eq $Null) { $jobName = \u0026#39;None\u0026#39; } $JobName = $JobName.replace(\u0026#39;\u0026#39;\u0026#39;\u0026#39;, \u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;\u0026#39;) I then insert the data per job after checking that it does not already exist which allows me to re-run the job should a number of servers be uncontactable at the time of the job running without any additional work\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 IF NOT EXISTS ( SELECT [AgetnJobDetailID] FROM [DBADatabase].[Info].[AgentJobDetail] where jobname = \u0026#39;$jobName\u0026#39; and InstanceID = (SELECT [InstanceID] FROM [DBADatabase].[dbo].[InstanceList] WHERE [ServerName] = \u0026#39;$ServerName\u0026#39; AND [InstanceName] = \u0026#39;$InstanceName\u0026#39; AND [Port] = \u0026#39;$Port\u0026#39;) and lastruntime = \u0026#39;$Time\u0026#39; ) INSERT INTO [Info].[AgentJobDetail] ([Date] ,[InstanceID] ,[Category] ,[JobName] ,[Description] ,[IsEnabled] ,[Status] ,[LastRunTime] ,[Outcome]) VALUES (GetDate() ,(SELECT [InstanceID] FROM [DBADatabase].[dbo].[InstanceList] WHERE [ServerName] = \u0026#39;$ServerName\u0026#39; AND [InstanceName] = \u0026#39;$InstanceName\u0026#39; AND [Port] = \u0026#39;$Port\u0026#39;) ,\u0026#39;$Category\u0026#39; ,\u0026#39;$jobName\u0026#39; ,\u0026#39;$Description\u0026#39; ,\u0026#39;$jobEnabled\u0026#39; ,\u0026#39;$RunStatus\u0026#39; ,\u0026#39;$Time\u0026#39; ,\u0026#39;$jobLastRunOutcome\u0026#39;) I put this in a here-string variable and pass it to Invoke-SQLCmd I do the same with the roll up using this query\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 INSERT INTO [Info].[AgentJobServer] ([Date] ,[InstanceID] ,[NumberOfJobs] ,[SuccessfulJobs] ,[FailedJobs] ,[DisabledJobs] ,[UnknownJobs]) VALUES (GetDate() ,(SELECT [InstanceID] FROM [DBADatabase].[dbo].[InstanceList] WHERE [ServerName] = \u0026#39;$ServerName\u0026#39; AND [InstanceName] = \u0026#39;$InstanceName\u0026#39; AND [Port] = \u0026#39;$Port\u0026#39;) ,\u0026#39;$JobCount\u0026#39; ,\u0026#39;$successCount\u0026#39; ,\u0026#39;$failedCount\u0026#39; ,\u0026#39;$JobsDisabled\u0026#39; ,\u0026#39;$UnknownCount\u0026#39;) This job runs as a SQL Agent Job every morning a half an hour or so before¬†the DBA arrives for the morning shift vastly improving the ability of the DBA to prioritise their morning routine.\nTo create the report open Power Bi Desktop and click Get Data\nThen choose SQL Server and click connect\nEnter the Connection string, the database and the¬†query to gather the data\nThe query is\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Select IL.InstanceID, IL.ServerName, IL.InstanceName, IL.Environment, IL.Location, AJD.Category, AJD.Date, AJD.Description, AJD.IsEnabled, AJD.JobName, AJD.LastRunTime, AJD.Outcome, AJD.Status FROM [dbo].[InstanceList] IL JOIN [Info].[AgentJobDetail] AJD ON IL.InstanceID = AJD.InstanceID WHERE LastRunTime \u0026gt; DATEADD(Day,-31,GETDATE()) Once we have gathered the data we then create some extra columns and measures for the reports. First I create a date column from the datetime Date Column\n1 DayDate = DATE(YEAR(\u0026#39;Agent Job Detail\u0026#39;[Date]),MONTH(\u0026#39;Agent Job Detail\u0026#39;[Date]),DAY(\u0026#39;Agent Job Detail\u0026#39;[Date])) I also do the same for the LastRuntime. I create a day of the week column so that I can report on jobs outcome by day\n1 DayyOfWeek = CONCATENATE(WEEKDAY(\u0026#39;Agent Job Detail\u0026#39;[Date],2),FORMAT(\u0026#39;Agent Job Detail\u0026#39;[Date],\u0026#34; -dddd\u0026#34;)) My friend Terry McCann b | t helped me create a column that¬†returns true if the last run time is within 24 hours of the current time to help identify the recent jobs that have failed NOTE - On a Monday morning you will need to change this if you do not check your jobs on the weekend.\n1 Last Run Relative Hour = ((1.0\\*(NOW()-\u0026#39;Agent Job Detail\u0026#39;[LastRunTime]))\\*24)\u0026lt;24 I create a measure for Succeeded, Failed and Unknown\n1 2 3 Succeeded = IF(\u0026#39;Agent Job Detail\u0026#39;[Outcome] = \u0026#34;Succeeded\u0026#34; , 1 , 0) Next we have to create some¬†measures for the sum of failed jobs and the averages¬†This is the code for 7 day sum\n1 2 3 4 Failed7Days = CALCULATE(SUM(\u0026#39;Agent Job Detail\u0026#39;[Failed]),FILTER ( ALL ( \u0026#39;Agent Job Detail\u0026#39;[Last Run Date] ), \u0026#39;Agent Job Detail\u0026#39;[Last Run Date] \u0026gt; ( MAX ( \u0026#39;Agent Job Detail\u0026#39;[Last Run Date] ) - 7 ) \u0026amp;\u0026amp; \u0026#39;Agent Job Detail\u0026#39;[Last Run Date] \u0026lt;= MAX ( \u0026#39;Agent Job Detail\u0026#39;[Last Run Date] ) ) ) and for the 7 Day average\n1 Failed7DayAverage = DIVIDE([Failed7Days],7) I did the same for 30 days. I used the¬†TechNet reference for DAX expressions and got ideas from Chris Webbs blog\nFirst I created the 30 day historical trend chart using a Line and Clustered column chart using the last run date as the axis and the succeed measure as the column and the Failed, Failed 7 Day Average and failed 30 day average as the lines\nI then formatted the lines and title and column\nTo create the gauge which shows how well we have done today I created a measure to quickly identify todays jobs\n1 LastRun Relative Date Offset = INT(\u0026#39;Agent Job Detail\u0026#39;[LastRunTime] - TODAY()) which I use as a filter for the gauge as shown below. I also create two measures zero and twenty for the minimum and maximum for the gauge\nThe rest of the report is measures for 7 day average and 30 day average, a slicer for environment¬†and two tables, one to show the historical job counts and one to show the jobs that have failed in the last 24 hours using the Last Run Relative Hour measure from above\nThere are many other reports that you can or may want to create maybe by day of the week or by category depending on your needs. Once you have the data gathered you are free to play with the data as you see fit. Please add any further examples of reports you can run or would like to run in the comments below.\nOnce you have your report written you can publish it to PowerBi.com and create a dashboard and query it with natural language. I have explained the process in previous posts\nFor example - How many Jobs failed today\nWhich server had most failed jobs\nor using the category field which database maintenance jobs failed today\nI hope these posts have given you ideas about how you can use PowerShell, a DBA Database and Power Bi to help you to manage and report on your environment.\nYou can get the scripts and reports here\nI have written further posts about this\nUsing Power Bi with my DBA Database\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Server Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì SQL Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Databases\nPower Bi, PowerShell and SQL Agent Jobs\n","date":"2015-09-28T00:00:00Z","permalink":"https://blog.robsewell.com/blog/power-bi-powershell-and-sql-agent-jobs/","title":"Power Bi, PowerShell and SQL Agent Jobs"},{"content":"Following my post about using Power Bi with my DBA Database I have been asked if I would share the PowerShell scripts which I use to populate my database.\nIn this post I will show how to create the following report\nAlthough you will find so many items of data that I expect that you will want to create different reports for your own requirements. You will also want to put the report onto PowerBi.com and explore the natural language querying as I show at the end of this post\nYou will find the latest version of my DBADatabase creation scripts and PowerShell scripts here.\nThe SQLInfo table is created using this code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 CREATE TABLE [Info].[Databases]( [DatabaseID] [int] IDENTITY(1,1) NOT NULL, [InstanceID] [int] NOT NULL, [Name] [nvarchar](256) NULL, [DateAdded] [datetime2](7) NULL, [DateChecked] [datetime2](7) NULL, [AutoClose] [bit] NULL, [AutoCreateStatisticsEnabled] [bit] NULL, [AutoShrink] [bit] NULL, [AutoUpdateStatisticsEnabled] [bit] NULL, [AvailabilityDatabaseSynchronizationState] [nvarchar](16) NULL, [AvailabilityGroupName] [nvarchar](128) NULL, [CaseSensitive] [bit] NULL, [Collation] [nvarchar](30) NULL, [CompatibilityLevel] [nvarchar](15) NULL, [CreateDate] [datetime2](7) NULL, [DataSpaceUsageKB] [float] NULL, [EncryptionEnabled] [bit] NULL, [IndexSpaceUsageKB] [float] NULL, [IsAccessible] [bit] NULL, [IsFullTextEnabled] [bit] NULL, [IsMirroringEnabled] [bit] NULL, [IsParameterizationForced] [bit] NULL, [IsReadCommittedSnapshotOn] [bit] NULL, [IsSystemObject] [bit] NULL, [IsUpdateable] [bit] NULL, [LastBackupDate] [datetime2](7) NULL, [LastDifferentialBackupDate] [datetime2](7) NULL, [LastLogBackupDate] [datetime2](7) NULL, [Owner] [nvarchar](30) NULL, [PageVerify] [nvarchar](17) NULL, [ReadOnly] [bit] NULL, [RecoveryModel] [nvarchar](10) NULL, [ReplicationOptions] [nvarchar](40) NULL, [SizeMB] [float] NULL, [SnapshotIsolationState] [nvarchar](10) NULL, [SpaceAvailableKB] [float] NULL, [Status] [nvarchar](35) NULL, [TargetRecoveryTime] [int] NULL, CONSTRAINT [PK_Databases] PRIMARY KEY CLUSTERED ( [DatabaseID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO The PowerShell script uses Jason Wasser @wasserja Write-Log function to write to a text file but I also enable some logging into a new event log by following the steps here http://blogs.technet.com/b/heyscriptingguy/archive/2013/02/01/use-PowerShell-to-create-and-to-use-a-new-event-log.aspx to create a log named SQLAutoScript with a source SQLAUTOSCRIPT\nTo run the script I simply need to add the values for\n1 2 3 $CentralDBAServer = \u0026#39;\u0026#39; ## Add the address of the instance that holds the DBADatabase $CentralDatabaseName = \u0026#39;DBADatabase\u0026#39; $LogFile = \u0026#34;\\DBADatabaseServerUpdate_\u0026#34; + $Date + \u0026#34;.log\u0026#34; ## Set Path to Log File And the script will do the rest. Call the script from a PowerShell Job Step and schedule it to run at the frequency you wish, I gather the information every week. You can get the script from here or you can read on to see how it works and how to create the report and publish it to powerbi.com and query it with natural langauge\nI create a function called Catch-Block to save keystrokes and put my commands inside a try catch to make the scripts as robust as possible.¬†I won\u0026rsquo;t include the try catch in the examples below. I gather all of the server names from the InstanceList table and set the results to an array variable called $ServerNames holding the server name, instance name and port\n1 2 3 4 5 6 7 8 9 10 11 12 $Query = @\u0026#34; SELECT [ServerName] ,[InstanceName] ,[Port] FROM [DBADatabase].[dbo].[InstanceList] Where Inactive = 0 AND NotContactable = 0 \u0026#34;@ try{ $AlltheServers= Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query $query $ServerNames = $AlltheServers| Select ServerName,InstanceName,Port } I then loop through the array and create a $Connection variable for my SMO connection string and connect to the server\n1 2 3 4 5 6 7 8 9 10 11 foreach ($ServerName in $ServerNames) { ## $ServerName $InstanceName = $ServerName|Select InstanceName -ExpandProperty InstanceName $Port = $ServerName| Select Port -ExpandProperty Port $ServerName = $ServerName|Select ServerName -ExpandProperty ServerName $Connection = $ServerName + \u0026#39;\\\u0026#39; + $InstanceName + \u0026#39;,\u0026#39; + $Port try { $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection Even though I place the creation of the SMO server object in a try block you still need to an additional check to ensure that you can connect and populate the object as the code above creates an empty SMO Server object with the name property set to the $Connection variable if you can\u0026rsquo;t connect to that server and doesn‚Äôt error as you may expect The way I have always validated an SMO Server object is to check the version property. There is no justifiable reason for choosing that property, you could choose any one but that‚Äôs the one I have always used. I use an if statement to do this ( This post about Snippets will show you the best way to learn PowerShell code) The reference I use for exiting a loop in the way that you want is this one In this case we use a continue to carry on iterating the loop\n1 2 3 4 if (!( $srv.version)){ Catch-Block \u0026#34; Failed to Connect to $Connection\u0026#34; continue } I then loop through the user databases\n1 2 3 4 foreach($db in $srv.databases|Where-Object {$_.IsSystemObject -eq $false }) { $Name = $db.Name $Parent = $db.Parent.Name To gather information on all databases just remove everything after the pipe symbol or if you wish to exclude certain databases from the collection gathering, maybe the database you keep your Change log table and DBA Team info in you can do that as well here\n1 2 3 4 foreach($db in $srv.databases|Where-Object {$_.Name -ne \u0026#39;EXCLUDENAME\u0026#39; }) { $Name = $db.Name $Parent = $db.Parent.Name If you wish to view all of the different properties that you can gather information on in this way you can use this code to take a look. (This is something you should get used to doing when writing new PowerShell scripts)\n1 2 3 $Connection = \u0026#39;SERVERNAMEHERE\u0026#39; $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection $srv.databases | Get-Member An alternative method of doing this is to set a variable to a $db and then to select all of the properties so that you can see the values and identify the ones you want. Again this a good thing to do when exploring new objects\n1 2 $db = $srv.databases[\u0026#39;DBNAMEHERE\u0026#39;] $db| Select * You can see from the screen shot below that there are 170 properties available to you on a SQL2014 instance. You can gather any or all of that information as long as you ensure that you have the columns with the correct data types in your table and that your script has the logic to deal with properties that do not exist although I have had less issue with this for the database object than the server object\nYou can look for the property that you want by using the Get-Member cmdlet as shown above or use MSDN to find it starting from here or by GoogleBingDuckDuckGo ing \u0026ldquo;PowerShell SMO\u0026rdquo; and the property you wish to find.\nThe rest of the script follows exactly the same pattern as the previous post by checking the SQL Info table for an entry for that instance and updating the table if it exists and inserting if it does not.\nThis is how I created the reports shown above.\nConnect to the DBA Database and run these queries to gather the data for the report.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 SELECT IL.ServerName ,IL.InstanceName ,IL.Location ,IL.Environment ,IL.Inactive ,IL.NotContactable ,D.[DatabaseID] ,D.[InstanceID] ,D.[Name] ,D.[DateAdded] ,D.[DateChecked] ,D.[AutoClose] ,D.[AutoCreateStatisticsEnabled] ,D.[AutoShrink] ,D.[AutoUpdateStatisticsEnabled] ,D.[AvailabilityDatabaseSynchronizationState] ,D.[AvailabilityGroupName] ,D.[CaseSensitive] ,D.[Collation] ,D.[CompatibilityLevel] ,D.[CreateDate] ,D.[DataSpaceUsageKB] ,D.[EncryptionEnabled] ,D.[IndexSpaceUsageKB] ,D.[IsAccessible] ,D.[IsFullTextEnabled] ,D.[IsMirroringEnabled] ,D.[IsParameterizationForced] ,D.[IsReadCommittedSnapshotOn] ,D.[IsUpdateable] ,D.[LastBackupDate] ,D.[LastDifferentialBackupDate] ,D.[LastLogBackupDate] ,D.[Owner] ,D.[PageVerify] ,D.[ReadOnly] ,D.[RecoveryModel] ,D.[ReplicationOptions] ,D.[SizeMB] ,D.[SnapshotIsolationState] ,D.[SpaceAvailableKB] ,D.[Status] ,D.[TargetRecoveryTime] FROM [DBADatabase].[Info].[Databases] as D JOIN [DBADatabase].[dbo].[InstanceList] as IL ON IL.InstanceID =D.InstanceID To get all the database and instance information and\n1 2 3 4 5 6 7 SELECT C.ClientName ,[DatabaseID] ,[InstanceID] ,[Notes] FROM [DBADatabase].[dbo].[ClientDatabaseLookup] as CDL JOIN [DBADatabase].[dbo].[Clients] as C ON CDL.clientid = c.clientid To get the client information. The client information needs to be manually added to the table as this (in general) needs a human bean to understand. When the script runs every night it will pick up new databases and I add a default value of \u0026ldquo;Not Entered\u0026rdquo; to the table which makes it easier to identify the databases that need this additional work. (This also means that as a Team Leader I can monitor that my team are doing this) It can also be added to any scripts which create new databases for deployment.\nThen we need to create some measures and calculated columns for our report. I did this as I realised that I needed it when making the report rather than all up front.\nI created two calculated columns for size for the databases one for Gb and one for Tb by clicking on the data icon on the left and then new measure\n1 2 SizeGb = Query1[SizeMB]/1024 SizeTb = Query1[SizeGb]/1024 Some measures for count of Databases, Instances and Servers\n1 2 3 Databases = COUNT(Query1[DatabaseID]) Instances = DISTINCTCOUNT(Query1[InstanceID]) Servers = DISTINCTCOUNT(Query1[ServerName]) I also wanted to be able to differentiate between \u0026lsquo;External\u0026rsquo; and \u0026lsquo;Internal\u0026rsquo; customers. So I created a calculated column for this value using a switch statement.\n1 External = SWITCH(Clients[ClientName],\u0026#34;Not Entered\u0026#34;, 0 , \u0026#34;Dev Team\u0026#34;,0,\u0026#34;Mi Team\u0026#34;,0,\u0026#34;DBA Team\u0026#34;,0,\u0026#34;Finance Department\u0026#34;,0,\u0026#34;HR\u0026#34;,0,\u0026#34;Operations\u0026#34;,0,\u0026#34;Payroll\u0026#34;,0,\u0026#34;Test Team\u0026#34;,0,\u0026#34;Systems Team\u0026#34;,0,\u0026#34;Unknown\u0026#34;,0,1) I create a donut chart to show the size of the database in Gb by client (and no, my real clients are not rock bands :-) ) as shown below. I formatted the title, legend and background by clicking on the paintbrush in the visualisation pane. I would encourage you to investigate the options here.\nThe other donut chart is number of clients per location (and those are SQL User group locations in the UK and my hometown Bolton)\nThe rest of the visualisations on that report are cards and tables which I am sure that you can work out.\nI created a map to show the location of the databases\nAnd after reading this post http://sqldusty.com/2015/08/03/power-bi-tip-use-the-treemap-chart-as-a-colorful-slicer/ by Dustin Ryan I created a colourful slicer for environment and the client and then added some other information. The important thing here is to pick the information that the person looking at the report needs to see. So if it is recovery model, compatibility level, collation, page verify setting, mirroring, replication, size and number of databases then this report is correct but I doubt that‚Äôs what you want :-)\nYou can slice this report by location, client or environment. For example, I can easily see which clients have data in Exeter and the size and number of databases\nOr if Metallica ring me up I can quickly see that they have 4 databases, just under 69Gb of data in Exeter and it isn\u0026rsquo;t mirrored. You will notice that it is not easy to see the recovery model or the compatibility level. If you hover over the results you get a highlight figure which shows the data is filtered but it is not shown visually very well as there are over a thousand databases using full recovery model.\nIf we are asked about the Integration environment we can see that it is hosted in Bolton, Manchester, Southampton and Exeter and comprises of 394 databases and 739 Gb of data. It is also easier to see the compatibility level and recovery model as the ratios are larger\nOnce we have created the report in the way that we want we can then publish it to powerbi.com and share it with others if we wish. Publishing is as easy as pressing the publish button and entering your powerbi credentials but if you want your data to automatically refresh (and this is the point of the exercise to remove manual work) then you will need to install and configure the PowerBi gateway and schedule a refresh I will post about this later.\nOnce the report is published you can access it in the browser and create a dashboard by clicking the pin in the top right of a visualisation and a pop up will ask you which dashboard you wish to pin it to (Another recent update to Power Bi)\nOnce you have a dashboard you can then perform some natural language question and answer on it. This can be quite interesting and not always quite what you (or your report readers) might expect but it is getting better all the time\nYou have to remember to use the names of the columns correctly\nBut once you have the query correct you can alter it by adding \u0026ldquo;as a VISUALISATION\u0026rdquo; and choose the visualisation\nAnd once you have the visualisation you can pin it to the dashboard\nI think you can see how useful it can be\nThis doesn‚Äôt work quite as you expect\nBut this does\nHow about this (and yes it felt wrong to type!)\nAnd the auditors would love to be able to do this. (This is an old copy of the database in case The Eagles people are reading this - your database is backed up every 15 minutes)\nOr this for a DBA ( Yes, my obfuscation script database naming convention is a bit bland)\nOr the DBA team manager might choose this one\nThe advantage that I cannot show via static pictures is that the data, visualisation and the suggestions alter in real time as you type\nI hope that you have found this useful and that you can see the benefits and advantages of using a DBA Database and empowering people to use self-service to answer their own questions leaving the DBA time to do more important things like drinking coffee :-)\nAs always if you have any questions or comments please feel free to post them on the blog.\nI have written further posts about this\nUsing Power Bi with my DBA Database\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Server Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì SQL Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Databases\nPower Bi, PowerShell and SQL Agent Jobs\n","date":"2015-09-22T00:00:00Z","permalink":"https://blog.robsewell.com/blog/populating-my-dba-database-for-power-bi-with-powershell-databases/","title":"Populating My DBA Database for Power Bi with PowerShell - Databases"},{"content":"\nThis months TSQL2sDay blog post party is hosted by Jen McCown and is about Enterprise Strategy.\nAdam Mechanic started TSQL Tuesdays over 5 years ago and you will find many brilliant posts under that heading if you search for them\nManaging SQL servers at enterprise scale is not a straightforward task. Your aim as a DBA should be to simplify it as much as possible and to automate everything that you possibly can. This post by John Sansom could have been written for this months party and I recommend that you read it.\nSo here are a few points that I think you should consider if you look after SQL in an Enterprise environment.\nEnterprise Strategy will undoubtedly garner a whole host of excellent posts and Jen will provide a round up post which will I am certain will be an excellent resource. Take a look here Know where your instances are and have a single place that you can reference them from. Some people recommend a Central Management Server but I find this too restrictive for my needs. I use an InstanceList table in my DBA Database with the following columns [ServerName], [InstanceName] , [Port] , [AG] , [Inactive] , [Environment] and [Location]. This enables me to target instances not just by name but by environment (Dev, Test, Pre-Prod, Live etc), by location or by joining the InstanceList table with another table I can target by the application or any number of other factors. I also capture information about the servers at windows and SQL level to this database so I can target the SQL 2012 servers specifically if need be or any other metric. This is very powerful and enables far greater flexibility than the CMS in my opinion. Use PowerShell (no surprise I would mention this!) PowerShell is a brilliant tool for automation and I use it all of the time Get used to using this piece of PowerShell code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $Query = @\u0026#34; SELECT [ServerName],[InstanceName],[Port] FROM [DBADatabase].[dbo].[InstanceList] Where Inactive = 0 AND NotContactable = 0 \u0026#34;@ try{ $AlltheServers= Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query $query $ServerNames = $AlltheServers| Select ServerName,InstanceName,Port } foreach ($ServerName in $ServerNames) { ## $ServerName $InstanceName = $ServerName|Select InstanceName -ExpandProperty InstanceName $Port = $ServerName| Select Port -ExpandProperty Port $ServerName = $ServerName|Select ServerName -ExpandProperty ServerName $Connection = $ServerName + \u0026#39;\\\u0026#39; + $InstanceName + \u0026#39;,\u0026#39; + $Port try { $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection Notice the query variable above, this is where the power lies as it enables you to gather all the instances that you need for your task as described in the bullet post above. Once you get used to doing this you can do things like this identify all the instances with Remote DAC disabled using a query against the DBA Database and then enable it on all servers by adding this code to the loop above\n1 2 $srv.RemoteDacEnabled = $true $srv.alter() Very quick very simple and very very powerful. You can also use this to run TSQL scripts against the instances you target but there are some added complications with Invoke-SQLCmd that you need to be aware of\nBE CAREFUL. Test and understand and test before you run any script on a live system especially using a script like this which enables you to target ALL of your servers. You must definitely check that your $ServerNames array contains only the instances you need before you make any changes. You need to be ultra-cautious when it is possible to do great damage Write scripts that are robust and handle errors gracefully. I use Jason Wasser @wasserja Write-Log function to write to a text file and wrap my commands in a try catch block. Include comments in your scripts to assist either the future you or the folks in your position in 5 years time. I would also add one of my bug bears - Use the description block in Agent Jobs. The first place any DBA is going to go to when that job fails is to open the properties of the job. Please fill in that block so that anyone troubleshooting knows some information about what the job does or at the very least a link to some documentation about it Finally in my list, don\u0026rsquo;t overdo the alerts. Alerting is vital for any DBA it is a brilliant way to ensure that you quickly know about any issues affecting your estate but all alerts should be actionable and in some cases you can automate the action that you can take but the message here is don\u0026rsquo;t send messages to the DBA team email for every single tiny thing or they will get swamped and ignore the vital one. This holds for whichever alerting or monitoring system that you use This is but a small sub-section of things that you need to consider when responsible for a large SQL estate but if you need help and advice or just moral support and you don‚Äôt already interact with the SQL community then make today the day you start. Maybe this post by Thomas La Rock is a good place to start or your nearest User Group/Chapter or the #sqlfamily hashtag or give me a shout and I will gladly help.\n","date":"2015-09-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/enterprise-strategies-a-tsql2sday-post/","title":"Enterprise Strategies - A #TSQL2sDay post"},{"content":"Following my post about using Power Bi with my DBA Database I have been asked if I would share the PowerShell scripts which I use to populate my database.\nIn this post I will show how to create the following report\nAlthough you will find so many items of data that I expect that you will want to create different reports for your own requirements. You will also want to put the report onto PowerBi.com and explore the natural language querying as I show at the end of this post\nYou will find the latest version of my DBADatabase creation scripts and PowerShell scripts here.\nThe SQLInfo table is created using this code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 CREATE TABLE [Info].[SQLInfo]( [SQLInfoID] [int] IDENTITY(1,1) NOT NULL, [DateChecked] [datetime] NULL, [DateAdded] [datetime] NULL, [ServerName] [nvarchar](50) NULL, [InstanceName] [nvarchar](50) NULL, [SQLVersionString] [nvarchar](100) NULL, [SQLVersion] [nvarchar](100) NULL, [ServicePack] [nvarchar](3) NULL, [Edition] [nvarchar](50) NULL, [ServerType] [nvarchar](30) NULL, [Collation] [nvarchar](30) NULL, [IsHADREnabled] [bit] NULL, [SQLServiceAccount] [nvarchar](35) NULL, [SQLService] [nvarchar](30) NULL, [SQLServiceStartMode] [nvarchar](30) NULL, [BAckupDirectory] [nvarchar](256) NULL, [BrowserAccount] [nvarchar](50) NULL, [BrowserStartMode] [nvarchar](25) NULL, [IsSQLClustered] [bit] NULL, [ClusterName] [nvarchar](25) NULL, [ClusterQuorumstate] [nvarchar](20) NULL, [ClusterQuorumType] [nvarchar](30) NULL, [C2AuditMode] [nvarchar](30) NULL, [CostThresholdForParallelism] [tinyint] NULL, [MaxDegreeOfParallelism] [tinyint] NULL, [DBMailEnabled] [bit] NULL, [DefaultBackupCComp] [bit] NULL, [FillFactor] [tinyint] NULL, [MaxMem] [int] NULL, [MinMem] [int] NULL, [RemoteDacEnabled] [bit] NULL, [XPCmdShellEnabled] [bit] NULL, [CommonCriteriaComplianceEnabled] [bit] NULL, [DefaultFile] [nvarchar](100) NULL, [DefaultLog] [nvarchar](100) NULL, [HADREndpointPort] [int] NULL, [ErrorLogPath] [nvarchar](100) NULL, [InstallDataDirectory] [nvarchar](100) NULL, [InstallSharedDirectory] [nvarchar](100) NULL, [IsCaseSensitive] [bit] NULL, [IsFullTextInstalled] [bit] NULL, [LinkedServer] [nvarchar](max) NULL, [LoginMode] [nvarchar](20) NULL, [MasterDBLogPath] [nvarchar](100) NULL, [MasterDBPath] [nvarchar](100) NULL, [NamedPipesEnabled] [bit] NULL, [OptimizeAdhocWorkloads] [bit] NULL, [InstanceID] [int] NULL, [AGListener] [nvarchar](150) NULL, [AGs] [nvarchar](150) NULL, CONSTRAINT [PK__SQL__50A5926BC7005F29] PRIMARY KEY CLUSTERED ( [SQLInfoID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [Info].[SQLInfo] WITH CHECK ADD CONSTRAINT [FK_SQLInfo_InstanceList] FOREIGN KEY([InstanceID]) REFERENCES [dbo].[InstanceList] ([InstanceID]) GO ALTER TABLE [Info].[SQLInfo] CHECK CONSTRAINT [FK_SQLInfo_InstanceList] GO The PowerShell script uses Jason Wasser @wasserja Write-Log function to write to a text file but I also enable some logging into a new event log by following the steps here http://blogs.technet.com/b/heyscriptingguy/archive/2013/02/01/use-PowerShell-to-create-and-to-use-a-new-event-log.aspx to create a log named SQLAutoScript with a source SQLAUTOSCRIPT\nTo run the script I simply need to add the values for\n1 2 3 $CentralDBAServer = \u0026#39;\u0026#39; ## Add the address of the instance that holds the DBADatabase $CentralDatabaseName = \u0026#39;DBADatabase\u0026#39; $LogFile = \u0026#34;\\DBADatabaseServerUpdate_\u0026#34; + $Date + \u0026#34;.log\u0026#34; ## Set Path to Log File And the script will do the rest. Call the script from a PowerShell Job Step and schedule it to run at the frequency you wish, I gather the information every week. You can get the script from here or you can read on to see how it works and how to create the report and publish it to powerbi.com\nI create a function called Catch-Block to save keystrokes and put my commands inside a try catch to make the scripts as robust as possible.\n1 2 3 4 5 6 7 8 9 function Catch-Block { param ([string]$Additional) $ErrorMessage = \u0026#34; On $Connection \u0026#34; + $Additional + $_.Exception.Message + $_.Exception.InnerException.InnerException.message $Message = \u0026#34; This message came from the Automated PowerShell script updating the DBA Database with Server Information\u0026#34; $Msg = $Additional + $ErrorMessage + \u0026#34; \u0026#34; + $Message Write-Log -Path $LogFile -Message $ErrorMessage -Level Error Write-EventLog -LogName SQLAutoScript -Source \u0026#34;SQLAUTOSCRIPT\u0026#34; -EventId 1 -EntryType Error -Message $Msg } I give the function an additional parameter which will hold each custom error message which I write to both the event log and a text message to enable easy troubleshooting and include the message from the $Error variable by accessing it with $_. I won\u0026rsquo;t include the try catch in the examples below. I gather all of the server names from the InstanceList table and set the results to an array variable called $ServerNames holding the server name, instance name and port\n1 2 3 4 5 6 7 8 9 10 11 12 $Query = @\u0026#34; SELECT [ServerName] ,[InstanceName] ,[Port] FROM [DBADatabase].[dbo].[InstanceList] Where Inactive = 0 AND NotContactable = 0 \u0026#34;@ try{ $AlltheServers= Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query $query $ServerNames = $AlltheServers| Select ServerName,InstanceName,Port } I then loop through the array and create a $Connection variable for my SMO connection string and connect to the server\n1 2 3 4 5 6 7 8 9 10 11 foreach ($ServerName in $ServerNames) { ## $ServerName $InstanceName = $ServerName|Select InstanceName -ExpandProperty InstanceName $Port = $ServerName| Select Port -ExpandProperty Port $ServerName = $ServerName|Select ServerName -ExpandProperty ServerName $Connection = $ServerName + \u0026#39;\\\u0026#39; + $InstanceName + \u0026#39;,\u0026#39; + $Port try { $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection Even though I place the creation of the SMO server object in a try block you still need to an additional check to ensure that you can connect and populate the object as the code above creates an empty SMO Server object with the name property set to the $Connection variable if you can\u0026rsquo;t connect to that server and doesn‚Äôt error as you may expect The way I have always validated an SMO Server object is to check the version property. There is no justifiable reason for choosing that property, you could choose any one but that‚Äôs the one I have always used. I use an if statement to do this ( This post about Snippets will show you the best way to learn PowerShell code) The reference I use for exiting a loop in the way that you want is this one In this case we use a continue to carry on iterating the loop\n1 2 3 4 if (!( $srv.version)){ Catch-Block \u0026#34; Failed to Connect to $Connection\u0026#34; continue } If you wish to view all of the different properties that you can gather information on in this way you can use this code to take a look. (This is something you should get used to doing when writing new PowerShell scripts)\n1 2 $srv = New-Object (\u0026#39;Microsoft.SqlServer.Management.Smo.Server\u0026#39;) $Connection $srv | Get-Member As you can see from the screenshot below on my SQL2014 server there are 184 properties. I havent chosen to gather all of them, only the ones that are of interest to me, our team or others who request information from our team such as auditors and project managers etc\nYou can choose to use any or all of these properties as long as you ensure you have the columns in your table with the correct data type and that you have the correct knowledge and logic to stop the script from erroring if/when the property is not available. Here is an example\n1 2 3 4 5 6 7 8 9 10 11 12 13 if ($srv.IsHadrEnabled -eq $True) {$IsHADREnabled = $True $AGs = $srv.AvailabilityGroups|Select Name -ExpandProperty Name|Out-String $Expression = @{Name = \u0026#39;ListenerPort\u0026#39; ; Expression = {$_.Name + \u0026#39;,\u0026#39; + $_.PortNumber }} $AGListener = $srv.AvailabilityGroups.AvailabilityGroupListeners|select $Expression|select ListenerPort -ExpandProperty ListenerPort } else { $IsHADREnabled = $false $AGs = \u0026#39;None\u0026#39; $AGListener = \u0026#39;None\u0026#39; } $BackupDirectory = $srv.BackupDirectory I check if the property IsHADREnabled is true and if it is I then gather the information about the Availability Group names and the listener port and if it doesn‚Äôt exist I set the values to None.\nYou will find that not all of the properties that you want are at the root of the Server SMO object. If you want you max and min memory values and you want to know if remote admin connections or xp_cmdshell are enabled you will need to look at the $Srv.Configuration object\n1 2 3 4 $MaxMem = $srv.Configuration.MaxServerMemory.ConfigValue $MinMem = $srv.Configuration.MinServerMemory.ConfigValue $RemoteDacEnabled = $srv.Configuration.RemoteDacConnectionsEnabled.ConfigValue $XPCmdShellEnabled = $srv.Configuration.XPCmdShellEnabled.ConfigValue You can look for the property that you want by using the Get-Member cmdlet as shown above or use MSDN to find it starting from here or by GoogleBingDuckDuckGo ing \u0026ldquo;PowerShell SMO\u0026rdquo; and the property you wish to find.\nThe rest of the script follows exactly the same pattern as the previous post by checking the SQL Info table for an entry for that instance and updating the table if it exists and inserting if it does not.\nThere are other uses for gathering this information than just for reporting on it. You can target different versions of SQL for different scripts. You can identify values that are outside what is expected and change them. If xp_cmdshell should not be enabled, write the TSQL to gather the connection string of all of the servers from the DBADatabase where the SQLInfo table has XPCMDShellenabled = 1 and loop through them exactly as above and change the value of $srv.Configuration.XPCmdShellEnabled.ConfigValue to 0 and then $Srv.Alter()\nIt is a very powerful way of dynamically targeting your estate if you are looking after many instances and with great power comes great responsibility.\nALWAYS TEST THESE AND ANY SCRIPTS YOU FIND OR SCRIPTS YOU WRITE BEFORE YOU RUN THEM IN YOUR PRODUCTION ENVIRONMENT\nYeah, I shouted and some people thought it was rude. But its important, it needs to be repeated and drilled in so that it becomes habitual. You can do great damage to your estate with only a few lines of PowerShell and a DBA Database so please be very careful and ensure that you have a suitable test subset of servers that you can use to test\nThe other thing we can do is report on the data and with Power Bi we can create self service reports and dashboards and also make use of the natural language query at powerbi.com so that when your systems team ask \u0026ldquo;What are all the servers in X data center?\u0026rdquo; you can enable them to answer it themselves or when the compliance officer asks how many SQL 2005 instances do we have and which clients do they serve you can give them a dashboard they can query themselves.\nThis is how I create the two reports you see at the top. I start by connecting to the data source, my DBA Database\nAnd I use this query\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 SELECT IL.ServerName ,IL.InstanceName ,IL.Location ,IL.Environment ,IL.Inactive ,IL.NotContactable ,SI.[SQLInfoID] ,SI.[DateChecked] ,SI.[DateAdded] ,SI.[ServerName] ,SI.[InstanceName] ,SI.[SQLVersionString] ,SI.[SQLVersion] ,SI.[ServicePack] ,SI.[Edition] ,SI.[ServerType] ,SI.[Collation] ,SI.[IsHADREnabled] ,SI.[SQLServiceAccount] ,SI.[SQLService] ,SI.[SQLServiceStartMode] ,SI.[BAckupDirectory] ,SI.[BrowserAccount] ,SI.[BrowserStartMode] ,SI.[IsSQLClustered] ,SI.[ClusterName] ,SI.[ClusterQuorumstate] ,SI.[ClusterQuorumType] ,SI.[C2AuditMode] ,SI.[CostThresholdForParallelism] ,SI.[MaxDegreeOfParallelism] ,SI.[DBMailEnabled] ,SI.[DefaultBackupCComp] ,SI.[FillFactor] ,SI.[MaxMem] ,SI.[MinMem] ,SI.[RemoteDacEnabled] ,SI.[XPCmdShellEnabled] ,SI.[CommonCriteriaComplianceEnabled] ,SI.[DefaultFile] ,SI.[DefaultLog] ,SI.[HADREndpointPort] ,SI.[ErrorLogPath] ,SI.[InstallDataDirectory] ,SI.[InstallSharedDirectory] ,SI.[IsCaseSensitive] ,SI.[IsFullTextInstalled] ,SI.[LinkedServer] ,SI.[LoginMode] ,SI.[MasterDBLogPath] ,SI.[MasterDBPath] ,SI.[NamedPipesEnabled] ,SI.[OptimizeAdhocWorkloads] ,SI.[InstanceID] ,SI.[AGListener] ,SI.[AGs] FROM [DBADatabase].[Info].[SQLInfo] as SI JOIN [DBADatabase].[dbo].[InstanceList] as IL ON IL.InstanceID = SI.InstanceID So that I can easily add any and all the data to the reports if I choose or query using them in powerbi.com\nFirst I created 3 measures.\n1 AG = DISTINCTCOUNT(Query1[AGs]) Instances = DISTINCTCOUNT(Query1[InstanceID]) Servers = DISTINCTCOUNT(Query1[ServerName]) I click on map\nAnd drag the location column to location and the Instances measure to both the Values and Color Saturation\nI then click on edit and format the title and change the colours for the data\nNext I created I heat map for Instances by Edition. The picture shows the details\nAnd a column chart for Instances by Version\nI also add a table showing the number of instances in each location and a slicer for environment.\nEven though you have added one slicer, you are able to slice the data by clicking on the charts. If I click on Developer Edition I can quickly see which versions and locations they are in\nThis works for the map and the column chart as well. This has all been created using live data as a base with all identifying information altered, Bolton is where I was born and the other locations are chosen at random, all other figures and rollups have also been altered.\nTo create the other report I create two donut charts for Instances by version and by location using steps similar to my previous post and then add some tables for location, edition and xp_cmdshell enabled as well as some cards showing total numbers of Servers, Instances and Availability Groups and a slicer for environment to create a report like this, you can use the donut charts to slice the data as well\nBut there are so many different points of information gathered by this script that you get extra value using the natural language query on powerbi.com.\nClick Publish and enter your powerbi.com credentials and then log into powerbi.com in a browser and you will see your report and your dataset. (Note, you can easily filter to find your dashboards, reports and data sets)\nClick the plus sign to create a new dashboard and click the pin on any of the objects in your report to pin them to the dashboard\nThen you can view (and share) your dashboard\nOnce you have done this you can query your data using natural language. It will cope with spelling mistakes and expects the column names so you may want to think about renaming them in your report by right clicking on them after you get your data.\nYou can ask it questions and build up information on the fly and alter it as you need it. As a DBA doing this and imagining enabling others to be able to ask these questions whenever they want from a browser and as many times as they like, it was very cool!\nPretty cool, I think you and any of your \u0026lsquo;requestors\u0026rsquo; would agree\nYou can get all of the scripts here\nI have written further posts about this\nUsing Power Bi with my DBA Database\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Server Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì SQL Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Databases\nPower Bi, PowerShell and SQL Agent Jobs\n","date":"2015-09-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/populating-my-dba-database-for-power-bi-with-powershell-sql-info/","title":"Populating My DBA Database for Power Bi with PowerShell - SQL Info"},{"content":"A quick post today just to add some weight to something that Mike Fal b | t¬†has kicked off. The #SQLHelp hashtag is well known and well used with in the SQL world. It is a fantastic resource and one that I recommend to all SQL folk I meet who are not aware of it. Heres how it started\nMike has suggested that there should be a similar resource for PowerShell questions #PoSHHelp. We want to create a useful and positive place for people to go with their PowerShell queries and some good folks like Mike, Shawn Melton(@wsmelton), Adam Bertram(@adbertram), Derik Hammer(@SQLHammer), Boe Prox(@proxb), myself and others will be looking for your PowerShell problems and try to assist you over Twitter with the same care and grace as Sqlhelp.\nAs with Sqlhelp we would like there to be a few rules that we all can follow to ensure that this remains a brilliant resource. Mike has suggested the following\nQuestions should fit into 140 characters. If they don‚Äôt, put your question and information on another site (like ServerFault.com) and link to it. DO NOT SPAM THE HASH TAG. This is important, because in order to make it useful it needs to be kept clean. Don‚Äôt use it to advertise your blog posts or articles, but only for Q\u0026amp;A. Don‚Äôt be a dick, a.k.a. Wheaton‚Äôs Law. It‚Äôs all too easy to let the anonymity of the internet get the better of us. Be polite and respectful to those using and accidentally mis-using the hash tag. I notice that Aaron Nelson had already suggested this a few years ago but it seems like it has fallen by the wayside. I would like to see this grow for all PowerShell folk so I ask you all to do two things.\nFirstly, please add #PoSHHelp to your Tweetdeck column list or pin it to Tweetium (like I have) If you see a question you can help with then jump in and give your answer and help the community.\nSecondly, let people know, if you see or hear a question about PowerShell then advise them to make use of the hashtag. If you blog about PowerShell then write a quick blog post like this one and let your readers know.\nPass on the word\n","date":"2015-09-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/use-twitter-to-get-%23powershell-help/","title":"Use Twitter to get #PowerShell help"},{"content":"Following my last post about using Power Bi with my DBA Database I have been asked if I would share the PowerShell scripts which I use to populate my database. They are the secondary part to my DBADatabase which I also use to automate the installation and upgrade of all of my DBA scripts as I started to blog about in this post Installing and upgrading default scripts automation - part one - Introduction which is a series I will continue later.\nIn this post I will show how to create the following report\nYou will find the latest version of my DBADatabase creation scripts here.\nI create the following tables\ndbo.ClientDatabaseLookup dbo.Clients dbo.InstanceList dbo.InstanceScriptLookup dbo.ScriptList Info.AgentJobDetail Info.AgentJobServer Info.Databases Info.Scriptinstall Info.ServerOSInfo Info.SQLInfo By adding Server name, Instance Name , Port, Environment, NotContactable, and Location into the InstanceList table I can gather all of the information that I need and also easily add more information to other tables as I need to.\nThe not contactable column is so that I am able to add instances that I am not able to contact due to permission or environment issues. I can still gather information about them manually and add it to the table. I use the same script and change it to generate the SQL query rather than run it, save the query and then run the query manually to insert the data. This is why I have the DateAdded and Date Checked column so that I know how recent the data is. I don‚Äôt go as far as recording the change however as that will be added to a DBA-Admin database on every instance which stores every change to the instance.\nThe ServerOSInfo table is created like so\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \\*\\*\\*\\*\\*\\* Object: Table [Info].[ServerOSInfo] Script Date: 26/08/2015 19:50:38 \\*\\*\\*\\*\\*\\* SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [Info].[ServerOSInfo]( [ServerOSInfoID] [int] IDENTITY(1,1) NOT NULL, [DateAdded] [datetime] NULL, [DateChecked] [datetime] NULL, [ServerName] [nvarchar](50) NULL, [DNSHostName] [nvarchar](50) NULL, [Domain] [nvarchar](30) NULL, [OperatingSystem] [nvarchar](100) NULL, [NoProcessors] [tinyint] NULL, [IPAddress] [nvarchar](15) NULL, [RAM] [int] NULL, CONSTRAINT [PK__ServerOS__50A5926BC7005F29] PRIMARY KEY CLUSTERED ( [ServerOSInfoID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO The PowerShell script uses Jason Wasser @wasserja Write-Log function to write to a text file but I also¬†enable some logging into a new event log by following the steps here http://blogs.technet.com/b/heyscriptingguy/archive/2013/02/01/use-PowerShell-to-create-and-to-use-a-new-event-log.aspx to create a log named SQLAutoScript with a source SQLAUTOSCRIPT\nTo run the script I simply need to add the values for\n1 2 3 $CentralDBAServer = \u0026#39;\u0026#39; ## Add the address of the instance that holds the DBADatabase $CentralDatabaseName= \u0026#39;DBADatabase\u0026#39; $LogFile = \u0026#34;\\DBADatabaseServerUpdate_\u0026#34; + $Date + \u0026#34;.log\u0026#34; ## Set Path to Log File And the script will do the rest. Call the script from a PowerShell Job Step and schedule it to run at the frequency you wish, I gather the information every week. You can get the script from here or you can read on to see how it works and how to create the report\nI create a function called Catch-Block to save keystrokes and put my commands inside a try catch to make the scripts as robust as possible.\n1 2 3 4 5 6 7 8 9 function Catch-Block{ param ([string]$Additional) $ErrorMessage = \u0026#34; On $Connection \u0026#34; + $Additional + $_.Exception.Message + $_.Exception.InnerException.InnerException.message $Message = \u0026#34; This message came from the Automated PowerShell script updating the DBA Database with Server Information\u0026#34; $Msg = $Additional + $ErrorMessage + \u0026#34; \u0026#34; + $Message Write-Log -Path $LogFile -Message $ErrorMessage -Level Error Write-EventLog -LogName SQLAutoScript -Source \u0026#34;SQLAUTOSCRIPT\u0026#34; -EventId 1 -EntryType Error -Message $Msg } I give the function an additional parameter which will hold each custom error message which I write to both the event log and a text message to enable easy troubleshooting and include the message from the $Error variable by accessing it with $_. I won\u0026rsquo;t include the try catch in the examples below. I gather all of the server names from the InstanceList table and set the results to an array variable called $Servers\n1 2 $AlltheServers = Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query \u0026#34;SELECT DISTINCT [ServerName] FROM [DBADatabase].[dbo].[InstanceList] WHERE Inactive = 0 OR NotContactable = 1\u0026#34; $Servers = $AlltheServers| Select ServerName -ExpandProperty ServerName I then loop through the array and gather the information with three WMI queries.\n1 2 3 4 5 6 7 8 9 10 11 Write-Log -Path $LogFile -Message \u0026#34;Gathering Info for $Server \u0026#34; foreach($Server in $Servers) { Write-Log -Path $LogFile -Message \u0026#34;Gathering Info for $Servers\u0026#34; $DNSHostName = \u0026#39;NOT GATHERED\u0026#39; $Domain = \u0026#39;NOT GATHERED\u0026#39; $OperatingSystem = \u0026#39;NOT GATHERED\u0026#39; $IP = \u0026#39;NOT GATHERED\u0026#39; try{ $Info = get-wmiobject win32_computersystem -ComputerName $Server -ErrorAction Stop|select DNSHostName,Domain, @{Name=\u0026#34;RAM\u0026#34;;Expression={\u0026#34;{0:n0}\u0026#34; -f($_.TotalPhysicalMemory/1gb)}},NumberOfLogicalProcessors I give the variables some default values in case they are not picked up and set the error action for the command to Stop to exit the try and the first query gathers the DNSHostName, Domain Name, the amount of RAM in GB and the number of logical processors, the second gathers the Operating System version but the third was the most interesting to do. There are many methods of gathering the IP Address using PowerShell and I tried a few of them before finding one that would work with all of the server versions that I had in my estate but the one that worked remotely the best for me and this is a good point to say that this works in my lab and in my shop but may not necessarily work in yours, so understand, check and test this and any other script that you find on the internet before you let them anywhere near your production environment.\nUnfortunately the one that worked everywhere remotely errored with the local server so I added a check to see if the server name in the variable matches the global environment variable of Computer Name\n1 2 3 4 5 $OS = gwmi Win32_OperatingSystem -ComputerName $Server| select @{name=\u0026#39;Name\u0026#39;;Expression={($_.caption)}} if($Server -eq $env:COMPUTERNAME) {$IP = (Get-WmiObject -ComputerName $Server -class win32_NetworkAdapterConfiguration -Filter \u0026#39;ipenabled = \u0026#34;true\u0026#34;\u0026#39; -ErrorAction Stop).ipaddress[0] } else {$IP = [System.Net.Dns]::GetHostAddresses($Server).IPAddressToString } Write-Log -Path $LogFile -Message \u0026#34;WMI Info gathered for $Server \u0026#34; Once I have all of the information I check if the server already exists in the ServerOs table and choose to either insert or update.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $Exists = Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query \u0026#34;SELECT [ServerName] FROM [DBADatabase].[Info].[ServerOSInfo] WHERE ServerName = \u0026#39;$Server\u0026#39;\u0026#34; if ($Exists) { $Query = @\u0026#34; UPDATE [Info].[ServerOSInfo] SET [DateChecked] = GetDate() ,[ServerName] = \u0026#39;$Server\u0026#39; ,[DNSHostName] = \u0026#39;$DNSHostName\u0026#39; ,[Domain] = \u0026#39;$Domain\u0026#39; ,[OperatingSystem] = \u0026#39;$OperatingSystem\u0026#39; ,[NoProcessors] = \u0026#39;$NOProcessors\u0026#39; ,[IPAddress] = \u0026#39;$IP\u0026#39; ,[RAM] = \u0026#39;$RAM\u0026#39; WHERE ServerName = \u0026#39;$Server\u0026#39; \u0026#34;@ } else { $Query = @\u0026#34; INSERT INTO [Info].[ServerOSInfo] ([DateChecked] ,[DateAdded ,[ServerName] ,[DNSHostName] ,[Domain] ,[OperatingSystem] ,[NoProcessors] ,[IPAddress] ,[RAM]) VALUES ( GetDate() ,GetDate() ,\u0026#39;$Server\u0026#39; ,\u0026#39;$DNSHostName\u0026#39; ,\u0026#39;$Domain\u0026#39; ,\u0026#39;$OperatingSystem\u0026#39; ,\u0026#39;$NoProcessors\u0026#39; ,\u0026#39;$IP\u0026#39; ,\u0026#39;$RAM\u0026#39;) \u0026#34;@ } Invoke-Sqlcmd -ServerInstance $CentralDBAServer -Database $CentralDatabaseName -Query $Query ``` And that‚Äôs it. Now if you wish to gather different data about your servers then you can examine the data available to you by get-wmiobject Win32_OperatingSystem -ComputerName $Server | Get-Member get-wmiobject win32_computersystem -ComputerName $Server | Get-Member\n1 2 3 4 5 6 7 If you find something that you want to gather you can then add the property to the script and gather that information as well, make sure that you add the column to the table and to both the insert and update statements in the PowerShell Script **Creating the report in Power Bi** All data shown in the examples below has been generated from real-life data but all identifiable data has been altered or removed. I was born in Bolton and [SQL SouthWest](http://sqlsouthwest.co.uk/) is based in Exeter :-) Open Power Bi Desktop and click get data. Add the connection details for your DBA Database server and database and add the query SELECT SOI.[ServerOSInfoID]\r,SOI.[DateChecked]\r,SOI.[ServerName]\r,SOI.[DNSHostName]\r,SOI.[Domain]\r,SOI.[OperatingSystem]\r,SOI.[NoProcessors]\r,SOI.[IPAddress]\r,SOI.[RAM]\r,IL.ServerName\r,IL.InstanceName\r,IL.Location\r,IL.Environment\r,IL.Inactive\r,IL.NotContactable\rFROM [DBADatabase].[Info].[ServerOSInfo] as SOI\rJOIN [dbo].[InstanceList] as IL\rON IL.ServerName = SOI.[ServerName]\r```\rCreate a new column for the Operating Edition by clicking data on the left and using this code as described in my previous post\n1 2 3 4 5 6 7 8 9 10 11 Operating System Edition = SWITCH([OperatingSystem], \u0026#34;Microsoft Windows Server 2012 Datacenter\u0026#34;, \u0026#34;DataCenter\u0026#34;, \u0026#34;Microsoft Windows Server 2012 Standard\u0026#34;,\u0026#34;Standard\u0026#34;, \u0026#34;Microsoft Windows Server 2012 R2 Datacenter\u0026#34;, \u0026#34;DataCenter\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Standard\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Enterprise\u0026#34;, \u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Standard\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Enterprise\u0026#34;,\u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Standard Edition\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Enterprise Edition\u0026#34;, \u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft Windows 2000 Server\u0026#34;, \u0026#34;Server 2000\u0026#34;, \u0026#34;Unknown\u0026#34;) And one for OS Version using this code\n1 2 3 4 5 6 7 8 9 10 11 12 OS Version = SWITCH([OperatingSystem], \u0026#34;Microsoft Windows Server 2012 Datacenter\u0026#34;, \u0026#34;Server 2012\u0026#34;, \u0026#34;Microsoft Windows Server 2012 Standard\u0026#34;,\u0026#34;Server 2012\u0026#34;, \u0026#34;Microsoft Windows Server 2012 R2 Datacenter\u0026#34;, \u0026#34;Server 2012 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Standard\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Enterprise\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Standard\u0026#34;, \u0026#34;Server 2008\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Enterprise\u0026#34;,\u0026#34;Server 2008\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Standard Edition\u0026#34;, \u0026#34;Server 2003\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Enterprise Edition\u0026#34;, \u0026#34;Server 2003\u0026#34;, \u0026#34;Microsoft Windows 2000 Server\u0026#34;, \u0026#34;Server 2000\u0026#34;, \u0026#34;Unknown\u0026#34;) I also created a new measure to count the distinct number of servers and instances as follows\n1 2 3 Servers = DISTINCTCOUNT(Query1[Servers Name]) Instances = COUNT(Query1[Instance]) Then in the report area I start by creating a new text box and adding a title to the report and setting the page level filter to InActive is false so that all decommissioned servers are not included\nI then create a donut chart for the number of servers by Operating System by clicking the donut chart in the visualisations and then dragging the OS version to the Details and the Servers Name to the Values\nI then click the format button and added a proper title and the background colour\nThen create the server numbers by location in the same way by clicking donut chart and adding location and count of server names and adding the formatting in the same way as the previous donut\nI created a number of charts to hold single values for Domain, Instance, Server, RAM, Processors and the number of Not Contactable to provide a quick easy view of those figures, especially when you filter the report by clicking on a value within the donut chart. I find that managers really like this feature. They are all created in the same way by clicking the card in the visualisation and choosing the value\nI also add a table for the number of servers by operating system and the number of servers by location by dragging those values to a table visualisation. I find that slicers are very useful ways of enabling information to be displayed as required, use the live visualisation to do this, I add the environment column to slice so that I can easily see values for the live environment or the development environment\nI create a separate page in the report to display all of the server data as this can be useful for other teams such as the systems (server admin) team. I give them a lot of different slicers : - Domain, Location, Environment, OS Version, Edition and NotContactable with a table holding all of the relevant values to enable them to quickly see details\nYou can get all of the scripts here\nI have written further posts about this\nUsing Power Bi with my DBA Database\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Server Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì SQL Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Databases\nPower Bi, PowerShell and SQL Agent Jobs\n","date":"2015-08-31T00:00:00Z","permalink":"https://blog.robsewell.com/blog/populating-my-dba-database-for-power-bi-with-powershell-server-info/","title":"Populating My DBA Database for Power Bi with PowerShell - Server Info"},{"content":"Every good DBA should have a DBA database. A place to store information about all of their instances and databases.\nI have an InstanceList table which looks like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CREATE TABLE [dbo].[InstanceList]( [InstanceID] [int] IDENTITY(1,1) NOT NULL, [ServerName] [nvarchar](50) NOT NULL, [InstanceName] [nvarchar](50) NOT NULL, [Port] [int] NOT NULL, [AG] [bit] NULL, [Inactive] [bit] NULL CONSTRAINT [DF_InstanceList_Inactive] DEFAULT ((0)), [Environment] [nvarchar](25) NULL, [Location] [nvarchar](30) NULL, CONSTRAINT [PK_InstanceList_ID] PRIMARY KEY CLUSTERED ( [InstanceID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] I use this as the basis for all of my information gathering. By adding Server name, Instance Name , Port, Environment and Location to the table I use overnight Agent jobs to run PowerShell scripts to gather information about all of the instances. This way the information is dynamic and gathered from the server, so when we add RAM and change Max memory this is updated the next time the script runs. You can also automate your installation and decommission procedures (using PowerShell scripts) to add the information to the DBA database automatically\nI have 4 scripts\nServerInfo which gathers Windows OS information such as Version and edition of the operating system, number of processors,amount of RAM, IP address, domain name etc SQLInfo which gathers information about the instance such as SQL version, edition, collation, max and min memory, MAXDOP , service accounts and start modes, default file locations etc Database information such as size, data usage, index usage, last backup dates, owner and many more Agent Job which gathers the outcome of the jobs that have run, their names, category into two tables one for a server rollup and one for details about each job Recently I have received a lot of requests for information from various sources, auditors asking about encryption and backup policies, Project managers asking about database and sql versions, compliance asking about numbers of Windows 2003 servers or SQL 2005 servers, system teams asking which serves in a particular location can be turned off at which time dependant on which system they are supporting for a power down\nBefore we had the DBA database holding all of the information about the instances we would have struggled to be able to compile this information and when I saw Power Bi was released to GA I thought that it would be a good place to start to learn about it. By using data that I understood and answering questions that I knew the format of the answer I could be more confident about experimenting -¬†ie. if I know I have 100 servers then any result for servers that exceeds that is incorrect\nI have never been a BI guy, I claim no expertise in the correct methods of manipulating the data. There may very well be better methods of achieving these results and if there please feel free to comment below so that I can improve my knowledge and keep on learning\nAll data shown in the examples below has been generated from real-life data but all identifiable data has been altered or removed. I have no servers in Bolton, it is where I am from originally!!\nI downloaded Power BI Desktop from powerbi.com and ran the installer and the first screen you see is this one\nI then clicked on Get Data\nAnd then SQL Server and filled in the details for my DBA Database and clicked connect\nI used my current Windows credentials\nIt then asked me which tables I wanted to load so I said all of them :-)\nOnce I had loaded the data I looked at the queries and renamed some of the columns to make more sense to me. I also created some calculated columns by clicking New Column\nI created a relative date column using this code from Chris Webb http://blog.crossjoin.co.uk/2013/01/24/building-relative-date-reports-in-powerpivot/\n1 2 3 4 5 Relative Date Offset=INT([Date] ‚Äì TODAY() Relative Date=IF([Relative Date Offset]=0 , \u0026#34;Today\u0026#34; , \u0026#34;Today \u0026#34; \u0026amp; IF([Relative Date Offset]\u0026gt;0, \u0026#34;+\u0026#34;, \u0026#34;\u0026#34;) \u0026amp; [Relative Date Offset]) This will enable me to show data for the last day\nI also did the same for days of the week\n1 DayOfWeek = CONCATENATE(WEEKDAY(\u0026#39;Info AgentJobDetail\u0026#39;[LastRunTime],2),FORMAT(\u0026#39;InfoAgentJobDetail\u0026#39;[LastRunTime],\u0026#34; -dddd\u0026#34;)) Because I struggled to show the information about the Operating system I also created two columns for OS name and OS edition by adding columns as shown below\n1 2 3 4 5 6 7 8 9 10 11 12 Operating System Version = SWITCH(\u0026#39;Info ServerOSInfo\u0026#39;[OperatingSystem], \u0026#34;Microsoft Windows Server 2012 Datacenter\u0026#34;, \u0026#34;Server 2012\u0026#34;, \u0026#34;Microsoft Windows Server 2012 Standard\u0026#34;,\u0026#34;Server 2012\u0026#34;, \u0026#34;Microsoft Windows Server 2012 R2 Datacenter\u0026#34;, \u0026#34;Server 2012 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Standard\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Enterprise\u0026#34;, \u0026#34;Server 2008 R2\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Standard\u0026#34;, \u0026#34;Server 2008\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Enterprise\u0026#34;,\u0026#34;Server 2008\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Standard Edition\u0026#34;, \u0026#34;Server 2003\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Enterprise Edition\u0026#34;, \u0026#34;Server 2003\u0026#34;, \u0026#34;Microsoft Windows 2000 Server\u0026#34;, \u0026#34;Server 2000\u0026#34;, \u0026#34;Unknown\u0026#34;) And\n1 2 3 4 5 6 7 8 9 10 11 Operating System Edition = SWITCH(\u0026#39;Info ServerOSInfo\u0026#39;[OperatingSystem], \u0026#34;Microsoft Windows Server 2012 Datacenter\u0026#34;, \u0026#34;DataCenter\u0026#34;, \u0026#34;Microsoft Windows Server 2012 Standard\u0026#34;,\u0026#34;Standard\u0026#34;, \u0026#34;Microsoft Windows Server 2012 R2 Datacenter\u0026#34;, \u0026#34;DataCenter\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Standard\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft Windows Server 2008 R2 Enterprise\u0026#34;, \u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Standard\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft¬Æ Windows Server¬Æ 2008 Enterprise\u0026#34;,\u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Standard Edition\u0026#34;, \u0026#34;Standard\u0026#34;, \u0026#34;Microsoft(R) Windows(R) Server 2003, Enterprise Edition\u0026#34;, \u0026#34;Enterprise\u0026#34;, \u0026#34;Microsoft Windows 2000 Server\u0026#34;, \u0026#34;Server 2000\u0026#34;, \u0026#34;Unknown\u0026#34;) Then I started to play with the data.\nThis is probably not how a professional would phrase it but I would say that if you don\u0026rsquo;t know how to use a new application be brave and give it a try.\nOBVIOUSLY you are a PROFESSIONAL DBA and will not do anything that would endanger production, use a backup of your database and work locally if you need to.\nThe first thing I wanted to know was how many servers I had by operating system, how many by SQL version and the location of them so that I could answer the questions I had been asked. I had already written a query to get the correct information to give to the requestors so I knew the correct answers which was also an advantage. I did this like this\nI expanded the Info ServerOSInfo query and dragged the ServerName field to the report which created a table of names\nI then changed the ServerName values to Count\nI then dragged the calculated column Operating System Version to the table\nIf I click on the table and then donut chart in the visualisations it changes to\nSo you can quickly see how you want the data displayed\nI then decided to look at the number of SQL 2005 instances that I had and as I had relationships between SQLInfo and Instancelist and Clients I could build a more dynamic report.\nI created a donut chart with SQLVersion as the legend and InstanceID as the values and a table of SQLVersion, ServerName and Instance Name. I also created a card that was count of InstanceID\nNow it starts getting really useful. If I want to know how many SQL 2005 instances I have I simply click on SQL2005 in the donut chart and the rest of the report changes\nThis is very cool and I hope you can see how useful this could be and how brilliant it would be to enable relevant people within the organisation the ability to look at that report and answer their own questions.\nLets take it to the next step. I have a location column in the InstanceList table which comprises of town names. If I choose a map and drag that column to the Location field and set Values and Color Saturation to the Count of InstanceID\nand create two tables one of client with a count of instanceid and one location with a count of instance id I can do this\nLook at how it dynamically changes as you click on the data labels - This is very cool and makes me smile every time!! I altered the colour saturation colours to make it easier to see. Now if I am asked about SQL 2005 servers I can quickly click on SQL 2005 and\nI can see that there are 32 instances, most are in Southampton, and which clients they support\nIf I click a location rather than SQL version the report alters like so\nSo you can simply pass the report file to your colleagues to enable them to use it or you can publish it to Powerbi.com. I am not going to go into any detail about the costs or licensing etc I will just say it is as easy as clicking publish. If you wish to have the information automatically refreshed there are some more steps that you would need to go through which are detailed here which enable you to connect your on-premise database to Powerbi using the data management gateway, alternatively you can simply refresh the data in the report and then publish it and replace the existing report.\nOnce the report is in powerbi.com you can enable Q and A on the data. This is some kind of supernatural mystical magical query language which enables you to query your data with natural language and will alter the results as you type and even cope with (deliberate for screenshot) spelling mistakes :-)\nI also created a report for my Agent Jobs to enable me to quickly and easily see which Jobs have failed in the last day\nI did this by filtering the report by Relative Date Offset greater than -1 (today) and isenabled = True and Outcome = Failed\nThere are many many more ways I can see this being useful and I hope I have given you some ideas and encouraged you to try for yourself and find out more\nI have written further posts about this\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Server Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì SQL Info\nPopulating My DBA Database for Power Bi with PowerShell ‚Äì Databases\nPower Bi, PowerShell and SQL Agent Jobs\n","date":"2015-08-16T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-power-bi-with-my-dba-database/","title":"Using Power Bi with my DBA Database"},{"content":"This weekend I was creating some Azure VMs to test and was required to use the GUI for some screenshots. I have always used my PowerShell scripts described here to create my test systems and with a new job taking up a lot of my time had missed the announcement about Azure SQL Automated Backup and Azure SQL Automated Patching so was surprised to see this screen\nI read the announcement and also the details on MSDN https://msdn.microsoft.com/en-us/library/azure/dn906091.aspx which show that this requires the SQL Server IaaS Agent. This is a default option on new virtual machines.\nThere are some other considerations too. It is only supported for SQL Server 2014 and Windows Server 2012 and 2012R2 at present and you can set a retention period to a maximum of 30 days but it is automated. You do not have to decide upon the backup strategy Azure will decide the frequency and type of backups dependent upon the workload of the database and some other factors such as\nA full backup is taken ‚óã when an instance is added to use Managed backup ‚óã When transaction log growth is 1Gb or more ‚óã At least once a week ‚óã If the log chain is broken ‚óã When a database is created\nA transaction log backup is taken - If no log backup is found - Transaction log space used is 5Mb or larger - At least once every two hours - Any time the transaction log backup is lagging behind a full database backup. The goal is to keep the log chain ahead of full backup.\nFrom https://msdn.microsoft.com/en-gb/library/dn449496(v=sql.120).aspx\nThere are some restrictions - Only database backups are supported - System databases are not supported so you need to back those up yourself - You can only back up to Azure storage - Maximum backup size is 1Tb as this is the maximum size for a blob in Azure storage - Simple recovery is not supported - Maximum retention is 30 days - if you are required to keep your backups for longer than 30 days for regulatory or other reasons you could simply use Azure Automation to copy the files to another storage account in Azure)\nHow to set it up.\nIf you are using the GUI then you will find SQL Automated Backup in the optional config blade of the set up. You can follow the steps here to set it up. If (like me) you want to use PowerShell then use the following code after you have created your Virtual Machine\n1 2 3 4 5 6 7 $storageaccount = \u0026#34;\u0026lt;storageaccountname\u0026gt;\u0026#34; $storageaccountkey = (Get-AzureStorageKey -StorageAccountName $storageaccount).Primary $storagecontext = New-AzureStorageContext -StorageAccountName $storageaccount -StorageAccountKey $storageaccountkey $encryptionpassword = (Get-Credential -message \u0026#39;Backup Encryption Password\u0026#39; -User \u0026#39;IGNOREUSER\u0026#39;).password $autobackupconfig = New-AzureVMSqlServerAutoBackupConfig -StorageContext $storagecontext -Enable -RetentionPeriod 10 -EnableEncryption -CertificatePassword $encryptionpassword Get-AzureVM -ServiceName \u0026lt;vmservicename\u0026gt; -Name \u0026lt;vmname\u0026gt; | Set-AzureVMSqlServerExtension -AutoBackupSettings $autobackupconfig | Update-AzureVM Once you have run the code, Azure will take care of the rest. Add a couple of databases to your instance and look in the storage account and you will see this\nAnd in the automaticbackup container you will find the Certificates and master key backups\nIt will also create a credential\nYou can use the same credential to back up your system databases. If like me you use Ola Hallengrens excellent Maintenance Solution then simply change your systems backup job as follows\n1 2 3 4 5 6 USE [msdb] GO EXEC msdb.dbo.sp_update_jobstep @job_name = \u0026#39;DatabaseBackup - SYSTEM_DATABASES - FULL\u0026#39;, @step_id=1 , @command=N\u0026#39;sqlcmd -E -S $(ESCAPE_SQUOTE(SRVR)) -d master -Q \u0026#34;EXECUTE [dbo].[DatabaseBackup] @Databases = \u0026#39;\u0026#39;SYSTEM_DATABASES\u0026#39;\u0026#39;, \u0026#34;https://myaccount.blob.core.windows.net/mycontainer\u0026#34; , @Credential = \u0026#39;\u0026#39;AutoBackup_Credential\u0026#39;\u0026#39;, @BackupType = \u0026#39;\u0026#39;FULL\u0026#39;\u0026#39;, @Verify = \u0026#39;\u0026#39;Y\u0026#39;\u0026#39;, @CleanupTime = NULL, @CheckSum = \u0026#39;\u0026#39;Y\u0026#39;\u0026#39;, @LogToTable = \u0026#39;\u0026#39;Y\u0026#39;\u0026#39;\u0026#34; -b\u0026#39; GO If you need to restore your database then you can use the GUI and when you choose restore you will see this screen\nEnter your storage account and the key which you can get from the Azure portal. You will notice that the credential has already been selected, click connect and\nThere are all of your backups ready to restore to any point in time that you choose. By clicking script the T-SQL is generated which looks like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 USE [master] BACKUP LOG [Test] TO URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_LogBackup_2015-07-16_06-21-26.bak\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , NOFORMAT, NOINIT, NAME = N\u0026#39;Test_LogBackup_2015-07-16_06-21-26\u0026#39;, NOSKIP, NOREWIND, NOUNLOAD, NORECOVERY , STATS = 5 RESTORE DATABASE [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150714201240+00.bak\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150714202740+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150714224241+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715005741+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715031242+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715052742+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715074243+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715095743+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150715121243+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NORECOVERY, NOUNLOAD, STATS = 5 RESTORE LOG [Test] FROM URL = N\u0026#39;https://sqlbackupstoragebeard.blob.core.windows.net/asqlvm9-mssqlserver/Test_b8bb98d7a235487d9789b3ee8759cf3e_20150716060004+00.log\u0026#39; WITH CREDENTIAL = N\u0026#39;AutoBackup_Credential\u0026#39; , FILE = 1, NOUNLOAD, STATS = 5 GO There is an important note. Remember this when you have just set it up so that you don‚Äôt think that you have done it wrong (which is what I did!)\nWhen you enable Automated Patching for the first time, Azure configures the SQL Server IaaS Agent in the background. During this time, the portal will not show that Automated Patching is configured. Wait several minutes for the agent to be installed, configured. After that the portal will reflect the new settings.\nFrom \u0026lt;https://msdn.microsoft.com/en-us/library/azure/dn961166.aspx\u0026gt;\nAnd also look out for this\nThe password I had chosen was not complex enough but the PowerShell script had succeeded and not given me the warning\nTo set up SQL Automated Patching you follow a similar steps. The setting is again on the OS Config blade and click enable and then you can choose the frequency and duration of the patching.\nIt is important to remember to choose your maintenance window correctly. If you have set up your SQL VMs correctly you will have them in an availability set and be using either mirroring or Availability Groups and have the VMs set up in the same availability set to ensure availability during the underlying host patching but I had it confirmed by Principal Software Engineering Manager Sethu Srinivasan t via Microsoft PFE Arvind Shyamsundar b | t that the SQL Automated Patching is not HA aware so you will need to ensure that you set the maintenance windows on each VM to ensure that they do not overlap\n","date":"2015-07-24T00:00:00Z","permalink":"https://blog.robsewell.com/blog/setting-up-and-using-azure-vm-sql-automated-backup-and-restore/","title":"Setting Up and Using Azure VM SQL Automated Backup (and Restore)"},{"content":"First I must say thank you to all of the wonderful people who have put time and effort into providing free tools and scripts to enable not only myself but all SQL DBAs to ease their work. For this series I especially thank\nBrent Ozar - w|t Ola Hallengren - w Adam Mechanic - b|t Jared Zagelbaum - b|t The aim of this series is to share the methodology and the scripts that I have used to resolve this issue.\nHow can I automate the deployment and update of backup, integrity ,index maintenance and troubleshooting scripts as well as other default required scripts to all of the instances under my control and easily target any instances by OS version, SQL version, Environment, System or any other configuration of my choosing\nThis is Part 1 - Introduction I will link to the further posts here as I write them\nSo the scenario that lead to this series is a varied estate of SQL servers and instances where I wanted an automated method of deploying the scripts and being able to target different servers. It needed to be easy to maintain, easy to use and easy to alter. I wanted to be able to update all of the scripts easily when required. I also wanted to automate the install of new instances and ensure that I could ensure that all required scripts were installed and documented.\nThe method of doing this that I chose is just that - Its the way that I chose, whether it will work for you and your estate I don\u0026rsquo;t know but I hope you will find it of benefit. Of course you must test it first. Ensure that you understand what is happening, what it is doing and that that is what you want it to do. If you implement this methodology of installing scripts you will easily be able to start by targeting your Development Server and then gradually rolling it out to any other environments\u0026rsquo; whilst always making sure that you test, monitor and validate prior to moving to the next.\nI decided that I needed to have a DBA Database to start with. The role of the DBA Database is to be the single source of truth for the instances that are under my control, a source for the location of the scripts that I need to deploy and a place to hold the information that I gather from the servers. It is from this database that I will be able to target the instances as required and set the flags to update the scripts as and when I need to\nOn that instance I also chose to put the SQL Agent Job that will deploy all of the scripts. This is an important point. The account that you use to run that job whether it is the Agent Service Account or a proxy account will need to have privileges on every instance that you target. It will need to be able to run every script that you wish to target your servers. The privileges it requires are defined by the scripts that you want to run. How that is set up is unique to your environment and your system. I will only say that all errors are logged to the log files and will enable you to resolve these issues. You should always use the principle of least privilege required to get the job done. Domain and Sys Admin are not really the best answer here :-)\nI also created 2 further Agent Jobs to gather Windows and SQL Information from the servers. These jobs target all the instances and servers in the DBA Database and gather information centrally about Windows and SQL configurations making it easy to provide that information to any other teams that require it. I am always looking for methods to reduce the workload on DBAs and enabling people (using the correct permissions) to gather the information that they require by self-service is very beneficial\nDocumentation and logging about the scripts are provided by the log files stored as text files to troubleshoot the script and also documented in the Change log table in a DBA database on each instance which I blogged about previously here\nThe last thing was the script which needed to be modular and easy to add to and amend.\nThroughout this series of blog posts I will share and expand on the methods I used to do this. If you have any questions at any point please feel free to ask either by commenting on the post or by contacting me using the methods on my About Me page\n","date":"2015-06-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/installing-and-upgrading-default-scripts-automation-part-one-introduction/","title":"Installing and upgrading default scripts automation - part one - Introduction"},{"content":"If you are a SQL Server DBA you should know about Ola Hallengren and will probably have investigated his Maintenance Solution.\nIf you haven\u0026rsquo;t please start here https://ola.hallengren.com/\nYou can also watch his presentation at SQLBits at this link\nhttp://sqlbits.com/Sessions/Event9/Inside_Ola_Hallengrens_Maintenance_Solution\nwhere he talks about and demonstrates the solution.\nIt is possible to just run his script to install the solution and schedule the jobs and know that you have made a good start in keeping your databases safe. You should be more proactive than that and set specific jobs for your own special requirements but you can and should find that information in other places including the FAQ on Ola\u0026rsquo;s site\nI particularly like the parameter @ChangeBackupType which when running the transaction log¬†or differential backup will change the backup type to full if the backup type cannot be taken. This is excellent for picking up new databases and backing them up soon after creation\nWhen you run the script the jobs are created but not scheduled and it is for this reason I created this function. All it does it schedule the jobs so that I know that they will be run when a new server is created and all the databases will be backed up. I can then go back at a later date and schedule them correctly for the servers workload or tweak them according to specific needs but this allows me that fuzzy feeling of knowing that the backups and other maintenance will be performed.\nTo accomplish this I pass a single parameter $Server to the function this is the connection string and should be in the format of SERVERNAME, SERVERNAME\\INSTANCENAME or SERVERNAME\\INSTANCENAME,Port\nI then create a $srv SMO object as usual\n$srv = New-Object Microsoft.SQLServer.Management.SMO.Server $Server\nCreate a JobServer object and a Jobs array which holds the Jobs\n1 2 $JobServer = $srv.JobServer $Jobs = $JobServer.Jobs And set the schedule for each job. I pick each Job using the Where-Object Cmdlet and break out if the job does not exist\n1 2 3 4 $Job = $Jobs|Where-Object {$_.Name -eq \u0026#39;DatabaseBackup - SYSTEM_DATABASES - FULL\u0026#39;} if ($Job -eq $Null) {Write-Output \u0026#34;No Job with that name\u0026#34; break} Then I create a Schedule object and set its properties and create the schedule\n1 2 3 4 5 6 7 8 9 10 11 $Schedule = new-object Microsoft.SqlServer.Management.Smo.Agent.JobSchedule ($job, \u0026#39;Daily - Midnight ++ Not Sunday\u0026#39;) $Schedule.ActiveEndDate = Get-Date -Month 12 -Day 31 -Year 9999 $Schedule.ActiveEndTimeOfDay = \u0026#39;23:59:59\u0026#39; $Schedule.FrequencyTypes = \u0026#34;Weekly\u0026#34; $Schedule.FrequencyRecurrenceFactor = 1 $Schedule.FrequencySubDayTypes = \u0026#34;Once\u0026#34; $Schedule.FrequencyInterval = 126 # Weekdays 62 + Saturdays 64 - \u0026lt;a href=\u0026#34;https://msdn.microsoft.com/en-us/library/microsoft.sqlserver.management.smo.agent.jobschedule.frequencyinterval.aspx\u0026#34;\u0026gt;https://msdn.microsoft.com/en-us/library/microsoft.sqlserver.management.smo.agent.jobschedule.frequencyinterval.aspx\u0026lt;/a\u0026gt; $Schedule.ActiveStartDate = get-date $schedule.ActiveStartTimeOfDay = \u0026#39;00:16:00\u0026#39; $Schedule.IsEnabled = $true $Schedule.Create() I have picked this example for the blog as it shows some of the less obvious gotchas. Setting the active end date could only be achieved by using the Get-Date Cmdlet and defining the date. The schedule frequency interval above is for every day except Sundays. This achieved by using the following table from MSDN which is always my first port of call when writing these scripts\nWeekDays.Sunday = 1 WeekDays.Monday = 2 WeekDays.Tuesday = 4 WeekDays.Wednesday = 8 WeekDays.Thursday = 16 WeekDays.Friday = 32 WeekDays.Saturday = 64 WeekDays.WeekDays = 62 WeekDays.WeekEnds = 65 WeekDays.EveryDay = 127 Combine values using an OR logical operator to set more than a single day. For example, combine WeekDays.Monday and WeekDays.Friday (FrequencyInterval = 2 + 32 = 34) to schedule an activity for Monday and Friday.\nIt is easy using this to set up whichever schedule you wish by combining the numbers. I would advise commenting it in the script so that your future self or following DBAs can understand what is happening.\nYou can tweak this script or use the code to work with any Agent Jobs and set the schedules accordingly and you can check that you have set the schedules correctly with this code\n1 2 3 4 5 6 7 8 $srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server $JObserver = $srv.JobServer $JObs = $JObserver.Jobs $ActiveStartTimeOfDay = @{Name = \u0026#34;ActiveStartTimeOfDay\u0026#34;; Expression = {$_.JobSchedules.ActiveStartTimeOfDay}} $FrequencyInterval = @{Name = \u0026#34;FrequencyInterval\u0026#34;; Expression = {$_.JobSchedules.FrequencyInterval}} $FrequencyTypes = @{Name = \u0026#34;FrequencyTypes\u0026#34;; Expression = {$_.JobSchedules.FrequencyTypes}} $IsEnabled = @{Name = \u0026#34;IsEnabled\u0026#34;; Expression = {$_.JobSchedules.IsEnabled}} $Jobs|Where-Object{$_.Category -eq \u0026#39;Database Maintenance\u0026#39;}|select name,$IsEnabled,$FrequencyTypes,$FrequencyInterval,$ActiveStartTimeOfDay|Format-Table -AutoSize You can get the script from Script Center via the link below or by searching for \u0026ldquo;Ola\u0026rdquo; using the script browser add-in straight from ISE\nhttps://gallery.technet.microsoft.com/scriptcenter/Schedule-Ola-Hallengrens-a66a3c89\n","date":"2015-05-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/scheduling-ola-hallengrens-maintenance-solution-default-jobs-with-powershell/","title":"Scheduling Ola Hallengrens Maintenance Solution Default Jobs with PowerShell"},{"content":"Last weekend, we held our SQL Saturday event in Exeter. It was a brilliant event for many reasons but we were delighted to have a world exclusive keynote video by Phil Factor about Spinach and Database Development. With many thanks to those that made it possible and particularly to Phil Factor I have linked to the video here and also transcribed it. Please watch and read and understand the message\nWhat has spinach got to do with Database Development?\nGenerations of children were fed spinach in preference to more nutritious things, such as cardboard, because of the persistence of bad data.\nIt wasn\u0026rsquo;t in fact the decimal point error of legend but confusion over the way that iron was measured in the late 19th century data. As a result nutritionists persisted in believing for generations that it was a rich source of iron that the body needs in order to create bloodcells. In fact, the very little iron that there is in spinach isn\u0026rsquo;t in a form that can be readily absorbed by the body anyway.\nThe consequences of bad data can be dire\nGuarding the quality of your data is about the most important thing that you as a data professional can do. You may think that performance is important but it would just deliver you the wrong answer faster. Resilience? it would just make it more likely that you\u0026rsquo;d be able to deliver the wrong answer. Delivery? Yep you got it, the wrong answer quicker.\nThe spinach example is a good one because bad data is hard to detect and can go unnoticed for generations. This is probably because people don\u0026rsquo;t inspect and challenge data as much as they should. You would have thought it strange that a vegetable like spinach should have fifty times as much iron as any other vegetable but the fact came from a very reputable source so people just shrugged and accepted it\nWe have a touching faith in data,\nWe, as a culture, assume its correct and complete, we like to believe that it\u0026rsquo;s impossible that either prejudice, bias, criminality or foolishness could affect the result, worse we think that valuable truth can be sifted from any data no matter the source. If there\u0026rsquo;s enough of it then there must be value in it. It\u0026rsquo;s like panning for gold dust from a river. The sad truth is that this is a delusion but very common in our society. We are, in our mass culture, in the bronze age rather than the information age struggling with silvery toys imbued with mystical magical powers\nA good database professional must be unequivocal.\nBad data cannot be cleaned in the same way that one can clean mud of a diamond. If data contains bad data then the entire data set must be rejected\nThere\u0026rsquo;s no such thing as data cleansing.\nYou as a DBA may be asked to take out data that seems absurd such as ages that are negative or ages that are so great that the person couldn\u0026rsquo;t possibly be alive but then that leaves you in the same dataset, data that is plausible but wrong.\nOnly in very exceptional circumstances when you know precisely why a minority of your data is wrong would you be justified in correcting it.\nStatistics can help us to make very confident assertions about large datasets if they conform to one of the common distributions but they cannot tell us anything about individual items of data. You can of course remove outliers but in fact outliers are just items of data that don\u0026rsquo;t conform to your assumptions about the data and the whole point of data analysis is to test your assumptions. By cleaning data, by removing outliers you can prove almost anything scientifically\nA well designed database is defended in depth at every possible opportunity.\nDepth is actually an interesting analogy because experience tells us that bad data seems to leak in under pressure, through every crack when the database is working hard. Like you will see in a World War 2 submarine movie, in a well-used OLTP database, we are like the crew, swivelling our eyes in terror savouring the futility of any remediation as ghastly drips run down the walls of our database and wishing we had put in more constraints.\nIn terms of the defence of data, check constraints and foreign key constraints are excellent of course and triggers are good but there are other ways of getting warnings of errors in data such as sudden changes in the distribution of data and other anomalies. One check I like to do is the tourism check where you check your data all the way through back to source, this technique once famously picked up the fact that a famous motor manufacturer was reporting its deceleration figures in yards per second when it should have been metres per second.\nWhen you start putting in check constraints you say to yourself, this couldn\u0026rsquo;t possibly happen. This is the voice of superstition. A famous programmer of the 1970\u0026rsquo;s took to putting a message in his code saying \u0026ldquo;this error could never happen\u0026rdquo; and he put it in places where it couldn‚Äôt possibly ever be executed and the funny thing was the more he tested the programme, the more that error appeared on the screen and it is the same with constraints, the more traps you set the more critters you catch and you\u0026rsquo;re left wondering how on earth all that bad data was getting in\nIts misleading to go on about the value of the great flood of big data. There\u0026rsquo;s a strong superstition that data has some sort of intrinsic mystical value all of its own.\nUnless you can prove that data is correct its valueless because if you trust it you can end up with generations of children compelled to eat spinach for no good reason at all.\n","date":"2015-04-30T00:00:00Z","permalink":"https://blog.robsewell.com/blog/spinach-and-database-development-sqlsatexeter-keynote/","title":"Spinach and Database Development- SQLSatExeter Keynote"},{"content":"Just a quick post and a day late for #SQLNewBlogger There are some excellent posts on that hashtag and I recommend that you read them\nWhen you know a server name but not the name of the instances or the ports that they are using this function will be of use\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026lt;# .SYNOPSIS Shows the Instances and the Port Numbers on a SQL Server .DESCRIPTION This function will show the Instances and the Port Numbers on a SQL Server using WMI .PARAMETER Server The Server Name .EXAMPLE Get-SQLInstancesPort Fade2Black This will display the instances and the port numbers on the server Fade2Black .NOTES AUTHOR: Rob Sewell sqldbawithabeard.com DATE: 22/04/2015 #\u0026gt; function Get-SQLInstancesPort { param ([string]$Server) [system.reflection.assembly]::LoadWithPartialName(\u0026#34;Microsoft.SqlServer.Smo\u0026#34;)|Out-Null [system.reflection.assembly]::LoadWithPartialName(\u0026#34;Microsoft.SqlServer.SqlWmiManagement\u0026#34;)|Out-Null $mc = new-object Microsoft.SqlServer.Management.Smo.Wmi.ManagedComputer $Server $Instances = $mc.ServerInstances foreach ($Instance in $Instances) { $port = @{Name = \u0026#34;Port\u0026#34;; Expression = {$_.ServerProtocols[\u0026#39;Tcp\u0026#39;].IPAddresses[\u0026#39;IPAll\u0026#39;].IPAddressProperties[\u0026#39;TcpPort\u0026#39;].Value}} $Parent = @{Name = \u0026#34;Parent\u0026#34;; Expression = {$_.Parent.Name}} $Instance|Select $Parent, Name, $Port } } ","date":"2015-04-22T00:00:00Z","permalink":"https://blog.robsewell.com/blog/instances-and-ports-with-powershell/","title":"Instances and Ports with PowerShell"},{"content":"Please go and check the New SQL Bloggers posting here¬†https://twitter.com/search?q=%23sqlnewblogger¬†There are some brilliant new and older bloggers adding great value to the SQL Community\nThis is my most viewed post so I thought it made a good candidate to be updated and reblogged\n","date":"2015-04-14T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-sql-server-user-role-membership-with-powershell/","title":"Checking SQL Server User Role Membership with PowerShell"},{"content":"Following on from my previous post about parsing XML where I used the information from Steve Jones blog post to get information from the SQL Saturday web site I thought that this information and script may be useful for others performing the same task.\nEdit - This post was written prior to the updates to the SQL Saturday website over the weekend. When it can back up the script worked perfectly but the website is unavailable at the moment again so I will check and update as needed once it is back.\nWe are looking at using the Guidebook app to provide an app for our attendees with all the session details for SQL Saturday Exeter\nThe Guidebook admin website requires the data for the sessions in a certain format. You can choose CSV or XLS.\nIn the admin portal you can download the template\nwhich gives an Excel file like this\nSo now all we need to do is to fill it with data.\nI have an Excel Object Snippet which I use to create new Excel Objects when using PowerShell to manipulate Excel. Here it is for you. Once you have run the code you will be able to press CTRL + J and be able to choose the New Excel Object Snippet any time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $snippet = @{ Title = \u0026#34;New Excel Object\u0026#34;; Description = \u0026#34;Creates a New Excel Object\u0026#34;; Text = @\u0026#34; # Create a .com object for Excel \\`$xl = new-object -comobject excel.application \\`$xl.Visible = \\`$true # Set this to False when you run in production \\`$wb = \\`$xl.Workbooks.Add() # Add a workbook \\`$ws = \\`$wb.Worksheets.Item(1) # Add a worksheet \\`$cells=\\`$ws.Cells \u0026lt;# Do Some Stuff perhaps \\`$cells.item(\\`$row,\\`$col)=\u0026#34;Server\u0026#34; \\`$cells.item(\\`$row,\\`$col).font.size=16 \\`$Cells.item(\\`$row,\\`$col).Columnwidth = 10 \\`$col++ #\u0026gt; \\`$wb.Saveas(\u0026#34;C:\\temp\\Test\\`$filename.xlsx\u0026#34;) \\`$xl.quit() \u0026#34;@ } New-IseSnippet @snippet I needed to change this to open the existing file by using\n$wb = $xl.Workbooks.Open($GuideBookPath)\nIn the more help tab of the Excel workbook it says\nSo we need to do some manipulation of the data we gather. As before I selected the information from the XML as follows\n1 2 3 4 5 $Speaker = @{Name=\u0026#34;Speaker\u0026#34;; Expression = {$_.speakers.speaker.name}} $Room = @{Name=\u0026#34;Room\u0026#34;; Expression = {$_.location.name}} $startTime = @{Name=\u0026#34;StartTime\u0026#34;; Expression = {[datetime]($_.StartTime)}} $Endtime = @{Name =\u0026#34;EndTime\u0026#34;; Expression = {[datetime]($_.EndTime)}} $Talks = $Sessions.event|Where-Object {$_.title -ne \u0026#39;Coffee Break\u0026#39; -and $_.title -ne \u0026#39;Room Change\u0026#39; -and $_.title -ne \u0026#39;Lunch Break\u0026#39; -and $_.title -ne \u0026#39;Raffle and Cream Tea\u0026#39;}| select $Speaker,$Room,$Starttime,$Endtime,Title,Description |Sort-Object StartTime I then looped through the $Talks array and wrote each line to Excel like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 foreach ($Talk in $Talks) { $Date = $Talk.StartTime.ToString(\u0026#39;MM/dd/yyyy\u0026#39;) ## to put the info in the right format $Start = $talk.StartTime.ToString(\u0026#39;hh:mm tt\u0026#39;) ## to put the info in the right format $End = $Talk.Endtime.ToString(\u0026#39;hh:mm tt\u0026#39;) ## to put the info in the right format $Title = $Talk.Title $Description = $Talk.Description $Room = $Talk.Room $col = 2 $cells.item($row,$col) = $Title $col ++ $cells.item($row,$col) = $Date $col ++ $cells.item($row,$col) = $Start $col ++ $cells.item($row,$col) = $End $col ++ $cells.item($row,$col) = $Room $col ++ $col ++ $cells.item($row,$col) = $Description $row++ } I know that I converted the String to DateTime and then back to a String again but that was the easiest (quickest) way to obtain the correct format for the Excel file\nThen to finish save the file and quit Excel\n1 2 $wb.Save() $xl.quit() Then you upload the file in the Guidebook admin area wait for the email confirmation and all your sessions are available in the guidebook\nI hope that is useful to others. The full script is below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ## From http://www.sqlservercentral.com/blogs/steve_jones/2015/01/26/downloading-sql-saturday-data/ $i = 372 $baseURL = ‚Äúhttp://www.sqlsaturday.com/eventxml.aspx?sat=‚Äù $DestinationFile = ‚ÄúE:\\SQLSatData\\SQLSat‚Äù + $i + ‚Äú.xml‚Äù $GuideBookPath = \u0026#39;C:\\temp\\Guidebook_Schedule_Template.xls\u0026#39; $sourceURL = $baseURL + $i $doc = New-Object System.Xml.XmlDocument $doc.Load($sourceURL) $doc.Save($DestinationFile) $Sessions = $doc.GuidebookXML.events $Speaker = @{Name=\u0026#34;Speaker\u0026#34;; Expression = {$_.speakers.speaker.name}} $Room = @{Name=\u0026#34;Room\u0026#34;; Expression = {$_.location.name}} $startTime = @{Name=\u0026#34;StartTime\u0026#34;; Expression = {[datetime]($_.StartTime)}} $Endtime = @{Name =\u0026#34;EndTime\u0026#34;; Expression = {[datetime]($_.EndTime)}} $Talks = $Sessions.event|Where-Object {$_.title -ne \u0026#39;Coffee Break\u0026#39; -and $_.title -ne \u0026#39;Room Change\u0026#39; -and $_.title -ne \u0026#39;Lunch Break\u0026#39; -and $_.title -ne \u0026#39;Raffle and Cream Tea\u0026#39;}| select $Speaker,$Room,$Starttime,$Endtime,Title,Description |Sort-Object StartTime # Create a .com object for Excel $xl = new-object -comobject excel.application $xl.Visible = $true # Set this to False when you run in production $wb = $xl.Workbooks.Open($GuideBookPath) $ws = $wb.Worksheets.item(1) $cells=$ws.Cells $cells.item(2,1) = \u0026#39;\u0026#39; # To clear that entry $cells.item(3,1) = \u0026#39;\u0026#39; # To clear that entry $col = 2 $row = 2 foreach ($Talk in $Talks) { $Date = $Talk.StartTime.ToString(\u0026#39;MM/dd/yyyy\u0026#39;) ## to put the info in the right format $Start = $talk.StartTime.ToString(\u0026#39;hh:mm tt\u0026#39;) ## to put the info in the right format $End = $Talk.Endtime.ToString(\u0026#39;hh:mm tt\u0026#39;) ## to put the info in the right format $Title = $Talk.Title $Description = $Talk.Description $Room = $Talk.Room $col = 2 $cells.item($row,$col) = $Title $col ++ $cells.item($row,$col) = $Date $col ++ $cells.item($row,$col) = $Start $col ++ $cells.item($row,$col) = $End $col ++ $cells.item($row,$col) = $Room $col ++ $col ++ $cells.item($row,$col) = $Description $row++ } $wb.Save() $xl.quit() ","date":"2015-04-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershelling-sql-saturday-sessions-to-the-guidebook-app/","title":"PowerShelling SQL Saturday Sessions to the Guidebook app"},{"content":"As part of my organiser role for SQLSaturday Exeter (Training Day Information here and Saturday Information here) I needed to get some schedule information to input into a database.\nI had read Steve Jones blog posts on Downloading¬†SQL Saturday Data¬†and followed the steps there to download the data from the SQL Saturday website for our event.\nA typical session is held in the XML like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;event\u0026gt; \u0026lt;importID\u0026gt;27608\u0026lt;/importID\u0026gt; \u0026lt;speakers\u0026gt; \u0026lt;speaker\u0026gt; \u0026lt;id\u0026gt;27608\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;William Durkin\u0026lt;/name\u0026gt; \u0026lt;/speaker\u0026gt; \u0026lt;/speakers\u0026gt; \u0026lt;track\u0026gt;Track 2\u0026lt;/track\u0026gt; \u0026lt;location\u0026gt; \u0026lt;name\u0026gt;Buccaneer\u0026#39;s Refuge \u0026lt;/name\u0026gt; \u0026lt;/location\u0026gt; \u0026lt;title\u0026gt;Stories from the Trenches: Upgrading SQL with Minimal Downtime\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;SQL Server has come a long way in the last few years, with Microsoft investing heavily in High Availability features. This session will show you how to use these features to enable you to safely upgrade a SQL Server, while ensuring you have a return path if things should go wrong. You will leave the session knowing what features you can use to upgrade either the OS, Hardware or SQL Server version while keeping your maintenance window to a minimum. The session will apply to Standard Edition as well as Enterprise Edition, so doesn\u0026#39;t only apply to \u0026#39;High Rollers\u0026#39;!\u0026lt;/description\u0026gt; \u0026lt;startTime\u0026gt;4/25/2015 3:20:00 PM\u0026lt;/startTime\u0026gt; \u0026lt;endTime\u0026gt;4/25/2015 4:10:00 PM\u0026lt;/endTime\u0026gt; \u0026lt;/event\u0026gt; I needed to output the following details -¬†Speaker Name , Room , Start time,Duration and Title\nTo accomplish this I examined the node for Williams session\n1 2 3 4 5 6 7 8 9 10 11 12 $i = 372 $baseURL = ‚Äúhttp://www.sqlsaturday.com/eventxml.aspx?sat=‚Äù $DestinationFile = ‚ÄúE:\\SQLSatData\\SQLSat‚Äù + $i + ‚Äú.xml‚Äù $sourceURL = $baseURL + $i $doc = New-Object System.Xml.XmlDocument $doc.Load($sourceURL) $doc.Save($DestinationFile) $Sessions = $doc.GuidebookXML.events $Sessions.event[39] I then established that to get the speakers name I had to obtain the value from the child node which I accomplished as follows\n1 2 $Speaker = @{Name=\u0026#34;Speaker\u0026#34;; Expression = {$_.speakers.speaker.name}} $Sessions.event[39]|select $Speaker #To check that it worked This is an easy way to obtain sub(or child) properties within a select in PowerShell and I would recommend that you practice and understand that syntax of @{Name=\u0026quot;\u0026quot;; Expression = {} } which will enable you to perform all kinds of manipulation on those objects. You are not just limited to obtaining child properties but can perform calculations as well\nI did the same thing to get the room and the start time\n1 2 3 $Room = @{Name=\u0026#34;Room\u0026#34;; Expression = {$_.location.name}} $StartTime = @{Name=\u0026#34;StartTime\u0026#34;; Expression = {$_.StartTime}} $Sessions.event[39]|select $Speaker,$Room,$StartTime #To check that it worked I then needed duration and thought that I could use\n1 2 $Duration = @{Name =\u0026#34;Duration\u0026#34;; Expression = {($_.EndTime) - ($_.StartTime)}} $Sessions.event[39]|select $duration However that just gave me a blank result so to troubleshoot I ran\n$Sessions.event[39].endtime - $sessions.event[39].startTime\nWhich errored with the (obvious when I thought about it) message\nCannot convert value \u0026ldquo;4/25/2015 4:10:00 PM\u0026rdquo; to type \u0026ldquo;System.Int32\u0026rdquo;. Error: \u0026ldquo;Input string was not in a correct format.\u0026rdquo; At line:1 char:1 + $Sessions.event[39].endtime - $sessions.event[39].startTime + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument: (:) [], RuntimeException + FullyQualifiedErrorId : InvalidCastFromStringToInteger\nThe value was stored as a string\nRunning\n$Sessions.event[39].endtime |Get-Member\nshowed me that there was a method called ToDateTime but there is an easier way.¬†By defining the datatype of an object PowerShell will convert it for you so the resulting code looks like this\n1 2 3 4 5 6 $Sessions = $doc.GuidebookXML.events $Speaker = @{Name=\u0026#34;Speaker\u0026#34;; Expression = {$_.speakers.speaker.name}} $Room = @{Name=\u0026#34;Room\u0026#34;; Expression = {$_.location.name}} $Duration = @{Name =\u0026#34;Duration\u0026#34;; Expression = {[datetime]($_.EndTime) - [datetime]($_.StartTime)}} $startTime = @{Name=\u0026#34;StartTime\u0026#34;; Expression = {[datetime]($_.StartTime)}} $Sessions.event|select $Speaker,$Room,$Starttime,$Duration,Title |Format-Table -AutoSize -Wrap and the resulting entry is finally as I required it. I believe that this will use the regional settings from the installation on the machine that you are using but I have not verified that. If anyone in a different region would like to run this code and check that that is the case I will update the post accordingly\nHopefully you have learnt from this how you can extend select from the pipeline and how defining the datatype can be beneficial. Any questions please comment below\n","date":"2015-03-21T00:00:00Z","permalink":"https://blog.robsewell.com/blog/parsing-xml-child-nodes-and-converting-to-datetime-with-powershell/","title":"Parsing XML Child Nodes and Converting to DateTime with PowerShell"},{"content":"Just a quick post to say that I will be speaking at the¬†PowerShell Virtual Chapter meeting this Thursday at 4pm GMT 12pm EDT and also at the Cardiff SQL User Group on Tuesday 31st March\nI will be giving my Making PowerShell Useful for your Team presentation\nYou have heard about PowerShell and may be spent a little bit of time exploring some of the ways in which it will benefit you at work. You want to be able to perform tasks more specific to your organisation and need to share them with your team. I will show you how you can achieve this by demonstrating\nAn easy way to learn the syntax How to explore SQL Server with PowerShell How to turn your one off scripts into shareable functions How to ensure that your team can easily and quickly make use of and contribute to PowerShell solutions Where else to go for help You can find out more about the Virtual Chapter here\nhttp://PowerShell.sqlpass.org/¬†and the Cardiff meeting here\nhttp://www.meetup.com/Cardiff-SQL-Server-User-Group/events/219492623/¬†The Cardiff meeting has been named The Battle Of The Beards as it features¬†Tobiasz Koprowski: talking about Windows Azure SQL Database - Tips and Tricks for beginners and Terry McCann with SSRS Inception. I will be giving the same presentation as at the Virtual Chapter\nI hope to see you at one or both sessions\n","date":"2015-03-17T00:00:00Z","permalink":"https://blog.robsewell.com/blog/speaking-at-powershell-virtual-chapter-and-sql-cardiff-user-group-this-month/","title":"Speaking at PowerShell Virtual Chapter and SQL Cardiff User Group this month"},{"content":"A slightly different topic today.\nOnce you have built up knowledge, you become the person that people ask to solve things. This is something I really enjoy, taking a problem and solving it for people and in the process teaching them and enabling them to automate more things.\nA colleague was performing a new deployment of a product via SCCM and wanted to trigger the clients to update and receive the new update instead of waiting for it to be scheduled.\nThey had found some code that would do this\n1 2 3 4 Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000121}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000021}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000022}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000002}\u0026#34;|Out-Null They had the idea of using this command and a text file containing the machines and PS Remote.\nI looked at it a different way and gave them a function so that they could provide the Collection Name (In SCCM a collection is a list of machines for a specific purpose) and the function would import the SCCM module, connect to the Site get the names of the machines in the collection and run the command on each one\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 function Trigger-DeploymentCycle { param ( [string]$CollectionName ) # PS script to run $scriptblock = { Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000121}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000021}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000022}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000002}\u0026#34;|Out-Null } ## import SCCM module Import-Module (Join-Path $(Split-Path $env:SMS_ADMIN_UI_PATH) ConfigurationManager.psd1) #open drive for SCCM cd \u0026lt;Site Code\u0026gt;:\\ #### cd \u0026lt;Site Code\u0026gt;:\\ replace with Site Code or add param $SiteCOde and use cd ${$SiteCode}:\\ # Get Computer names in collection $PCs = (Get-CMDeviceCollectionDirectMembershipRule -CollectionName $CollectionName).rulename $Count = $PCs.count Write-Output \u0026#34;Total number of PCs = $Count\u0026#34; Invoke-Command ‚ÄìComputerName $PCs ‚ÄìScriptBlock $scriptblock ‚ÄìThrottleLimit 50 } This would work very well but they wanted some error checking to enable them to identify machines they were unable to connect to following the deployment so the final solution which will run a little slower\nSet up function and parameters and create log files\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function Trigger-DeploymentCycle { param ( [string]$CollectionName ) # Create log file $StartTime = Get-Date $Date = Get-Date -Format ddMMyyHHss $Errorlogpath = \u0026#34;C:\\temp\\SCCMError\u0026#34; + $Date + \u0026#34;.txt\u0026#34; $Successlogpath = \u0026#34;C:\\temp\\SCCMSuccess\u0026#34; + $Date + \u0026#34;.txt\u0026#34; New-Item -Path $Errorlogpath -ItemType File New-Item -Path $Successlogpath -ItemType File $StartLog = \u0026#34;Script Started at $StartTime\u0026#34; $StartLog | Out-File -FilePath $Successlogpath -Append Create the script block, import the SCCM module, connect to the SCCM site and get the machines in the collection. Note that you will have to change \u0026lt;Site Code\u0026gt; with your own site code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $scriptblock = { Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000121}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000021}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000022}\u0026#34;|Out-Null Invoke-WMIMethod -Namespace root\\ccm -Class SMS_CLIENT -Name TriggerSchedule \u0026#34;{00000000-0000-0000-0000-000000000002}\u0026#34;|Out-Null } ## import SCCM module Import-Module (Join-Path $(Split-Path $env:SMS_ADMIN_UI_PATH) ConfigurationManager.psd1) #open drive for SCCM cd \u0026lt;Site Code\u0026gt;:\\ #### cd \u0026lt;Site Code\u0026gt;:\\ replace with Site Code or add param $SiteCOde and use cd ${$SiteCode}:\\ # Get Computer names in collection $PCs = (Get-CMDeviceCollectionDirectMembershipRule -CollectionName $CollectionName).rulename $Count = $PCs.count Write-Output \u0026#34;Total number of PCs = $Count\u0026#34; I wanted to give them a progress output so I needed to be able to identify the number of machines in the collection by using the count property. I then needed to output the number of the item within the array which I did with\n1 2 $a= [array]::IndexOf($PCs, $PC) + 1 Write-Output \u0026#34; Connecting to PC - $PC -- $a of $count\u0026#34; I then pinged the machine,ran the script block and wrote to the log files and finally opened the log files\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 if (Test-Connection $PC -Quiet -Count 1) { # Run command on PC Invoke-Command -ComputerName $PC -scriptblock $scriptblock $Success = \u0026#34;SUCCESS - finished - $PC -- $a of $count\u0026#34; $Success | Out-File -FilePath $Successlogpath -Append Write-Output $Success } else { $ErrorMessage = \u0026#34;ERROR - $PC is not available -- $PC -- $a of $count\u0026#34; $ErrorMessage| Out-File -FilePath $Errorlogpath -Append Write-Output $ErrorMessage } } notepad $Errorlogpath notepad $Successlogpath Now they can load the function into their PowerShell sessions and type\nTriggerDeplyment COLLECTIONNAME\nand they will be able to manually trigger the tasks. This function will trigger the following tasks for a list of PCs in a collection.\nMachine Policy Assignment Request \u0026ndash; {00000000-0000-0000-0000-000000000021} Machine Policy Evaluation \u0026ndash; {00000000-0000-0000-0000-000000000022} Software Inventory \u0026ndash; {00000000-0000-0000-0000-000000000002} Application Deployment Evaluation Cycle: {00000000-0000-0000-0000-000000000121} Here is the list of other tasks you can trigger:\nDiscovery Data Collection Cycle: {00000000-0000-0000-0000-000000000003} Hardware Inventory Cycle: {00000000-0000-0000-0000-000000000001} Machine Policy Retrieval and Evaluation Cycle: {00000000-0000-0000-0000-000000000021} Software Metering Usage Report Cycle: {00000000-0000-0000-0000-000000000031} Software Updates Deployment Evaluation Cycle: {00000000-0000-0000-0000-000000000108} Software Updates Scan Cycle: {00000000-0000-0000-0000-000000000113} Windows Installer Source List Update Cycle: {00000000-0000-0000-0000-000000000032} Hardware Inventory={00000000-0000-0000-0000-000000000001} Software Update Scan={00000000-0000-0000-0000-000000000113} Software Update Deployment Re-eval={00000000-0000-0000-0000-000000000114} Data Discovery={00000000-0000-0000-0000-000000000003} Refresh Default Management Point={00000000-0000-0000-0000-000000000023} Refresh Location (AD site or Subnet)={00000000-0000-0000-0000-000000000024} Software Metering Usage Reporting={00000000-0000-0000-0000-000000000031} Sourcelist Update Cycle={00000000-0000-0000-0000-000000000032} Cleanup policy={00000000-0000-0000-0000-000000000040} Validate assignments={00000000-0000-0000-0000-000000000042} Certificate Maintenance={00000000-0000-0000-0000-000000000051} Branch DP Scheduled Maintenance={00000000-0000-0000-0000-000000000061} Branch DP Provisioning Status Reporting={00000000-0000-0000-0000-000000000062} Refresh proxy management point={00000000-0000-0000-0000-000000000037} Software Update Deployment={00000000-0000-0000-0000-000000000108} State Message Upload={00000000-0000-0000-0000-000000000111} State Message Cache Cleanup={00000000-0000-0000-0000-000000000112} You can find the function here\nTrigger-Deployment\nand all of my Script Center Submissions are here\nAs always ‚Äì The internet lies, fibs and deceives and everything you read including this post should be taken with a pinch of salt and examined carefully. All code should be understood and tested prior to running in a live environment.\n","date":"2015-02-18T00:00:00Z","permalink":"https://blog.robsewell.com/blog/triggering-a-system-center-configuration-manager-deployment-task/","title":"Triggering a System Center Configuration Manager deployment task"},{"content":"This week I was reading Pinal Daves post about Autogrowth Events\nhttp://blog.sqlauthority.com/2015/02/03/sql-server-script-whenwho-did-auto-grow-for-the-database/\nas it happened I had a requirement to make use of the script only a few days later. I was asked to provide the information in a CSV so that the person who required the information could manipulate it in Excel.\nI am a great believer in Automation. If you are going to do something more than once then automate it so I wrote two functions, added them to TFS and now they will be available to all of my team members next time they load PowerShell.\nWhy two functions? Well Pinal Daves script gets the information from the default trace for a single database but there may be times when you need to know the autogrowth events that happened on a server with multiple databases.\nI use a very simple¬†method for doing this as I have not found the correct way to parse the default trace with PowerShell.¬†The functions rely on Invoke-SQLCMD2¬†which I also have in my functions folder and pass the query from Pinal Daves Blog post as a here string\n$Results = Invoke-Sqlcmd2 -ServerInstance $Server -Database master -Query $Query\nTo output to CSV I use the Export-CSV cmdlet\n1 2 3 4 if($CSV) { $Results| Export-Csv -Path $CSV } And to open the CSV I add a [switch] parameter. You can find out more about parameters here¬†or by\nGet-Help about_Functions_Advanced_Parameters\nso the parameter block of my function looks like\n1 2 3 4 5 6 7 8 9 10 11 param ( [Parameter(Mandatory=$true)] [string]$Server, [Parameter(Mandatory=$true)] [string]$Database, [Parameter(Mandatory=$false)] [string]$CSV, [Parameter(Mandatory=$false)] [switch]$ShowCSV ) Now when I am asked again to provide this information it is as easy as typing\nShow-AutogrowthServer -Server SQL2014Ser12R2\nor\nShow-AutogrowthDatabase -Server SQL2014Ser12R2 -Database Autogrowth\nand the results will be displayed as below\njust a side note. Pinal Daves script uses @@servername in the where clause and if you have renamed your host the script will be blank. The resolution to this is to runt he following T-SQL\n1 2 3 4 sp_dropserver \u0026#39;OLDSERVERNAME\u0026#39;; GO sp_addserver NEWSERVERNAME, local; GO You can find the scripts here\nShow-AutoGrowthServer\nShow-AutoGrowthDatabase\nand all of my Script Center Submissions are here\nAs always - The internet lies, fibs and deceives and everything you read including this post should be taken with a pinch of salt and examined carefully. All code should be understood and tested prior to running in a live environment.\n","date":"2015-02-15T00:00:00Z","permalink":"https://blog.robsewell.com/blog/show-autogrowth-events-with-powershell-to-csv/","title":"Show AutoGrowth Events with PowerShell to CSV"},{"content":"Azure File Storage enables you to present an Azure Storage Account to your IaaS VMs as a share using SMB. You can fid out further details here\nhttp://azure.microsoft.com/en-gb/documentation/articles/storage-dotnet-how-to-use-files/¬†Once you have created your Azure File Storage Account and connected your Azure Virtual Machines to it, you may need to upload data from your premises into the storage to enable it to be accessed by the Virtual Machines\nTo accomplish this I wrote a function and called it Upload-ToAzureFileStorage\nI started by creating a source folder and files to test\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New2 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New3 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New4 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New5 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\b -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\c -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\d -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\1 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\2 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\3 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\4 -ItemType Directory New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New2\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New3\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New4\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New5\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\1\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\2\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\3\\\\file.txt -ItemType File New-Item -Path C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\4\\\\file.txt -ItemType File Then we needed to connect to the subscription, get the storage account access key and create a context to store them\n1 2 3 4 5 6 7 8 #Select Azure Subscription Select-AzureSubscription -SubscriptionName $AzureSubscriptionName # Get the Storage Account Key $StorageAccountKey = (Get-AzureStorageKey -StorageAccountName $StorageAccountName).Primary # create a context for account and key $ctx=New-AzureStorageContext $StorageAccountName $StorageAccountKey The¬†Get-AzureStorageShare cmdlet¬†shows the shares available for the context so we can check if the share exists\n$S = Get-AzureStorageShare -Context $ctx -ErrorAction SilentlyContinue|Where-Object {$\\_.Name -eq $AzureShare}\nand if it doesnt exist create it using¬†New-AzureStorageShare\n1 $s = New-AzureStorageShare $AzureShare -Context $ctx For the sake only of doing it a different way we¬†can check for existence of the directory in Azure File Storage that we are going to upload the files to like this\n1 2 3 $d = Get-AzureStorageFile -Share $s -ErrorAction SilentlyContinue|select Name if ($d.Name -notcontains $AzureDirectory) and if it doesnt exist create it using¬†New-AzureStorageDirectory\n1 $d = New-AzureStorageDirectory -Share $s -Path $AzureDirectory Now that we have the directory created in the storage account we need to create any subfolders. First get the folders\n1 2 \\# get all the folders in the source directory $Folders = Get-ChildItem -Path $Source -Directory -Recurse We can then iterate through them using a foreach loop. If we do this and select the FullName property the results will be\n1 C:\\\\temp\\\\TestUpload\\\\New1 C:\\\\temp\\\\TestUpload\\\\New2 C:\\\\temp\\\\TestUpload\\\\New3 C:\\\\temp\\\\TestUpload\\\\New4 C:\\\\temp\\\\TestUpload\\\\New5 C:\\\\temp\\\\TestUpload\\\\New1\\\\list C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\b C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\c C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\d C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\1 C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\2 C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\3 C:\\\\temp\\\\TestUpload\\\\New1\\\\list\\\\a\\\\4 but to create new folders we need to remove the \u0026quot;C:\\\\temp\\\\TestUpload\u0026quot; and replace it with the Directory name in Azure. I chose to do this as follows using the substring method and the length of the source folder path.\n1 2 3 4 foreach($Folder in $Folders) { $f = ($Folder.FullName).Substring(($source.Length)) $Path = $AzureDirectory + $f and tested that the results came out as I wanted\n1 AppName\\\\New1 AppName\\\\New2 AppName\\\\New3 AppName\\\\New4 AppName\\\\New5 AppName\\\\New1\\\\list AppName\\\\New1\\\\list\\\\a AppName\\\\New1\\\\list\\\\b AppName\\\\New1\\\\list\\\\c AppName\\\\New1\\\\list\\\\d AppName\\\\New1\\\\list\\\\a\\\\1 AppName\\\\New1\\\\list\\\\a\\\\2 AppName\\\\New1\\\\list\\\\a\\\\3 AppName\\\\New1\\\\list\\\\a\\\\4 I could then create the new folders in azure using¬†New-AzureStorageDirectory again\n1 New-AzureStorageDirectory -Share $s -Path $Path -ErrorAction SilentlyContinue I followed the same process with the files\n1 2 3 4 5 $files = Get-ChildItem -Path $Source -Recurse -File\u0026lt;/pre\u0026gt; \u0026lt;pre\u0026gt;foreach($File in $Files) { $f = ($file.FullName).Substring(($Source.Length)) $Path = $AzureDirectory + $f and then created the files using Set-AzureStorageFileContent this has a -Force and a -Confirm switch and I added those into my function by using a [switch] Parameter\n1 2 3 4 5 6 7 8 9 10 #upload the files to the storage if($Confirm) { Set-AzureStorageFileContent -Share $s -Source $File.FullName -Path $Path -Confirm } else { Set-AzureStorageFileContent -Share $s -Source $File.FullName -Path $Path -Force } You can download the function from the Script Center\nhttps://gallery.technet.microsoft.com/scriptcenter/Recursively-upload-a-bfb615fe\nAs also, any comments or queries are welcome and obviously the internet lies so please understand and test all code you find before using it in production\n","date":"2015-02-01T00:00:00Z","permalink":"https://blog.robsewell.com/blog/uploading-a-source-folder-to-azure-file-storage/","title":"Uploading a Source Folder to Azure File Storage"},{"content":"Twas 2 days before Xmas \u0026amp; all through the office,\nnot a creature was stirring not even old Maurice.\nWith merriment going on outside of his window\nThere sat a bearded DBA without much to do\nNo changes can be made through the holiday season\nWe‚Äôre on skeleton support, which is a good reason\nEnsure you are making the most of your time\nYou mustn‚Äôt be wasting the company dime\nThe backups are checked, there isn‚Äôt an issue\nSo documentation writing should ensue\nInstead he decided to procrastinate\nAnd so, this little ditty he proceeded¬†to create\nLooking back over last year he did ruminate\nAbout all the progress he had made, it was great\nSo much had been learned, so many improvements\nDerived using content from fine ladies and gents\nImpossible to estimate how much it would cost\nOr calculate the amount of revenue lost\nFor all that he would have been unable to do\nOr the times that he knew how to get out of a stew\nBut also the friends old, new and the rest\nThe talking and dining and drinking and jest\nI am lucky to be a part of¬†the SQL Family\nSo thank you one and all, with love from me\n","date":"2014-12-23T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/12/beard.png","permalink":"https://blog.robsewell.com/blog/twas-2-days-before-xmas-or-thank-you-sqlfamily/","title":"Twas 2 Days Before Xmas or Thank you SQLFamily"},{"content":"\nT-SQL Tuesday, which was started by Adam Machanic (blog|twitter) and is now starting its 6th year, is hosted by a different person each month. The host selects the theme, and then the blogging begins. Worldwide, on the second Tuesday of the month (all day, based on GMT time), bloggers attend this party by blogging about the theme. This month it is hosted by Wayne Sheffield blog|twitter and in the spirit of the holiday season it is about giving. This is my post on giving back, in the little ways as well as the bigger ones, how you can give back and why it not only benefits others but also yourself\nWhat‚Äôs the SQL Family? The SQL Family (or SQL community call it what you will) is a fabulous place to be. Full of many wonderful, talented, passionate and generous people. Every method of interaction that you wish for can be found if you look. Ever wanted to know how others do it? or Does this happen in other shops? or I wish I had access to someone who knows about ‚Äòinsert new shiny thing here‚Äô?\nI guess that that is how I joined. I had no peers in my shop. I had no one to turn to to ask questions or get advice. I had no support and I turned to the internet. Now we all know that the internet lies. It is full of accidental and deliberate mistruths, of part information and downright bad advice. You have to be careful where you go and who you trust. I gradually found myself going back to the same resources and from those I found out about PASS and user groups\nI am in the UK. I found the list of UK SQL User Groups\nYou can find more here both local and virtual\nUser Groups I found a user group near me and went along to a meeting not knowing what to expect. I found a group of people like me willing to give up their time to learn and share knowledge. A wide range of people from DBAs, Developers, BI Professionals and SysAdmins. Working for International multi regional companies looking after many hundreds of SQL Servers to single sysadmins looking after the whole kit and caboodle and everything in between. A wealth and breadth of knowledge to tap into. You will learn so much not only from the sessions but also the conversation with all these other talented people\nCome along.\nSimply coming along will bring benefit. Other people will be interested in what you have to say even if you are in week 0 of your first ever job. Your view will still be valued. Everyone can learn from everybody and NO-ONE knows it all.\nThere will come a point where you will pass on a piece of knowledge or an idea or a way of working and someone will say thank you I didn‚Äôt know that. You just gave back. It may even be someone you look up to, someone whose knowledge and experience far outweighs yours whose word you hang on to. That feels good.\nYou may ask the questions that others thought but didnt ask and boy are they glad you asked the question. You just gave back. It‚Äôs something I do often. I ask questions and sometimes I ask questions I know the answer to so that they will be of benefit to the group.\nWhat will you get? More than you can ever put in. Free training, often free pizza, knowledge, advice,guidance, contacts, support, a network of people in your field, notice of job openings, swag, fun, friends, more social events and more and more\nThe user groups are run by volunteers in their own time out of the goodness of their hearts. They will always need your help. Turn up 5 minutes earlier and help set out the chairs or put out the handouts or assist with the tech. You just gave back. Afterwards before going to the pub clear the tables, help carry the boxes, put the pizza evidence in the bin. You just gave back\nSQL Saturdays and other community events SQL Saturdays are held all over the world most every Saturday You can find more about them here https://www.sqlsaturday.com/ There are also other larger events such as SQL Bits and SQL Relay here in the UK. Everything I wrote about User groups counts here just in a slightly larger scale. You will be able to attend several sessions across many different areas for free on a Saturday\nThese events are also run by volunteers and they will also need your help. If you can spare some time to help on a registration desk you just gave back. A room monitor to ensure the speaker and delegates have everything they need, the room is tidy and the session runs to time. You just gave back. Putting things out and tidying them away again. You just gave back.\nYou can become a volunteer by asking the people organising the events if they would like your help. These events will all have twitter feeds and emails and facebook pages and many methods of getting in touch. Contact them and offer your help if you can. You just gave back.\nIf you fancy taking the next step then you can get involved in organising the events. This is hard work, great fun, a good thing to add to your CV and you just gave back. There are so many areas to get involved organising an event. Premises and technology, speakers and printers, volunteers and sponsors all need co-ordination. Websites,twitter feeds, feedback forms, posters, marketing materials all need designing and producing. There are so many ways in which you will be able to provide value to the event and you just gave back\nOh and whilst I am at it, when you attend an event\nSay Thank You to the volunteers. You just gave back.\nSpeaking and Blogging All the events named above need speakers. The bigger events like the SQL Saturdays and the large events like SQL Bits will generally have more established speakers but every user group will need speakers and they will be more likely to accept new speakers and will be very supportive if you say that you are a new speaker. Every speaker had to make their first presentation at some point and they all know how it feels and can provide guidance and advice. You will feel that you don‚Äôt have anything to speak about that others will want to hear about. You do. Your experience and your knowledge or how you solved something or created something will be of interest to people. Of course, you need to check with the user group leaders and members if your idea for a presentation is suitable. Like anything you do that is new, researching it and taking advice from people with more experience is always useful. Maybe you can start with a lightning talk. Give it a go. You just gave back.\nWhat do you get back from Speaking and Blogging?\nI‚Äôll tell you a secret. The vast majority of my posts ( This is an exception) are written for the benefit of one person. Me.\nI write posts to record my learning for myself. To document my knowledge. I use my posts to help me to do my job. I know that I wrote the method of doing something somewhere and this is part of my store of knowledge. I write posts to answer peoples questions so that I have a place to point them to. Occasionally people will email me asking a question and if it requires a detailed response I will write a post and email them to tell them that this is the answer. I often point my work colleagues at my blog when they ask me questions about Azure or Powershell. You could also see your blog as an extension of your CV and use it when job hunting and develop it in that way\nI also write posts to expand my knowledge and this is the same for speaking. When I am writing a blog post or a presentation I will read and watch videos and ensure I know more about it. The process of creating that content will improve my own knowledge and work practices and you will find that, as you write your blog posts you will have a deeper knowledge also. When you give your presentations you will learn as you answer questions or find the answer to the question afterwards (It‚Äôs ok to do that) that you are improving yourself and your knowledge.You will also be giving back.\nPutting your information online will enable people to find it. Sure you can worry about SEO and getting to the top of search pages but you know that sometimes the answer is on the ninth page. What you write will be of benefit to others and by taking the time to post you will be giving back to the community\nYou can do one, many or all of those and you will be giving back. I hope you do\nI will be giving back. You will find me at SQL Bits where I shall be room monitoring and volunteering.\nYou will find me at SQL Saturday Exeter. I am again one of the fabulous SQL South West team\nwho are again organising a SQL Saturday in Exeter in the UK on April 24th/25th 2015 You can find out more here http://sqlsouthwest.co.uk/sql-saturday-372/\nYou still have time, if you are quick, to submit a session to speak or present a pre-con at Exeter. Submissions close on 15th December and we would love to have yours\nI shall carry on blogging and hopefully present at some user groups again this year. If you see me any where, come up and say hi to me. You just gave back\n","date":"2014-12-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/giving-back-tsql2sday/","title":"Giving Back ‚Äì #TSQL2sday"},{"content":"Having a Change Log is a good thing. A quick and simple place to find out what has changed on a server and when. This can be invaluable when troubleshooting, matching a change to a symptom especially when assessed alongside your performance counter collection. Here is a simple way to make use of a change log and automate it\nCreate a simple table\nUSE [MDW]\rGO\rCREATE TABLE [dbo].[ChangeLog](\r[ChangeID] [int] IDENTITY(1,1) PRIMARY KEY ,\r[Date] [datetime] NOT NULL,\r[Server] [varchar](50) NOT NULL,\r[UserName] [nvarchar](50) NOT NULL,\r[Change] [nvarchar](max) NOT NULL,\r)\rGO\rYou can keep this on a central server or create a database on each server, whichever fits your needs best. You can add other columns if you want your information in a different format\nOnce you have your table you can create a couple of Powershell functions to easily and quickly add to and retrieve data from the table. I make use of Invoke-SQLCMD2 in these functions\nThis can then be included in any automation tasks that you use to update your environments whether¬†you are using automated deployment methods for releases or using SCCM to patch your environments making it easy to update and also easy to automate by¬†making it part of your¬†usual deployment process.\nTo add a new change\n\u0026lt;#\r.Synopsis\rA function to add a ChangeLog information\r.DESCRIPTION\rLoad function for adding a change to the changelog table in the MDW database on MDWSERVER.\rUse Get-ChangeLog $Server to see details\rInputs the username of the account running powershell into the database as the user\rREQUIRES Invoke-SQLCMD2\rhttps://blog.robsewell.com\r.EXAMPLE\rAdd-ChangeLog SERVERNAME \u0026quot;Altered AutoGrowth Settings for TempDB to None\u0026quot;\rAdds ServerName UserName and Altered AutoGrowth Settings for TempDB to None to the change log table\r#\u0026gt;\rFunction Add-ChangeLog\r{\r[CmdletBinding()]\rParam(\r[Parameter(Mandatory=$True)]\r[string]$Server,\r[Parameter(Mandatory=$True)]\r[string]$Change\r)\r$UserName = $env:USERDOMAIN + '\\' + $env:USERNAME\r$Query = \u0026quot;INSERT INTO [dbo].[ChangeLog]\r([Date]\r,[Server]\r,[UserName]\r,[Change])\rVALUES\r(GetDate()\r,'$Server'\r,'$UserName'\r,'$Change')\r\u0026quot;\rInvoke-Sqlcmd2 -ServerInstance MDWSERVER -Database \u0026quot;MDW\u0026quot; -Query $Query -Verbose\r}\rYou can then run\nAdd-ChangeLog SERVERNAME \u0026quot;Added New Database SuperAppData\u0026quot;\rto add the change to the change log\nTo retrieve the data you can use\n\u0026lt;#\r.Synopsis\rA function to get ChangeLog information\r.DESCRIPTION\rLoad function for finding ChangeLog information. Information is selected from the MDW Database on SERVERNAME\rREQUIRES Invooke-SQLCMD2\rhttps://blog.robsewell.com\r.EXAMPLE\rGet-ChangeLog SERVERNAME\r#\u0026gt;\rFunction Get-ChangeLog\r{\r[CmdletBinding()]\r[OutputType([int])]\rParam\r(\r# Server Name Required\r[Parameter(Mandatory=$true,]\r$Server\r)\r$a = @{Expression={$_.Date};Label=\u0026quot;Date\u0026quot;;width=15}, `\r@{Expression={$_.Server};Label=\u0026quot;Server\u0026quot;;width=10},\r@{Expression={$_.UserName};Label=\u0026quot;UserName\u0026quot;;width=20}, `\r@{Expression={$_.Change};Label=\u0026quot;Change\u0026quot;;width=18}\rInvoke-Sqlcmd2 -ServerInstance MDWSERVER -Database \u0026quot;MDW\u0026quot; -Query \u0026quot;SELECT * FROM dbo.ChangeLog WHERE Server = '$Server';\u0026quot; -Verbose|Format-table $a -Auto -Wrap\r}\rand use\nGet-ChangeLog SERVERNAME\rTo find out what changed when. Happy Automating\n","date":"2014-12-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/making-a-change-log-easier-with-powershell/","title":"Making a Change Log Easier With PowerShell"},{"content":"Operational Insights is a service that has been added in preview to Azure. It enables you to collect, combine, correlate and visualize all your machine data in one place. It can collect data from all of your machines either via SCOM or by using an agent. Once the data is collected Operational Insights has a number of Intelligence Packs which have pre-configured rules and algorithms to provide analysis in various areas including for SQL Server\nhttp://azure.microsoft.com/en-gb/services/operational-insights/\nI thought I would take a look. I have an installation of SCOM in my lab on my laptop and I read the instructions to see how to connect it to Operational Insights. (You don‚Äôt have to have a SCOM installation to use Operational insights you can make use of an agent as well just follow the steps from the page below)\nhttp://azure.microsoft.com/en-us/trial/operational-insights-get-started/\nIt really is very simple\nIf you have an Azure subscription already you can sign into the portal and join the preview program by clicking\nNew ‚Äì\u0026gt; App Services ‚Äì\u0026gt; Operational Insights\nand create a new Operational Insights Workspace.\nOnce you have done that, if you have an installation of SCOM 2012 you need to be running Service Pack 1 and¬†download and install the System Center Operational Insights Connector for Operations Manager and import the MPB files into SCOM.\nIf you have SCOM 2012R2 the connector is already installed and to connect your SCOM to Operational Insights is very very easy as you can see on\nhttp://azure.microsoft.com/en-us/trial/operational-insights-get-started/?step2=withaccount\u0026amp;step3=SCOMcustomer\nIn the Operations Manager Console, click¬†Administration. Under¬†Administration, select¬†System Center Advisor, and then click Advisor Connection. Click¬†Register to Advisor Service. Sign in with your Microsoft or Organizational account. Choose an existing Operational Insights workspace from the drop down menu Confirm your changes. In the System Center Advisor Overview page, Under¬†Actions, click¬†Add a Computer/Group. Under Options, select Windows Server or All Instance Groups, and then search and add servers that you want data That is it. No really, that is it. I was amazed how quickly I was able to get this done in my lab and it would not take very long in a large implementation of SCOM either as you will have your groups of computers defined which will make it easy to decide which groups to use. You could use a separate workspace for each type of server or split up the information per service. It really is very customisable.\nOnce you have done that, go and add some of the Intelligence Packs. Each intelligence pack will change the amount¬†and type of data that is collected. At November 23rd there are\nAlert Management ‚Äì for your SCOM Alerts Change Tracking ‚Äì Tracking Configuration Changes Log Management ‚Äì for event log collection and interrogation System Update Assessment ‚Äì Missing Security Updates Malware Assessment ‚Äì Status of Anti-Malware and Anti-Virus scans Capacity Planning ‚Äì Identify Capacity and Utilisation bottlenecks SQL Assessment ‚Äì The risk and health of SQL Server Environment There are also two ‚Äòcoming soon‚Äô Intelligence packs\nAD Assessment ‚Äì Risk and health of Active Directory Security ‚Äì Explore security related data and help identify security breaches You then (if you are like me) have a period of frustration whilst you wait for all of the data to be uploaded and aggregated but once it is you sign into the Operational Insights Portal\nhttps://preview.opinsights.azure.com and it will look like this\nThere is a lot of information there. As it is on my laptop and the lab is not running all of the time and is not connected to the internet most of the time I am not surprised that there are some red parts to my assessment!!\nObviously I was interested in the SQL Assessment and I explored it a bit further\nClicking on the SQL Assessment tile takes you to a screen which shows the SQL Assessment broken down into 6 Focus areas\nSecurity and Compliance, Availability and Business Continuity, Performance and Scalability, Upgrade, Migration and¬†Deployment, Operations and Monitoring and Change and Configuration Management. MSDN http://msdn.microsoft.com/en-us/library/azure/dn873967.aspx gives some more information about each one\nSecurity and Compliance¬†‚Äì Safeguard the reputation of your organization by defending yourself from security threats and breaches, enforcing corporate policies, and meeting - technical, legal and regulatory compliance requirements. Availability and Business Continuity¬†‚Äì Keep your services available and your business profitable by ensuring the resiliency of your infrastructure and by having the right - level of business protection in the event of a disaster. Performance and Scalability¬†‚Äì Help your organization to grow and innovate by ensuring that your IT environment can meet current performance requirements and can respond - quickly to changing business needs. Upgrade, Migration and Deployment¬†‚Äì Position your IT department to be the key driver of change and innovation, by taking full advantage of new enabling technologies to - unlock more business value for organizational units, workforce and customers. Operations and Monitoring¬†‚Äì Lower your IT maintenance budget by streamlining your IT operations and implementing a comprehensive preventative maintenance program to - maximize business performance. Change and Configuration Management¬†‚Äì Protect the day-to-day operations of your organization and ensure that changes won‚Äôt negatively affect the business by establishing change control procedures and by tracking and auditing system configurations. You will be able to see some dials showing you how well you are doing in each area for the servers whose data has been collected.\nEach area will have the High Priority Recommendations shown below the dial and you can click on them to see more information about those recommendations\nYou can also click the dial or the see all link to enter the search area where you can customise how you wish to see the data that has been collected, this looks a bit confusing at first\nThe top bar contains the search , the timescale and some buttons to save the search, view the saved searches and view the search history, all of which will be shown in the right hand column below\nThe left column contains a bar graph for the search and all of the filters. The middle column contains the results of the search and can be viewed in list or tabular format and exported to CSV using the button below. A little bit of experimentation will give you a better understanding of how the filtering works and how you can make use of that for your environment\nBy looking at the search for the Operations and Monitoring Focus Area shown above\nType:SQLAssessmentRecommendation IsRollup=true RecommendationPeriod=2014-11 FocusArea=‚ÄùOperations and Monitoring‚Äù RecommendationResult=Failed | sort RecommendationWeight desc\nI saw that RecommendationResult=Failed and changed it to RecommendationResult=Passed. This enabled me to see all of the Recommendations that had been passed in the Focus Area and clicking the export button downloaded a csv file. I deleted RecommendationResult=Passed from the search and that gave me all of the recommendations that made up that Focus Area\nOperations and Monitoring Focus Area Recommendation Enable Remote Desktop on servers. Enable Remote Desktop on virtual machines. Ensure computers are able to download updates. Configure event logs to overwrite or archive old events automatically. Review event log configuration to ensure event data is retained automatically. This relates to System Logs Review event log configuration to ensure event data is retained automatically. This relates to Application Logs I decided then to do the same for each of the Focus Areas for the SQL Assessment Intelligence Pack\nSecurity and Compliance Focus Area\nRecommendation\nChange passwords that are the same as the login name. Remove logins with blank passwords. LAN Manager Hash for Passwords Stored Investigate why unsigned kernel modules were loaded. Apply security best practices to contained databases. Enable User Account control on all computers. Consider disabling the xp_cmdshell extended stored procedure. Implement Windows authentication on Microsoft Azure-hosted SQL Server deployments. Avoid using the Local System account to run the SQL Server service. Avoid adding users to the db_owner database role. Ensure only essential users are added to the SQL Server sysadmin server role. Disable SQL Server guest user in all user databases. Avoid running SQL Server Agent jobs using highly-privileged accounts. Configure the SQL Server Agent service to use a recommended account. Apply Windows password policies to SQL Server logins. Investigate failures to validate the integrity of protected files. Investigate failures to validate kernel modules. Availability and Business Continuity Focus Area\nRecommendation\nSchedule full database backups at least weekly. Optimize your backup strategy with Microsoft Azure Blob Storage. Avoid using the Simple database recovery model. Ensure all installations of Windows are activated. Investigate logical disk errors. Reduce the maximum Kerberos access token size. Investigate connection failures due to SSPI context errors. Set the PAGE_VERIFY database option to CHECKSUM. Increase free space on system drives. Investigate a write error on a disk. Check the network access to Active Directory domain controllers. Review DNS configuration on non-DNS servers. Increase free space on system drives. Investigate memory dumps. Increase free space on system drives. Investigate why the computer shut down unexpectedly. Enable dynamic DNS registration for domain-joined servers. Performance and Scalability Focus Area\nRecommendation\nIncrease the number of tempdb database files. Configure the tempdb database to reduce page allocation contention. Ensure all tempdb database files have identical initial sizes and growth increments. Set autogrowth increments for database files and log files to fixed values rather than percentage values. Set autogrowth increments for transaction log files to less than 1GB. Modify auto-grow settings to use a fixed size growth increment of less than 1GB and consider enabling Instant File Initialization. Change your Affinity Mask and Affinity I/O MASK settings to prevent conflicts. Resolve issues caused by excessive virtual log files. Modify the database file layout for databases larger than 1TB. Set the AUTO_CLOSE option to OFF for frequently accessed databases. Review memory requirements on servers with less than 4GB of physical memory installed. Configure system SiteName properties to be dynamic. Align the Max Degree of Parallelism option to the number of logical processors. Align the Max Degree of Parallelism option to the number of logical processors. Consider disabling the AUTO_SHRINK database option. Review memory requirements on computers with high paging file use. Ensure SQL Server does not consume memory required by other applications and system components. Consider changing your power saving settings to optimize performance. Increase the initial size of the tempdb database. Review the configuration of Maximum Transfer Unit (MTU) size. Review your paging file settings. Review and optimize memory cache configuration. Review the configuration of Maximum Transfer Unit (MTU) size. Review the system processor scheduling mode. Review network provider ordering settings. Remove invalid entries from the PATH environment variable. Remove network entries from the PATH environment variable. Investigate processes that use a large number of threads. Avoid hosting user database files on the same disk volume as tempdb database files. Review processes with large working set sizes. Reduce the length of the PATH environment variable. Reduce the number of entries in the PATH environment variable. Ensure SQL Server does not consume memory required by other applications and system components. Enable the backup compression default configuration option. Ensure the DNS Client service is running and is set to start automatically. Consider compressing database tables and indexes. Upgrade, Migration and Deployment Focus Area\nRecommendation\nEnsure all devices run supported operating system versions. Ensure that the guest user is enabled in the msdb database. Avoid using the Affinity64 Mask configuration setting in new development work. Avoid using the Affinity Mask configuration setting in new development work. Avoid using the Affinity I/O Mask configuration setting in new development work. Avoid using the Allow Updates configuration option in SQL Server. Avoid using the Allow Updates configuration option in SQL Server. Avoid using the Affinity64 I/O Mask configuration setting in new development work. Configure SQL Server to accept incoming connections. Configure SQL Server instances and firewalls to allow communication over TCP/IP. As I have no data for Change and Configuration Management I was not able to see the recommendations in my Operation Insights Workspace.\nEdit:¬†Daniele Muscetta has said in the comments that this is a bug¬†which is being tracked\nAs you can see from the type and description of the recommendations above these are all areas that a DBA will be concerned about and the benefit of having all of this information gathered, pre-sorted, prioritised and presented to you in this manner will enable you to work towards a better SQL environment and track your progress. You can read more about the SQL Assessment Intelligence Pack here\nhttp://msdn.microsoft.com/en-us/library/azure/dn873958.aspx\nAs well as the pre-determined queries that are built into the Intelligence pack you can search your data in any way that you require enabling you to present information about the health and risk of your SQL Environment to your team or your management with ease. The ‚Äúwith ease‚Äù bit is dependent on you understanding the language and structure of the search queries.\nYou will need to put this page into your bookmarks\nhttp://msdn.microsoft.com/library/azure/dn873997.aspx\nAs it contains the syntax and definitions to search your data\nA very useful page for a starter like me is\nhttp://blogs.msdn.com/b/dmuscett/archive/2014/10/19/advisor-searches-collection.aspx\nby Daniele Muscetta which has a list of useful Operational Insights search queries such as\nSQL Recommendation by Computer\nType=SQLAssessmentRecommendation IsRollup=false RecommendationResult=Failed | measure count() by Computer\nIf you click the star to the right of the search box you will find the saved searches. For the SQL Assessment Intelligence Pack there are\nDid the agent pass the prerequisite check (if not, SQL Assessment data won‚Äôt be complete)\nFocus Areas\nHow many SQL Recommendation are affecting a Computer a SQL Instance or a - Database? How many times did each unique SQL Recommendation trigger? SQL Assesments passed by Server SQL Recommendation by Computer SQL Recommendation by Database SQL Recommendation by Instance You can use these and you can save your own searches which show the data in a way that is valuable to you.\nOverall I am impressed with this tool and can see how it can be beneficial for a DBA as well as for System Administrators. I was amazed how easy it was to set up and¬†how quickly I was able to start manipulating the data once it had been uploaded.\n","date":"2014-11-24T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/11/opsman1.jpg","permalink":"https://blog.robsewell.com/blog/a-look-at-the-sql-assessment-intelligence-pack-in-operational-insights/","title":"A look at the SQL Assessment Intelligence Pack in Operational Insights"},{"content":"So you have read that you should have alerts for severity levels 16 to 24 and 823,824 and 825 on SQLSkills.com or maybe you have used sp_blitz and received the Blitz Result: No SQL Server Agent Alerts Configured and like a good and conscientious DBA you have set them up.\nHopefully you also have Jonathan Allens blog on your feed and if you look at his historical posts and seen this one where lack of a delay in response broke the Exchange Server!\nHowever sometimes the oft used delay between responses of 1 minute is too much. Alerts should be actionable after all and maybe you sync your email every 15 minutes and don‚Äôt need to see 15 alerts for the same error or you decide that certain level of errors require a lesser response and therefore you only need to know about them every hour or three. Or possibly you want to enforce a certain delay for all servers and want to set up a system to check regularly and enforce your rule\nWhatever the reason, changing the delay between response for every alert on every server with SSMS could be time consuming and (of course) I will use Powershell to do the job.\nTo find the alerts I follow the process I use when finding any new property in powershell\n$server = 'SERVERNAME'\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server\rI know that the Alerts will be found under the JobServer Property\n$srv.JobServer.Alerts|Get-Member\rShows me\nDelayBetweenResponses¬†Property¬†int DelayBetweenResponses {get;set;}\nAnd\nAlter¬†Method¬†void Alter(), void IAlterable.Alter()\nSo I use both of those as follows\nForeach($Alert in $srv.JobServer.Alerts){\r$Alert.DelayBetweenResponses = 600 # This is in seconds\r$Alert.Alter()\r}\rAnd place it in a foreach loop for the servers I want to change. If I only want to change certain alerts I can do so by filtering on Name\nForeach($Alert in $srv.JobServer.Alerts|Where-Object {$_.Name -eq 'NameOfAlert'})\rOr by category\nForeach($Alert in $srv.JobServer.Alerts|Where-Object {$_.CategoryName -eq 'Category Name'})\rWhen you have 5 minutes go and look at the results of\n$srv.JobServer|Get-Member\rAnd explore and let me know what you find\n","date":"2014-11-18T00:00:00Z","permalink":"https://blog.robsewell.com/blog/changing-delay-between-responses-for-sql-alerts-with-powershell/","title":"Changing Delay Between Responses for SQL Alerts with Powershell"},{"content":"What is T-SQL Tuesday? T-SQL Tuesday is a monthly blog party hosted by a different blogger each month. This blog party was started by Adam Machanic (blog|twitter). You can take part by posting your own participating post that fits the topic of the month and follows the requirements Additionally, if you are interested in hosting a future T-SQL Tuesday, contact Adam Machanic on his blog.\nThis month‚Äôs blog party is hosted by Chris Yates blog |twitter who asked people to share something newly learned.\nI love being a part of the SQL community. It gives me the opportunity to learn as much as I want to about anything I can think of within the data field. In the last couple of months I have presented at Newcastle User Group and learnt about migrating SQL using Powershell with Stuart Moore. At our user group in Exeter http://sqlsouthwest.co.uk/ we had Steph Middleton talking about version control for databases and lightning talks from Pavol Rovensky on Mocking in C# ,John Martin on Azure fault domains and availability sets using a pen and a whiteboard!, Annette Allen on Database Unit Testing,Terry McCann¬†on SQL Certifications. We also had Jonathan Allen talking about some free tools and resources to help manage both large and small SQL environments.¬†I went to SQL Relay in Southampton and saw Stuart Moore (again!) Scott Klein Alex Yates James Skipworth and I joined the PASS DBA fundamentals virtual chapter webinar for Changing Your Habits to Improve the Performance of Your T-SQL by Mickey Stuewe and that‚Äôs only the ‚Äòin-person‚Äô learning that I did. I also read a lot of blog posts!\nBut instead of repeating what I learnt from others within the community I thought I would write a blog post that I have been meaning to write for a few weeks about a solution pre-built into Windows that appears to not be well known. Problem Step Recorder.\nWhat is PSR? I found out about a little known tool included in Windows Operating System a couple of months ago which enables you to record what you are doing by taking screenshots of every mouse click. The tool is Step Recorder also known as PSR. It is included by default in Windows 7 , Windows 8 and 8.1 and Windows Server 2008 and above.\nWhat does it do? Simply put, it records ‚ÄúThis is what I did‚Äù There are many situations when this can be useful\nYou can use this during installations to help create documentation. ‚ÄúThis is what I did‚Äù when I installed X and now you can follow those steps and I know I haven‚Äôt missed anything. You can use it when communicating with 3rd parties or other support teams. ‚ÄúThis is what I did‚Äù when I got this error and here are all of the steps so that you can re-create the issue and I know that I haven‚Äôt missed anything You can use this when resolving high priority incidents. ‚ÄúThis is what I did‚Äù when System X broke, it includes all of the times of my actions. I still keep my notepad by my keyboard out of habit but I have a record of the exact steps that I took to try to resolve the issue which will be very useful for reporting on the incident in the near future and also placing into a Knowledge Base for others to use if it happens again and I know I haven‚Äôt missed anything For assisting family members. Like many, I am ‚ÄúThe IT guy‚Äù and PSR enables me to provide clear instructions with pictures showing exactly where I clicked to those family members who are having trouble with ‚ÄúThe internet being broken‚Äù It does this by automatically taking a screen shot after every mouse click or program event with a timestamp and a description of what happened. It does not record keystrokes though so if you need to record what you have typed there is some manual steps required\nSo how do you access PSR? Simple. Type ‚Äúpsr‚Äù into the run box, cmd or PowerShell and it will open\nOnce you click on Start Record it will start recording your clicks and taking screenshots. However I always open the settings by clicking on the drop down to the left of the help icon first and change the number of recent screen captures to store to the maximum value of 100.\nIf you do not you will get no warning but PSR will only save the last 25 screenshots it takes and your results will look like the below. It will still record your actions but not keep the screenshots.\nPrevious Next\nStep 16: (09/11/2014 13:47:45) User left click on ‚ÄúChris Yates (@YatesSQL) | Twitter (tab item)‚Äù\nNo screenshots were saved for this step.\nPrevious Next\nStep 17: (09/11/2014 13:47:47) User left click on ‚ÄúThe SQL Professor | ‚ÄòLeadership Through Service‚Äô (text)‚Äù\nNo screenshots were saved for this step.\nPrevious Next\nStep 18: (09/11/2014 13:47:47) User left click on ‚ÄúT-SQL Tuesday #60 ‚Äì Something New Learned | The SQL Professor (text)‚Äù in ‚ÄúT-SQL Tuesday #60 ‚Äì Something New Learned | The SQL Professor ‚Äì Google Chrome‚Äù\nYou can also set the name and location of the saved file in the settings but if you leave it blank it will prompt for a location and name once you click Stop Record\nHow do I add keyboard input? PSR allows you add keyboard input manually. You may need this if you need to include the text you have entered into prompts or address bars or if you wish to add further comment. You can do this by clicking add comment, drawing a box around the relevant part of the screen for the text input and inputting the text into the box\nIn the results this looks like\nStep 1: (09/11/2014 12:56:22) User Comment: ‚Äúhttp://www.microsoft.com/en-gb/download/details.aspx?id=42573‚Äù\nWhat do the results look like? Once you have finished the actions that you want to record (or when you think you are close to 100 screenshots) click stop record and the following screen will be displayed\nThis allows you to review what PSR has recorded. You can then save it to a location of your desire. It is saved as a zip file which has a single .mht file in it. You can open the file without unzipping the archive and it will open in Internet Explorer. As you can see from the shots below you can run PSR on your client and it will still record actions in your RDP sessions although it does not record as much detail. The first two are on my SCOM server in my lab and the second two are on the laptop using the SCOM console\nPrevious Next\nStep 11: (09/11/2014 13:02:13) User left click on ‚ÄúInput Capture Window (pane)‚Äù in ‚ÄúSCOM on ROB-LAPTOP ‚Äì Virtual Machine Connection‚Äù\nPrevious Next\nStep 12: (09/11/2014 13:02:16) User left click on ‚ÄúInput Capture Window (pane)‚Äù in ‚ÄúSCOM on ROB-LAPTOP ‚Äì Virtual Machine Connection‚Äù\nPrevious Next\nStep 13: (09/11/2014 13:06:25) User right click on ‚ÄúManagement Packs (tree item)‚Äù in ‚ÄúAgent Managed ‚Äì THEBEARDMANAGEMENTGROUP ‚Äì Operations Manager‚Äù\nPrevious Next\nStep 14: (09/11/2014 13:06:27) User left click on ‚ÄúImport Management Packs‚Ä¶ (menu item)‚Äù\nYou can then use the zip file as you wish. Maybe you email it to your third party support team (once you have edited any confidential data) or you can attach it to your incident in your IT Service Management solution or attach it to a report. If you wish to create documentation you can open the .mht file in Word, edit it as you see fit and save it appropriately.\nSo that is one of the many things that I have learnt recently and I am looking forward to seeing what others have learnt especially as many will have just been to the SQL PASS Summit. You will be able to find the other posts in this blog party in the comments on Chris‚Äôs page\n","date":"2014-11-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-60-something-new-learned-problem-step-recorder/","title":"#tsql2sday #60 ‚Äì Something New¬†Learned ‚Äì Problem Step Recorder"},{"content":"I have a lab on my laptop running various servers so that I can problem solve and learn and recently I wanted to add several months of data into a database. I had created a stored procedure to take some parameters perform some logic and insert the data.\nTo execute the stored procedure in T-SQL I simply run this\nEXECUTE [dbo].[usp_Insert_DriveSpace] 'Server1','C','2014-11-05','100','25'\rwhich uses the server name, drive letter, date, capacity and free space to add the data\nIn my wisdom I decided to create some data that was more ‚Äòreal-life‚Äô I was interested in storing drive space data and will be learning how to write reports on it. To do this I had pre-populated some tables in the database with 10 Server Names each with 5 drives so I needed 10*5*90 or 4500 statements\nI wanted to populate this with about 3 months of data as if it had been gathered every day. I read this post about using CTEs to create sequences and I am sure it can be done this way but I don‚Äôt have the T-SQL skills to do so. If someone can (or has) done that please let me know as I am trying to improve my T-SQL skills and would be interested in how to approach and solve this problem with T-SQL\nI solved it with Powershell in this way.\nCreated an array of Servers and an array of Drives to enable me to iterate though each.\n$Servers = 'Server1','Server2','Server3','Server4','Server5','Server6','Server7','Server8','Server9','Server10'\r$Drives = 'C','D','E','F','G'\rSet the drive capacity for each drive. To make my life slightly easier I standardised my ‚Äòservers‚Äô\n$CDriveCapacity = 100\r$DDriveCapacity = 50\r$EDriveCapacity = 200\r$FDriveCapacity = 200\r$GDriveCapacity = 500\rI needed to create a date. You can use Get-Date to get todays date and to get dates or times in the future or the past you can use the AddDays() function. You can also add ticks, milliseconds, seconds, minutes, hours, months or years\n(Get-Date).AddDays(1)\rI then needed to format the date. This is slightly confusing. If you just use Get-Date to get the current date (time) then you can use the format or uformat switch to format the output\nGet-Date -Format yyyyMMdd\rGet-Date -UFormat %Y%m%d\rHowever this does not work once you have used the AddDays() method. You have to use the ToString() method\n$Date = (Get-Date).AddDays(-7).ToString('yyyy-MM-dd')\rTo replicate gathering data each day I decided to use a while loop. I set $x to ‚Äì95 and pressed CTRL and J to bring up Snippets and typed w and picked the while loop. You can find out more about snippets in my previous post I started at ‚Äì95 so that all the identity keys incremented in a real-life manner oldest to newest.\n$x = -98\rwhile ($x -le 0)\r{\r$Date = (get-date).AddDays($x).ToString('yyyy-MM-dd')\rforeach($Server in $Servers)\r{\rforeach ($Drive in $Drives)\r{\rI could then use the while loop to generate data for each day and loop through each server and each drive and generate the T-SQL but I wanted more!\nI wanted to generate some random numbers for the free space available for each drive. I used the Get-Random cmdlet If you are going to use it make sure you read this post to make sure that you don‚Äôt get caught by the gotcha. I decided to set the free space for my OS,Data and Log Files to somewhere between 70 and 3 Gb free as in this imaginary scenario these drives are carefully monitored and the data and log file sizes under the control of a careful DBA but still able to go below thresholds.\nif($Drive -eq 'C')\r{\r$Free = Get-Random -Maximum 70 -Minimum 3\rI set the TempDB drive to have either 4,7 or 11 Gb free so that i can try to colour code my reports depending on values and if one field only has three values it makes it simpler to verify.\nI set the Backup Drive to somewhere between 50 and 0 so that I will hit 0 sometimes!!\nHere is the full script. It generated 4500 T-SQL statements in just under 16 seconds\n$Servers = 'Server1','Server2','Server3','Server4','Server5','Server6','Server7','Server8','Server9','Server10'\r$Drives = 'C','D','E','F','G'\r$CDriveCapacity = 100\r$DDriveCapacity = 50\r$EDriveCapacity = 200\r$FDriveCapacity = 200\r$GDriveCapacity = 500\r$x = -98\rwhile ($x -le 0)\r{\r$Date = (get-date).AddDays($x).ToString('yyyy-MM-dd')\rforeach($Server in $Servers)\r{\rforeach ($Drive in $Drives)\r{\rif($Drive -eq 'C')\r{\r$Free = Get-Random -Maximum 70 -Minimum 3\rWrite-Host \u0026amp;quot;EXECUTE \\[dbo\\].\\[usp\\_Insert\\_DriveSpace\\] '$Server','$Drive','$Date','$CDriveCapacity','$Free'\u0026amp;quot;\r}\relseif($Drive -eq 'D')\r{\r$Free = Get-Random -InputObject 4,7,11\rWrite-Host \u0026amp;quot;EXECUTE \\[dbo\\].\\[usp\\_Insert\\_DriveSpace\\] '$Server','$Drive','$Date','$DDriveCapacity','$Free'\u0026amp;quot;\r}\relseif($Drive -eq 'E')\r{\r$Free = Get-Random -Maximum 70 -Minimum 3\rWrite-Host \u0026amp;quot;EXECUTE \\[dbo\\].\\[usp\\_Insert\\_DriveSpace\\] '$Server','$Drive','$Date','$EDriveCapacity','$Free'\u0026amp;quot;\r}\relseif($Drive -eq 'F')\r{\r$Free = Get-Random -Maximum 70 -Minimum 3\rWrite-Host \u0026amp;quot;EXECUTE \\[dbo\\].\\[usp\\_Insert\\_DriveSpace\\] '$Server','$Drive','$Date','$FDriveCapacity','$Free'\u0026amp;quot;\r}\relseif($Drive -eq 'G')\r{\r$Free = Get-Random -Maximum 50 -Minimum 0\rWrite-Host \u0026amp;quot;EXECUTE \\[dbo\\].\\[usp\\_Insert\\_DriveSpace\\] '$Server','$Drive','$Date','$GDriveCapacity','$Free'\u0026amp;quot;\r}\r}\r}\r$X++\r}\rOnce it had run I simply copied the output into SSMS and was on my way\n","date":"2014-11-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/generating-t-sql-randomly-with-powershell/","title":"Generating T-SQL Randomly with Powershell"},{"content":"A DBA doesn‚Äôt want to run out of space on their servers, even in their labs! To avoid this happening I wrote a Powershell script to provide some alerts by email.\nThis is the script and how I worked my way through the solution. I hope it is of benefit to others.\nThe script works in the following way\nIterates through a list of servers Runs a WMI query to gather disk information If the free space has fallen below a threshold, checks to see if it has emailed before and if not emails a warning Resets if free space has risen above the threshold Logs what it does but manages the space the logs use As you will have seen before I use a Servers text file in my scripts. This is a text file with a single server name on each line. You could also use a query against a DBA or MDW database using Invoke-SQLCMD2, which ever is the most suitable for you.\n$Servers = Get-Content 'PATH\\\\TO\\\\Servers.txt' foreach($Server in $Servers) { The WMI query is a very simple one to gather the disk information. I format the results and place them in variables for reuse\n$Disks = Get-WmiObject win32\\_logicaldisk -ComputerName $Server | Where-Object {$\\_.drivetype -eq 3} $TotalSpace=\\[math\\]::Round(($Disk.Size/1073741824),2) # change to gb and 2 decimal places $FreeSpace=\\[Math\\]::Round(($Disk.FreeSpace/1073741824),2)\r# change to gb and 2 decimal places $UsedSpace = $TotalSpace - $FreeSpace $PercentFree = \\[Math\\]::Round((($FreeSpace/$TotalSpace)*100),2)\r# change to gb and 2 decimal places Use a bit of logic to check if the freespace is below a threshold and see if the email has already been sent\n# Check if percent free below warning level if ($PercentFree -le $SevereLevel) { # if text file has been created (ie email should already have been sent) do nothing if(Test-Path $CheckFileSevere) {} # if percent free below warning level and text file doesnot exist create text file and email else { If it has not create a unique named text file and create the email body using HTML and the values stored in the variables\nNew-Item $CheckFileSevere -ItemType File #Create Email Body $EmailBody = '' $EmailBody += \u0026quot; \u0026quot; and then send it\n$Subject = \u0026quot;URGENT Disk Space Alert 1%\u0026quot; $Body = $EmailBody $msg = new-object Net.Mail.MailMessage $smtp = new-object Net.Mail.SmtpClient($smtpServer) $smtp.port = '25' $msg.From = $From $msg.Sender = $Sender $msg.To.Add($To) $msg.Subject = $Subject $msg.Body = $Body $msg.IsBodyHtml = $True $smtp.Send($msg) If the freespace is above all of the warning levels, check for existence of the text file and delete it if found so that the next time the script runs it will send an email.\nif(Test-Path $CheckFile) { Remove-Item $CheckFile -Force\rTo enable logging create a log file each day\n$Logdate = Get-Date -Format yyyyMMdd $LogFile = $Location + 'logfile' + $LogDate+ '.txt' # if daily log file does not exist create one if(!(Test-Path $LogFile)) { New-Item $Logfile -ItemType File\r} And write the info to it at each action\n$logentrydate = (Get-Date).DateTime $Log = $logentrydate + ' ' + $ServerName + ' ' + $DriveLetter + ' ' + $VolumeName + ' ' + $PercentFree +' -- Severe Email Sent' Add-Content -Value $Log -Path $Logfile\rMaking sure that you clean up after\n# any logfiles older than 7 days delete Get-ChildItem -Path $Location \\*logfile\\* |Where-Object {$_.LastWriteTime -gt (Get-Date).AddDays(7) }|Remove-Item -Force I run the script in a Powershell Step in an SQL Agent Job every 5 minutes and now I know when my servers in my lab are running out of space with an email like this\nYou can find the script here\n","date":"2014-11-04T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/11/image_thumb.png","permalink":"https://blog.robsewell.com/blog/emailing-disk-space-alerting-with-powershell/","title":"Emailing Disk Space Alerting With Powershell"},{"content":"So you have read up on VLFs\nNo doubt you will have read this post by Kimberly Tripp and this one and maybe this one too and you want to identify the databases in your environment which have a large number of VLFs and also the initial size and the autogrowth settings of the log files.\nThere are several posts about this and doing this with PowerShell like this one or this one. As is my wont I chose to output to Excel and colour code the cells depending on the number of VLFs or the type of Autogrowth.\nThere is not a pure SMO way of identifying the number of VLFs in a log file that I am aware of and it is simple to use DBCC LOGINFO to get that info.\nI also wanted to input the autogrowth settings, size, space used, the logical name and the file path. I started by getting all of my servers into a $Servers Array as follows\n$Servers = Get-Content 'PATHTO\\sqlservers.txt'\rWhilst presenting at the Newcastle User Group, Chris Taylor b | t asked a good question. He asked if that was the only way to do this or if you could use your DBA database.\nIt is much better to make use of the system you already use to record your databases. It will also make it much easier for you to be able to run scripts against more specific groups of databases without needing to keep multiple text files up to date. You can accomplish this as follows\n$Query = 'SELECT Name FROM dbo.databases WHERE CONDITION meets your needs'\r$Servers = Invoke-Sqlcmd -ServerInstance MANAGEMENTSERVER -Database DBADATABASE -Query $query\rI then create a foreach loop and a server SMO object (Did you read my blog post about snippets? the code for a SMO Server snippet is there) returned the number of rows for DBCC LOGINFO and the information I wanted.\nforeach ($Server in $Servers)\r{\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server\rforeach ($db in $srv.Databases|Where-Object {$_.isAccessible -eq $True})\r{\r$DB.ExecuteWithResults('DBCC LOGINFO').Tables[0].Rows.Count\r$db.LogFiles | Select Growth,GrowthType,Size, UsedSpace,Name,FileName\r}\r}\rIt‚Äôs not very pretty or particularly user friendly so I decided to put it into Excel\nI did this by using my Excel Snippet\n$snippet = @{\rTitle = 'Excel Object';\rDescription = 'Creates a Excel Workbook and Sheet';\rText = @'\r# Create a .com object for Excel\r`$xl = new-object -comobject excel.application\r`$xl.Visible = `$true # Set this to False when you run in production\r`$wb = `$xl.Workbooks.Add() # Add a workbook\r`$ws = `$wb.Worksheets.Item(1) # Add a worksheet\r`$cells=`$ws.Cells\r#Do Some Stuff - perhaps -\r`$cells.item(`$row,`$col)=`'Server`'\r`$cells.item(`$row,`$col).font.size=16\r`$Cells.item(`$row,`$col).Columnwidth = 10\r`$col++\r`$wb.Saveas(`'C:\\temp\\Test`$filename.xlsx`')\r`$xl.quit()\rStop-Process -Name EXCEL\r'@\r}\rNew-IseSnippet @snippet\rand placed the relevant bits into the foreach loop\nforeach ($Server in $Servers)\r{\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server\rforeach ($db in $srv.Databases|Where-Object {$_.isAccessible -eq $True})\r{\r$VLF = $DB.ExecuteWithResults('DBCC LOGINFO').Tables[0].Rows.Count\r$logFile = $db.LogFiles | Select Growth,GrowthType,Size, UsedSpace,Name,FileName\r$Name = $DB.name\r$cells.item($row,$col)=$Server\r$col++\r$cells.item($row,$col)=$Name\r$col++\r$cells.item($row,$col)=$VLF\r$col++\r$col++\r$Type = $logFile.GrowthType.ToString()\r$cells.item($row,$col)=$Type\r$col++\r$cells.item($row,$col)=($logFile.Size)\r$col++\r$cells.item($row,$col)=($logFile.UsedSpace)\r$col++\r$cells.item($row,$col)=$logFile.Name\r$col++\r$cells.item($row,$col)=$logFile.FileName\rI had to use the ToString() method on the Type property to get Excel to display the text. I wanted to set the colour for the VLF cells to yellow or red dependant on their value and the colour of the growth type cell to red if the value was Percent. This was achieved like this\nif($VLF -gt $TooMany)\r{\r$cells.item($row,$col).Interior.ColorIndex = 6 # Yellow\r}\rif($VLF -gt $WayTooMany)\r{\r$cells.item($row,$col).Interior.ColorIndex = 3 # Red\r}\rif($Type -eq 'Percent')\r{\r$cells.item($row,$col).Interior.ColorIndex = 3 #Red\r}\rI also found this excellent post by which has many many snippets of code to work with excel sheets.\nI used\n$cells.item($row,$col).HorizontalAlignment = 3 #center\r$cells.item($row,$col).HorizontalAlignment = 4 #right\r$ws.UsedRange.EntireColumn.AutoFit()\ralthough I had to move the Title so that it was after the above line so that it looked ok.\nYou can find the script here. As always test it somewhere safe first, understand what it is doing and any questions get in touch.\n","date":"2014-10-06T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/10/image_thumb.png","permalink":"https://blog.robsewell.com/blog/number-of-vlfs-and-autogrowth-settings-colour-coded-to-excel-with-powershell/","title":"Number of VLFs and Autogrowth Settings Colour Coded to Excel with PowerShell"},{"content":"When I talk to people about Powershell they often ask how can they easily learn the syntax. Here‚Äôs a good tip\nOpen PowerShell ISE and press CTRL + J\nYou will find a number of snippets that will enable you to write your scripts easily.¬†Johnathan Medd PowerShell MVP has written a good post about snippets on the Hey, Scripting Guy! blog so I will not repeat that but suggest that you go and read that post. It will show you how quickly and easily you will be able to write more complex Powershell scripts as you do not have to learn the syntax but can use the snippets to insert all the code samples you require.\nNot only are there default snippets for you to use but you can create your own snippets. However there isn‚Äôt a snippet for creating a new snippet so here is the code to do that\n$snippet1 = @{\rTitle = 'New-Snippet'\rDescription = 'Create a New Snippet'\rText = @\u0026quot;\r`$snippet = @{\rTitle = `'Put Title Here`'\rDescription = `'Description Here`'\rText = @`\u0026quot;\rCode in Here `\u0026quot;@\r}\rNew-IseSnippet @snippet\r\u0026quot;@\r}\rNew-IseSnippet @snippet1 ‚ÄìForce\rI frequently use the SQL Server SMO Object in my code so I created this snippet\n$snippet = @{\rTitle = 'SMO-Server'\rDescription = 'Creates a SQL Server SMO Object'\rText = @\u0026quot;\r`$srv = New-Object Microsoft.SqlServer.Management.Smo.Server `$Server\r\u0026quot;@\r}\rNew-IseSnippet @snippet\rI also use Data Tables a lot so I created a snippet for that too\n$snippet = @{\rTitle = 'New-DataTable'\rDescription = 'Creates a Data Table Object'\rText = @\u0026quot;\r# Create Table Object\r`$table = New-Object system.Data.DataTable `$TableName\r# Create Columns\r`$col1 = New-Object system.Data.DataColumn NAME1,([string])\r`$col2 = New-Object system.Data.DataColumn NAME2,([decimal])\r#Add the Columns to the table\r`$table.columns.add(`$col1)\r`$table.columns.add(`$col2)\r# Create a new Row\r`$row = `$table.NewRow() # Add values to new row\r`$row.Name1 = 'VALUE'\r`$row.NAME2 = 'VALUE'\r#Add new row to table\r`$table.Rows.Add($row)\r\u0026quot;@\r}\rNew-IseSnippet @snippet\rDenniver Reining has created a Snippet Manager which you can use to further expand your snippets usage and it is free as well.\nIf you have further examples of useful snippets please feel free to post them in the comments below\nEdit 16/12/2014\nI am proud that this article was nominated for the Tribal Awards. Please go and vote for your winners in all the categories\nhttp://www.sqlservercentral.com/articles/Awards/119953/\nPersonally in the article category I will be voting for\nGail Shaw‚Äôs SQL Server Howlers\n","date":"2014-09-09T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/09/image9.png","permalink":"https://blog.robsewell.com/blog/powershell-snippets-a-great-learning-tool/","title":"PowerShell Snippets A Great Learning Tool"},{"content":"A short post today to pass on a script I wrote to fulfil a requirement I had.\nWhich indexes are on which filegroups. I found a blog post showing how to do it with T-SQL but as is my wont I decided to see how easy it would be with PowerShell. I also thought that it would make a good post to show how I approach this sort of challenge.\nI generally start by creating a SQL Server SMO Object You can use the SMO Object Model Diagram or Get-Member to work out what you need. As we are talking indexes and filegroups I will also create a Database object\n$Server = \u0026quot;SQL2012Ser2012\u0026quot;\r$DBName = \u0026quot;AdventureWorks2012\u0026quot;\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server\r$DB = $srv.Databases[$DBName]\rThen by piping the database object to Get-Member I can see the properties\nLets take a look at the table object in the same way\nI can see the indexes object so I pipe that to Get-Member as well\nNow I have enough to information to create the report. I will select the Name, Table, Type and Space Used of the Indexes and format them nicely\n$Server = \u0026quot;SQL2012Ser2012\u0026quot;\r$DBName = \u0026quot;AdventureWorks2012\u0026quot;\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $Server\r$DB = $srv.Databases[$DBName]\r$db.tables.Indexes|select Name,Parent,Filegroup,IndexType,SpaceUsed|Format-Table ‚ÄìAutoSize\rand here are the results\nHowever, you may want the results to be displayed in a different manner, maybe CSV,HTML or text file and you can do this as follows\n$db.tables.Indexes|select Name,Parent,Filegroup,IndexType,SpaceUsed|ConvertTo-Csv c:\\temp\\filegroups.csv\rInvoke-Item c:\\temp\\filegroups.csv\r$db.tables.Indexes|select Name,Parent,Filegroup,IndexType,SpaceUsed| Out-File c:\\temp\\filegroups.txt\rInvoke-Item c:\\temp\\filegroups.txt\r$db.tables.Indexes|select Name,Parent,Filegroup,IndexType,SpaceUsed|ConvertTo-Html |Out-File c:\\temp\\filegroups.html\rInvoke-Item c:\\temp\\filegroups.html\rHopefully this has shown you how easy it can be to use PowerShell to get all of the information that you need from your SQL Server and how to approach getting that information as well as several ways to display it\n","date":"2014-09-07T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/09/image8.png","permalink":"https://blog.robsewell.com/blog/find-out-which-indexes-are-on-which-filegroups-using-powershell-and-how-to-find-other-information/","title":"Find Out Which Indexes are on which Filegroups using PowerShell And How To Find Other Information"},{"content":"Following last weeks post on Refreshing A Mirrored Database with PowerShell I thought I would write the script to refresh an Availability Group Database.\nAn availability group supports a failover environment for a discrete set of user databases, known as availability databases, that fail over together. An availability group supports a set of primary databases and one to eight sets of corresponding secondary databases.You can read more about Availability groups here\nThere are situations where you may need to refresh these databases. Disaster Recovery is an obvious one but also during development to provide testing or development environments to test your High Availability implementations, run through disaster scenarios, create run books or ensure that the code changes still work with AG. There are other scenarios but this post covers the automation of restoring an Availability Group Database from a backup.\nThe steps that you need to take to restore an Availability Group Database are\nRemove Database from the Availability Group Restore the Primary Replica Database Backup the Primary Replica Database Transaction Log Restore the Secondary and Tertiary Replica Databases with no recovery Add the Database back into the Availability Group Resolve Orphaned Users ‚Äì Not covered in this script Check the status Here is my set up for this post\nI have 3 servers SQL2012SER08AG1, SQL2012SER08AG2 and SQL2012SER08AG3 with 3 databases in an Availability Group called AG_THEBEARD1. SQL2012SER08AG2 is set up as a secondary replica using Synchronous-Commit Mode SQL2012SER08AG3 is set up as a read only replica using Asynchronous-Commit Mode. I have three databases in my Availability Group and today I shall use the database called TestDatabase (I have no imagination today!) to demonstrate the refresh\nThe script requires some variables to be set up at the beginning. You can easily change this and make the script into a function and call it if you desire, but for this post I shall consider the script as a standalone. The reasoning for this is that I imagine that it will be placed into a run book or stored for use in a repository for specific use and therefore reduces any pre-requisites for using it.\nFirst we will remove the database from the Availability Group. This is achieved using the Remove-SqlAvailabilityDatabase CMDLet\nRemove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$SecondaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName\rRemove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$TertiaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName Remove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$PrimaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName\rNext Restore the Primary Replica Database, Backup the Primary Replica Database Transaction Log\nand Restore the Secondary and Tertiary Replica Databases with no recovery using Restore-SqlDatabase and Backup-SqlDatabase (You can also use the SMO method in the previous post if you wish)\nRestore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $PrimaryServer -ReplaceDatabase\r# Backup Primary Database\rBackup-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $PrimaryServer -BackupAction 'Log'\r# Remove connections to database for Restore\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $SecondaryServer\r$srv.KillAllProcesses($dbname)\r# Restore Secondary Replica Database Restore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $SecondaryServer -NoRecovery -ReplaceDatabase Restore-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $SecondaryServer -RestoreAction 'Log' -NoRecovery -ReplaceDatabase\r# Remove connections to database for Restore\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $TertiaryServer\r$srv.KillAllProcesses($dbname)\r# Restore Tertiary Replica Database Restore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $TertiaryServer -NoRecovery -ReplaceDatabase\rRestore-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $TertiaryServer -RestoreAction 'Log' -NoRecovery -ReplaceDatabase\rThen add the database back to the Availability Group\nAdd-SqlAvailabilityDatabase -Path $MyAgPrimaryPath -Database $DBName Add-SqlAvailabilityDatabase -Path $MyAgSecondaryPath -Database $DBName Add-SqlAvailabilityDatabase -Path $MyAgTertiaryPath -Database $DBName Finally test the status of the Availability Group\n$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $PrimaryServer\r$AG = $srv.AvailabilityGroups[$AGName]\r$AG.DatabaseReplicaStates|ft -AutoSize\rI also like to add some output to show the progress of the script. This can be logged using Out-File or displayed on the screen using Out-Host.\n$EndDate = Get-Date\r$Time = $EndDate - $StartDate\rWrite-Host \u0026quot;\r##########################################\rResults of Script to refresh $DBName on\r$PrimaryServer , $SecondaryServer , $TertiaryServer\ron AG $AGName\rTime Script anded at $EndDate and took\r$Time\r\u0026quot; -ForegroundColor Green\rHere are the results of my script\nHere is the script\n\u0026lt;#\r.NOTES Name: Availability Group Refresh\rAuthor: Rob Sewell https://blog.robsewell.com\r.DESCRIPTION Refreshes an Availbaility group database from a backup\rYOU WILL NEED TO RESOLVE ORPHANED USERS IF REQUIRED\r#\u0026gt; ## http://msdn.microsoft.com/en-gb/library/hh213078.aspx#PowerShellProcedure?WT.mc_id=DP-MVP-5002693\r# http://msdn.microsoft.com/en-us/library/hh213326(v=sql.110).aspx?WT.mc_id=DP-MVP-5002693\rcls\r# To Load SQL Server Management Objects into PowerShell\r[System.Reflection.Assembly]::LoadWithPartialName(‚ÄòMicrosoft.SqlServer.SMO‚Äô) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(‚ÄòMicrosoft.SqlServer.SMOExtended‚Äô) | out-null\r$LoadServer = \u0026quot;SQL2012Ser2012\u0026quot; # The Load Server $Date = Get-Date -Format ddMMyy\r$PrimaryServer = \u0026quot;SQL2012SER08AG1\u0026quot; # The Primary Availability Group Server\r$SecondaryServer = \u0026quot;SQL2012SER08AG2\u0026quot; # The Secondary Availability Group Server\r$TertiaryServer = \u0026quot;SQL2012SER08AG3\u0026quot; # The Tertiary Availability Group Server\r$AGName = \u0026quot;AG_THEBEARD1\u0026quot; # Availability Group Name\r$DBName = \u0026quot;TestDatabase\u0026quot; # Database Name\r$LoadDatabaseBackupFile = \u0026quot;\\\\sql2012ser2012\\Backups\\GoldenBackup\\LoadTestDatabase\u0026quot; + $Date + \u0026quot;.bak\u0026quot; # Load database Backup location - Needs access permissions granted\r$DatabaseBackupFile = \u0026quot;\\\\sql2012ser2012\\Backups\\GoldenBackup\\TestDatabase\u0026quot; + $Date + \u0026quot;.bak\u0026quot; # database Backup location - Needs access permissions granted\r$LogBackupFile = \u0026quot;\\\\sql2012ser2012\\Backups\\GoldenBackup\\TestDatabase\u0026quot; + $Date + \u0026quot;.trn\u0026quot; # database Backup location - Needs access permissions granted\r# Path to Availability Database Objects\r$MyAgPrimaryPath = \u0026quot;SQLSERVER:\\SQL\\$PrimaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\u0026quot;\r$MyAgSecondaryPath = \u0026quot;SQLSERVER:\\SQL\\$SecondaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\u0026quot;\r$MyAgTertiaryPath = \u0026quot;SQLSERVER:\\SQL\\$TertiaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\u0026quot;\r$StartDate = Get-Date\rWrite-Host \u0026quot;\r##########################################\rResults of Script to refresh $DBName on\r$PrimaryServer , $SecondaryServer , $TertiaryServer\ron AG $AGName\rTime Script Started $StartDate\r\u0026quot; -ForegroundColor Green\rcd c:\r# Remove old backups\rIf(Test-Path $LoadDatabaseBackupFile){Remove-Item -Path $LoadDatabaseBackupFile -Force}\rIf(Test-Path $DatabaseBackupFile){Remove-Item -Path $DatabaseBackupFile}\rIf(Test-Path $LogBackupFile ) {Remove-Item -Path $LogBackupFile }\rWrite-Host \u0026quot;Backup Files removed\u0026quot; -ForegroundColor Green\r# Remove Secondary Replica Database from Availability Group to enable restore\rcd SQLSERVER:\\SQL\\$SecondaryServer\\DEFAULT\rRemove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$SecondaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName Write-Host \u0026quot;Secondary Removed from Availability Group\u0026quot; -ForegroundColor Green\r# Remove Tertiary Replica Database from Availability Group to enable restore\rcd SQLSERVER:\\SQL\\$TertiaryServer\\DEFAULT\rRemove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$TertiaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName\rWrite-Host \u0026quot;Tertiary removed from Availability Group\u0026quot; -ForegroundColor Green\r# Remove Primary Replica Database from Availability Group to enable restore\rcd SQLSERVER:\\SQL\\$PrimaryServer\\DEFAULT\rRemove-SqlAvailabilityDatabase -Path SQLSERVER:\\SQL\\$PrimaryServer\\DEFAULT\\AvailabilityGroups\\$AGName\\AvailabilityDatabases\\$DBName\rWrite-Host \u0026quot;Primary removed from Availability Group\u0026quot; -ForegroundColor Green\r# Backup Load Database\rBackup-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $LoadServer\rWrite-Host \u0026quot;Load Database Backed up\u0026quot; -ForegroundColor Green\r# Remove connections to database for Restore\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $PrimaryServer\r$srv.KillAllProcesses($dbname)\r# Restore Primary Replica Database from Load Database\rRestore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $PrimaryServer -ReplaceDatabase\rWrite-Host \u0026quot;Primary Database Restored\u0026quot; -ForegroundColor Green\r# Backup Primary Database\r# Backup-SqlDatabase -Database $DBName -BackupFile $DatabaseBackupFile -ServerInstance $PrimaryServer\rBackup-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $PrimaryServer -BackupAction 'Log'\rWrite-Host \u0026quot;Primary Database Backed Up\u0026quot; -ForegroundColor Green\r# Remove connections to database for Restore\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $SecondaryServer\r$srv.KillAllProcesses($dbname)\r# Restore Secondary Replica Database Restore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $SecondaryServer -NoRecovery -ReplaceDatabase Restore-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $SecondaryServer -RestoreAction 'Log' -NoRecovery -ReplaceDatabase\rWrite-Host \u0026quot;Secondary Database Restored\u0026quot; -ForegroundColor Green\r# Remove connections to database for Restore\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $TertiaryServer\r$srv.KillAllProcesses($dbname)\r# Restore Tertiary Replica Database Restore-SqlDatabase -Database $DBName -BackupFile $LoadDatabaseBackupFile -ServerInstance $TertiaryServer -NoRecovery -ReplaceDatabase\rRestore-SqlDatabase -Database $DBName -BackupFile $LogBackupFile -ServerInstance $TertiaryServer -RestoreAction 'Log' -NoRecovery -ReplaceDatabase\rWrite-Host \u0026quot;Tertiary Database Restored\u0026quot; -ForegroundColor Green\r# Add database back into Availability Group\rcd SQLSERVER:\\SQL\\$PrimaryServer\rAdd-SqlAvailabilityDatabase -Path $MyAgPrimaryPath -Database $DBName Add-SqlAvailabilityDatabase -Path $MyAgSecondaryPath -Database $DBName Add-SqlAvailabilityDatabase -Path $MyAgTertiaryPath -Database $DBName Write-Host \u0026quot;Database Added to Availability Group \u0026quot; -ForegroundColor Green\r# Check Availability Group Status\r$srv = New-Object Microsoft.SqlServer.Management.Smo.Server $PrimaryServer\r$AG = $srv.AvailabilityGroups[$AGName]\r$AG.DatabaseReplicaStates|ft -AutoSize\r$EndDate = Get-Date\r$Time = $EndDate - $StartDate\rWrite-Host \u0026quot;\r##########################################\rResults of Script to refresh $DBName on\r$PrimaryServer , $SecondaryServer , $TertiaryServer\ron AG $AGName\rTime Script ended at $EndDate and took\r$Time\r\u0026quot; -ForegroundColor Green\r","date":"2014-09-04T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/09/image1.png","permalink":"https://blog.robsewell.com/blog/refreshing-availability-group-database-with-powershell/","title":"Refreshing Availability Group Database with PowerShell"},{"content":"SQL mirroring is a means of providing high availability for your SQL database. It is available in Standard Edition and although the feature is deprecated it is still widely utilised. You can read more about it on MSDN here and Jes Borland wrote a useful post answering many questions here\nThere are situations where you may need to refresh these databases. Disaster Recovery is an obvious one but also during development to provide testing or development environments to test your High Availability implementations, run through disaster scenarios, create run books or ensure that the code changes still work with mirroring. There are other scenarios but this post covers the automation of restoring a mirrored database from a backup.\nI have mentioned before and no doubt I shall again, John Sansom wrote a great post about automation and I am a strong follower of that principle.\nTo refresh a SQL mirror the following steps are required, there are some gotchas that you need to be aware of which I will discuss later\nremove mirroring restore principle database from backup perform a transaction log backup of the principle database restore both backups on the mirror server with no recovery recreate mirroring resolve orphaned users check mirroring status Regular blog followers will know that I prefer to use Powershell when I can (and where it is relevant to do so) and so I have used Powershell to automate all of the steps above\nThe script requires some variables to be set up at the beginning. You can easily change this and make the script into a function and call it if you desire, but for this post I shall consider the script as a standalone. The reasoning for this is that I imagine that it will be placed into a run book or stored for use in a repository for specific use and therefore reduces any pre-requisites for using it.\nSet variables as follows, the last three variables set the types for the backup action type and device type and do not need to be altered.\n\\# Set up some variables\r$PrincipalServer = '' # Enter Principal Server Name\r$MirrorServer = '' # Enter Mirror Server Name\r$DBName = '' # Enter Database Name\r$FileShare = '' # Enter FileShare with trailing slash\r$LocationReplace = $FileShare + $DBName + 'Refresh.bak'\r$LocationTran = $FileShare + $DBName + 'formirroring.trn'\r$PrincipalEndPoint = 'TCP://SERVERNAME:5022' # Change as required\r$MirrorEndpoint = 'TCP://SERVERNAME:5022' # Change as required\r$WitnessEndpoint = 'TCP://SERVERNAME:5022' # Change as required\r$Full = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Database\r$Tran = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Log\r$File = [Microsoft.SqlServer.Management.Smo.DeviceType]::File After some error checking the first thing is to create server and database SMO objects\n\\# Create Server objects $Principal = New-Object Microsoft.SQLServer.Management.SMO.Server $PrincipalServer $Mirror = New-Object Microsoft.SQLServer.Management.Smo. server $MirrorServer\r#Create Database Objects\r$DatabaseMirror = $Mirror.Databases[$DBName]\r$DatabasePrincipal = $Principal.Databases[$DBName] (Added Extra ‚Äì Use New-ISESnippet to create a SMO Server Snippet and use CTRL + J to find it\nNew-IseSnippet -Title SMO-Server -Description \u0026quot;Create A SQL Server SMO Object\u0026quot; -Text \u0026quot;`$srv = New-Object Microsoft.SqlServer.Management.Smo.Server `$server\u0026quot;\rRemove Mirroring Before we can restore the database we need to remove mirroring\n$DatabasePrincipal.ChangeMirroringState([Microsoft.SqlServer.Management.Smo.MirroringOption]::Off)\rrestore principle database from backup Once mirroring has been removed we can restore the database. Stuart Moore‚Äôs Great Series provides all the code you need to backup and restore databases with Powershell. There is however a bug which can catch you out. Here‚Äôs the code\n$restore = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Restore|Out-Null\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationReplace,$File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.Devices.add($restoredevice)\r#Perform Restore\r$restore.sqlrestore($PrincipalServer)\r$restore.Devices.Remove($restoredevice)\rThe bug is as follows, if your restore is going to take longer than 10 minutes and you are using an earlier version of SQL than SQL 2012 SP1 CU8 then you will find that the restore fails after 10 minutes. This is the default timeout. You may try to set the\n$srv.ConnectionContext.StatementTimeout\rValue to a larger value or 0 and this will work after SQL 2012 SP1 CU8 but prior to that you will still face the same error. The simple workaround is to use Invoke-SQLCmd2 and to script the restore as follows\n#Set up Restore using refresh backup\r$restore = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Restore\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationReplace,$File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.Devices.add($restoredevice)\r#Perform Restore\r$restore.sqlrestore($PrincipalServer) # if query time \u0026amp;amp;lt; 600 seconds\r# $query = $restore.Script($PrincipalServer) # if using Invoke-SQLCMD2\r$restore.Devices.Remove($restoredevice)\rperform a transaction backup of the principle database We need to have a full and transaction log backup to set up mirroring. Again you may need to use the script method if your backup will take longer than 600 seconds.\n#Setup Trans Backup\r$Backup = New-Object Microsoft.SqlServer.Management.Smo.Backup|Out-Null\r$Full = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Database\r$Tran = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Log\r$File = [Microsoft.SqlServer.Management.Smo.DeviceType]::File\r$Backup.Action = $Tran\r$Backup.BackupSetDescription = ‚ÄúLog Backup of ‚Äú + $DBName\r$Backup.Database = $DBName\r$BackupDevice = New-Object ‚ÄìTypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationTran,$File)|Out-Null\r$Backup.Devices.Add($BackupDevice)\r# Perform Backup\r$Backup.SqlBackup($PrincipalServer)\r# $query = $Backup.Script($PrincipalServer) # if query time \u0026amp;amp;lt; 600 seconds\r$Backup.Devices.Remove($BackupDevice)\r# Invoke-Sqlcmd2 ‚ÄìServerInstance $PrincipalServer ‚ÄìDatabase master ‚ÄìQuery $query ‚ÄìConnectionTimeout 0 # comment out if not used\rRestore both backups on the mirror server with no recovery To complete the mirroring set up we need to restore the backups onto the mirror server with no recovery as follows\n#Set up Restore of Full Backup on Mirror Server\r$restore = New-Object -TypeName Microsoft.SqlServe r.Management.Smo.Restore|Out-Null\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationReplace,$File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.NoRecovery = $true\r$restore.Devices.add($restoredevice)\r$restore.sqlrestore($MirrorServer) # if query time \u0026amp;amp;lt; 600 seconds\r# $query = $restore.Script($MirrorServer) # if using Invoke-SQLCMD2\r$restore.Devices.Remove($restoredevice)\r# Invoke-Sqlcmd2 -ServerInstance $MirrorServer -Database master -Query $query -ConnectionTimeout 0 # comment out if not used\r# Set up Restore of Log Backup on Mirror Server\r$restore = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Restore|Out-Null\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationTran,$File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.NoRecovery = $true\r$restore.Devices.add($restoredevice)\r$restore.sqlrestore($MirrorServer)\r$restore.Devices.Remove($restoredevice)\rRecreate mirroring You recreate mirroring in the same way as you would if you were using T-SQL simply add the principal endpoint to the mirror, and the mirror and witness endpoints to the principal\n#Recreate Mirroring\r$DatabaseMirror.MirroringPartner = $PrincipalEndPoint\r$DatabaseMirror.Alter()\r$DatabasePrincipal.MirroringPartner = $MirrorEndpoint\r$DatabasePrincipal.MirroringWitness = $WitnessEndpoint\r$DatabasePrincipal.Alter()\rResolve orphaned users You will need to resolve any users and permissions on your destination servers. I do not know a way to do this with PowerShell and would be interested if anyone has found a way to replace the password or the SID on a user object, please contact me if you know.\nMany people do this with the sp_rev_logins stored procedure which will create the T-SQL for recreating the logins. However, Powershell cannot read the outputs of the message window where the script prints the script. If you know that your logins are staying static then run sp_rev_logins and store the output in a sql file and call it with Invoke-SQLCmd2\n$SQL = ‚Äò‚Äô #Path to File\rInvoke-Sqlcmd2 ‚ÄìServerInstance $Server ‚ÄìDatabase master ‚ÄìInputFile $SQL\rThe other option is to set up a SSIS package following this blog post and call it from Powershell as follows\n**2020 Edit ** - You should use dbatools to do this\nInvoke-Command ‚ÄìComputerName $Server ‚Äìscriptblock {DTExec.exe /File ‚ÄúPATHTOPackage.dtsx‚Äù}\rThis requires Powershell Remoting to have been set up on the server which may or may not be available to you in your environment.\nIMPORTANT NOTE ‚Äì The script does not include any methods for resolving orphaned users so you will need to test and then add your own solution to the script.\ncheck mirroring status Lastly you want to check that the script has run successfully and that mirroring is synchronised (I am from the UK!!) To do this I check that time and file used for the last database backup using this script\n#Check that correct file and backup date used\r$query = \u0026quot;SELECT TOP 1 [rs].[destination_database_name] as 'database',\r[rs].[restore_date] as 'restoredate',\r[bs].[backup_finish_date] as 'backuptime',\r[bmf].[physical_device_name] as 'Filename'\rFROM msdb..restorehistory rs\rINNER JOIN msdb..backupset bs\rON [rs].[backup_set_id] = [bs].[backup_set_id]\rINNER JOIN msdb..backupmediafamily bmf\rON [bs].[media_set_id] = [bmf].[media_set_id]\rORDER BY [rs].[restore_date] DESC\u0026quot;\rInvoke-Sqlcmd2 -ServerInstance $PrincipalServer -Database msdb -Query $query |Format-Table -AutoSize ‚ÄìWrap\rand that mirroring has synchronised using the following Powershell command\n$DatabasePrincipal | select Name, MirroringStatus, IsAccessible |Format-Table -AutoSize\rDepending on your needs you may add some error checking using the results of the above scripts. As I said at the top of the post, you can turn this script into a function and call it at will or add it to an Agent Job for regular scheduling or just kept in a folder ready to be run when required. The choice is yours but all usual rules apply. Don‚Äôt believe anything you read on this blog post, don‚Äôt run any scripts on production, test before running any scripts, understand what the code is doing before you run it or I am not responsible if you break anything\nHere is the script\n\u0026lt;# .NOTES Name: Refresh Mirrored Database\rAuthor: Rob Sewell https://blog.robsewell.com\rRequires: Invoke-SQLCMD2 (included)\rVersion History: 1.2 22/08/2014 .SYNOPSIS Refreshes a mirrored database\r.DESCRIPTION This script will refresh a mirrored database, recreate mirroring and chekc status of mirroring. Further details on the website\rRequires the variables at the top of the script to be filled in\rIMPORTANT - Orpahaned users are not resolved with this acript without additions. See blog post for options\r#\u0026gt; # Load Invoke-SQLCMD2\r#Load the assemblies the script requires\r[void][reflection.assembly]::LoadWithPartialName( \u0026quot;Microsoft.SqlServer.Management.Common\u0026quot; );\r[void][reflection.assembly]::LoadWithPartialName( \u0026quot;Microsoft.SqlServer.SmoEnum\u0026quot; );\r[void][reflection.assembly]::LoadWithPartialName( \u0026quot;Microsoft.SqlServer.Smo\u0026quot; );\r[void][reflection.assembly]::LoadWithPartialName( \u0026quot;Microsoft.SqlServer.SmoExtended \u0026quot; );\r[void][System.Reflection.Assembly]::LoadWithPartialName(\u0026quot;Microsoft.SqlServer.ConnectionInfo\u0026quot;) [System.Reflection.Assembly]::LoadWithPartialName(\u0026quot;System.Windows.Forms\u0026quot;)|Out-Null\r# Set up some variables\r$PrincipalServer = '' # Enter Principal Server Name\r$MirrorServer = '' # Enter Mirror Server Name\r$DBName = '' # Enter Database Name\r$FileShare = '' # Enter FileShare with trailing slash\r$LocationReplace = $FileShare + $DBName + 'Refresh.bak'\r$LocationFUll = $FileShare + $DBName + 'formirroring.bak'\r$LocationTran = $FileShare + $DBName + 'formirroring.trn'\r$PrincipalEndPoint = 'TCP://SERVERNAME:5022' # Change as required\r$MirrorEndpoint = 'TCP://SERVERNAME:5022' # Change as required\r$WitnessEndpoint = 'TCP://SERVERNAME:5022' # Change as required\r$Full = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Database\r$Tran = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Log\r$File = [Microsoft.SqlServer.Management.Smo.DeviceType]::File\r###################### \u0026lt;# .SYNOPSIS Runs a T-SQL script. .DESCRIPTION Runs a T-SQL script. Invoke-Sqlcmd2 only returns message output, such as the output of PRINT statements when -verbose parameter is specified .INPUTS None You cannot pipe objects to Invoke-Sqlcmd2 .OUTPUTS System.Data.DataTable .EXAMPLE Invoke-Sqlcmd2 -ServerInstance \u0026quot;MyComputer\\MyInstance\u0026quot; -Query \u0026quot;SELECT login_time AS 'StartTime' FROM sysprocesses WHERE spid = 1\u0026quot; This example connects to a named instance of the Database Engine on a computer and runs a basic T-SQL query. StartTime ----------- 2010-08-12 21:21:03.593 .EXAMPLE Invoke-Sqlcmd2 -ServerInstance \u0026quot;MyComputer\\MyInstance\u0026quot; -InputFile \u0026quot;C:\\MyFolder\\tsqlscript.sql\u0026quot; | Out-File -filePath \u0026quot;C:\\MyFolder\\tsqlscript.rpt\u0026quot; This example reads a file containing T-SQL statements, runs the file, and writes the output to another file. .EXAMPLE Invoke-Sqlcmd2 -ServerInstance \u0026quot;MyComputer\\MyInstance\u0026quot; -Query \u0026quot;PRINT 'hello world'\u0026quot; -Verbose This example uses the PowerShell -Verbose parameter to return the message output of the PRINT command. VERBOSE: hello world .NOTES Version History v1.0 - Chad Miller - Initial release v1.1 - Chad Miller - Fixed Issue with connection closing v1.2 - Chad Miller - Added inputfile, SQL auth support, connectiontimeout and output message handling. Updated help documentation v1.3 - Chad Miller - Added As parameter to control DataSet, DataTable or array of DataRow Output type #\u0026gt; function Invoke-Sqlcmd2 { [CmdletBinding()] param( [Parameter(Position = 0, Mandatory = $true)] [string]$ServerInstance, [Parameter(Position = 1, Mandatory = $false)] [string]$Database, [Parameter(Position = 2, Mandatory = $false)] [string]$Query, [Parameter(Position = 3, Mandatory = $false)] [string]$Username, [Parameter(Position = 4, Mandatory = $false)] [string]$Password, [Parameter(Position = 5, Mandatory = $false)] [Int32]$QueryTimeout = 600, [Parameter(Position = 6, Mandatory = $false)] [Int32]$ConnectionTimeout = 15, [Parameter(Position = 7, Mandatory = $false)] [ValidateScript( {test-path $_})] [string]$InputFile, [Parameter(Position = 8, Mandatory = $false)] [ValidateSet(\u0026quot;DataSet\u0026quot;, \u0026quot;DataTable\u0026quot;, \u0026quot;DataRow\u0026quot;)] [string]$As = \u0026quot;DataRow\u0026quot; ) if ($InputFile) { $filePath = $(resolve-path $InputFile).path $Query = [System.IO.File]::ReadAllText(\u0026quot;$filePath\u0026quot;) } $conn = new-object System.Data.SqlClient.SQLConnection if ($Username) { $ConnectionString = \u0026quot;Server={0};Database={1};User ID={2};Password={3};Trusted_Connection=False;Connect Timeout={4}\u0026quot; -f $ServerInstance, $Database, $Username, $Password, $ConnectionTimeout } else { $ConnectionString = \u0026quot;Server={0};Database={1};Integrated Security=True;Connect Timeout={2}\u0026quot; -f $ServerInstance, $Database, $ConnectionTimeout } \u0026amp;amp;n bsp; $conn.ConnectionString = $ConnectionString #Following EventHandler is used for PRINT and RAISERROR T-SQL statements. Executed when -Verbose parameter specified by caller if ($PSBoundParameters.Verbose) { $conn.FireInfoMessageEventOnUserErrors = $true $handler = [System.Data.SqlClient.SqlInfoMessageEventHandler] {Write-Verbose \u0026quot;$($_)\u0026quot;} $conn.add_InfoMessage($handler) } $conn.Open() $cmd = new-object system.Data.SqlClient.SqlCommand($Query, $conn) $cmd.CommandTimeout = $QueryTimeout $ds = New-Object system.Data.DataSet $da = New-Object system.Data.SqlClient.SqlDataAdapter($cmd) [void]$da.fill($ds) $conn.Close() switch ($As) { 'DataSet' { Write-Output ($ds) } 'DataTable' { Write-Output ($ds.Tables) } 'DataRow' { Write-Output ($ds.Tables[0]) } } } #Invoke-Sqlcmd2\r# Check for existence of Backup file with correct name\rIf (!(Test-Path $LocationReplace)) {\rWrite-Output \u0026quot; There is no file called \u0026quot; Write-Output $LocationReplace\rWrite-Output \u0026quot;Please correct and re-run\u0026quot;\rbreak\r}\r# Remove Old Backups\rif (Test-Path $locationFull) {\rRemove-Item $LocationFUll -Force\r}\rif (Test-Path $locationTran) {\rRemove-Item $LocationTran -Force\r}\r# Create Server objects\r$Principal = New-Object Microsoft.SQLServer.Management.SMO.Server $PrincipalServer\r$Mirror = New-Object Microsoft.SQLServer.Management.Smo.server $MirrorServer\r#Create Database Objects\r$DatabaseMirror = $Mirror.Databases[$DBName]\r$DatabasePrincipal = $Principal.Databases[$DBName]\r# If database is on Mirror server fail it over to Principal\rif ($DatabasePrincipal.IsAccessible -eq $False) {\r$DatabaseMirror.ChangeMirroringState([Microsoft.SqlServer.Management.Smo.MirroringOption]::Failover) }\r# remove mirroring\r$DatabasePrincipal.ChangeMirroringState([Microsoft.SqlServer.Management.Smo.MirroringOption]::Off)\r#Set up Restore using refresh backup\r$restore = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Restore\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationReplace, $File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.Devices.add($restoredevice)\r#Perform Restore\r$restore.sqlrestore($PrincipalServer) # if query time\u0026lt; 600 seconds\r# $query = $restore.Script($PrincipalServer) # if using Invoke-SQLCMD2\r$restore.Devices.Remove($restoredevice)\r# Invoke-Sqlcmd2 -ServerInstance $PrincipalServer -Database master -Query $query -ConnectionTimeout 0 # comment out if not used\r# Set up Full Backup\r$Backup = New-Object Microsoft.SqlServer.Management.Smo.Backup\r$Backup.Action = $Full\r$Backup.BackupSetDescription = \u0026quot;Full Backup of \u0026quot; + $DBName\r$Backup.Database = $DatabasePrincipal.Name\r$BackupDevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationFull, $File)\r$Backup.Devices.Add($BackupDevice)\r# Perform Backup\r$Backup.SqlBackup($PrincipalServer)\r# $query = $Backup.Script($PrincipalServer) # if query time\u0026lt; 600 seconds\r$Backup.Devices.Remove($BackupDevice)\r# Invoke-Sqlcmd2 -ServerInstance $PrincipalServer -Database master -Query $query -ConnectionTimeout 0 # comment out if not used\r#Setup Trans Backup\r$Backup = New-Object Microsoft.SqlServer.Management.Smo.Backup|Out-Null\r$Full = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Database\r$Tran = [Microsoft.SQLServer.Management.SMO.BackupActionType]::Log\r$File = [Microsoft.SqlServer.Management.Smo.DeviceType]::File\r$Backup.Action = $Tran\r$Backup.BackupSetDescription = \u0026quot;Log Backup of \u0026quot; + $DBName\r$Backup.Database = $DBName\r$BackupDevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationTran, $File)|Out-Null\r$Backup.Devices.Add($BackupDevice)\r# Perform Backup\r$Backup.SqlBackup($PrincipalServer)\r# $query = $Backup.Script($PrincipalServer) # if query time\u0026lt; 600 seconds\r$Backup.Devices.Remove($BackupDevice)\r# Invoke-Sqlcmd2 -ServerInstance $PrincipalServer -Database master -Query $query -ConnectionTimeout 0 # comment out if not used\r#Set up Restore of Full Backup on Mirror Server\r$restore = New-Object -TypeName Microsoft.SqlServe r.Management.Smo.Restore|Out-Null\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationFUll, $File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.NoRecovery = $true\r$restore.Devices.add($restoredevice)\r$restore.sqlrestore($MirrorServer) # if query time\u0026lt; 600 seconds\r# $query = $restore.Script($MirrorServer) # if using Invoke-SQLCMD2\r$restore.Devices.Remove($restoredevice)\r# Invoke-Sqlcmd2 -ServerInstance $MirrorServer -Database master -Query $query -ConnectionTimeout 0 # comment out if not used\r# Set up Restore of Log Backup on Mirror Server\r$restore = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Restore|Out-Null\r$restoredevice = New-Object -TypeName Microsoft.SQLServer.Management.Smo.BackupDeviceItem($LocationTran, $File)|Out-Null\r$restore.Database = $DBName\r$restore.ReplaceDatabase = $True\r$restore.NoRecovery = $true\r$restore.Devices.add($restoredevice)\r$restore.sqlrestore($MirrorServer)\r$restore.Devices.Remove($restoredevice)\r#Recreate Mirroring\r$DatabaseMirror.MirroringPartner = $PrincipalEndPoint\r$DatabaseMirror.Alter()\r$DatabasePrincipal.MirroringPartner = $MirrorEndpoint\r$DatabasePrincipal.MirroringWitness = $WitnessEndpoint\r$DatabasePrincipal.Alter()\r# Resolve Orphaned Users if needed\r#Check that correct file and backup date used\r$query = \u0026quot;SELECT TOP 20 [rs].[destination_database_name] as 'database', [rs].[restore_date] as 'restoredate', [bs].[backup_finish_date] as 'backuptime', [bmf].[physical_device_name] as 'Filename'\rFROM msdb..restorehistory rs\rINNER JOIN msdb..backupset bs\rON [rs].[backup_set_id] = [bs].[backup_set_id]\rINNER JOIN msdb..backupmediafamily bmf ON [bs].[media_set_id] = [bmf].[media_set_id] ORDER BY [rs].[restore_date] DESC\u0026quot;\rInvoke-Sqlcmd2 -ServerInstance $PrincipalServer -Database msdb -Query $query |Format-Table -AutoSize -Wrap\r$DatabasePrincipal | select Name, MirroringStatus, IsAccessible |Format-Table -AutoSize\r","date":"2014-08-25T00:00:00Z","permalink":"https://blog.robsewell.com/blog/refreshing-a-sql-mirrored-database-using-powershell/","title":"Refreshing A SQL Mirrored Database Using Powershell"},{"content":"I had an email last night from someone who attended my PowerShell Box of Tricks session at SQL Saturday Exeter\nHe was getting an error whilst trying to set CLR Enabled during an automatic install and asked if I had any ideas. The error he had was related to Invoke-SQLcmd and the method he was calling the PowerShell script\nI was unable to replicate his problem on my servers so I looked at other methods that may assist as well as following up with him to try and understand what was causing his issue. In doing so I worked out the following method to change the CLR Enabled setting by SMO and thought it worth a blog post to share\nOne way around his issue is to define and then call Invoke-SQLCmd2 by Chad Miller within his script. So his script would look in part as follows\nHowever, I prefer to use SMO so I examined the Server SMO as follows notice the ‚Äú.‚Äù for local server\n$srv = New-Object Microsoft.SQLServer.Management.SMO.Server .\r$srv |gm\rAnd noticed the Configuration property\n$srv.Configuration |Get-Member\rEnabled me to see the IsCLREnabled Property and using Get-Member I could see that the config value was settable\nWith this information I could write a simple script to alter the settings.\nPrior to running the script\nWe then run the following script\nLine 1 creates a Server SMO object there is a ‚Äú.‚Äù to denote local server at the end of the line although you can use the server name as well\nLine 4 sets the configvalue for the IsCLREnabled property\nAnd Line 5 Alters the Config object, essentially running the reconfigure\nAfter running the script\nHopefully this short post shows how easy it is to set SQL Server configuration values with Powershell using SMO\nAny questions or comments please feel free to ask\n","date":"2014-05-05T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/05/050514_0904_enableclrwi2.png","permalink":"https://blog.robsewell.com/blog/enable-clr-with-powershell/","title":"Enable CLR with Powershell"},{"content":"A very short blog today just to pass on this little script.\nI was required to list all of the SysAdmins across a large estate. Obviously I turned to PowerShell üôÇ\nI iterated through my server list collection and then created a server SMO object and used the EnumServerRoleMembers method to display all of the sysadmin members\nThis will work on SQL2000 ‚Äì SQL2012. You can see how you can easily change the rolename in the script to enumerate other server roles.\nAnother way you could do it is to use the query\nSELECT c.name AS Sysadmin_Server_Role_Members\rFROM sys.server_principals a\rINNER JOIN sys.server_role_members b\rON a.principal_id = b.role_principal_id AND a.type = 'R' AND a.name ='sysadmin'\rINNER JOIN sys.server_principals c\rON b.member_principal_id = c.principal_id\rand pass that with Invoke-SQLCMD through to every server (if you had to use Powershell üôÇ ). That query won‚Äôt work with SQL 2000 though\n","date":"2014-04-14T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/04/2014-04-12_152433.jpg","permalink":"https://blog.robsewell.com/blog/listing-the-sql-server-sysadmins-with-powershell/","title":"Listing the SQL Server SysAdmins With PowerShell"},{"content":"Editors Note This is still all valid but nowadays you would be much better off using dbatools to gather the information and the ImportExcel module to add it to an Excel sheet :-)\nOriginal Post Checking that your Agent Jobs have completed successfully is a vital part of any DBA‚Äôs responsibility. It is essential to ensure that all of the hard work you have put into setting up the jobs can be quickly and easily checked. In a large estate this can be very time consuming and if done manually prone to human error. I have repeatedly mentioned John Sansoms Blog Post entitled ‚ÄúThe Best DBAs Automate Everything‚Äù and I follow that advice. Today I will share with you one fo the first scripts that I wrote.\nWhen I started as a DBA I was told that my first job every morning was to check the Agent Jobs and resolve any errors. This is still something I do first before anything else. (Except coffee, experience has taught me that you get your coffee before you log into your computer otherwise on the bad days you can miss out on coffee for many an hour) I have two scripts to do this. The first sends me an email if the number of failed jobs on a server is greater than zero. This helps me to quickly and simply identify where to start in the case of multiple failures and is also a backup to the second script.\nThe second script runs on a different server and creates an excel worksheet and colour codes it. This makes it very simple to quickly scroll through the sheet and spot any red cells which designate failed jobs and also provides a nice easy to understand method to show management that on that specific day everything went well (or badly)\nAs with any Powershell script which manipulates Office applications you first need to create an object and add the workbook and worksheet to it. I also set a filename date variable and a Date variable for the Sheet.\nWhen you use Powershell to manipulate Excel you can access individual cells by identifying them by Row and Column. I use this to create a description for the work book as follows\nThere are lots of properties that you can play with within Excel. As with any Powershell the best way to find what you need is to use the Get-Member Cmdlet. If you run\n1 ($cells.item(1,3)|Get-Member).Count You will see that there are 185 Methods and Properties available to you (in Office 2013 on Windows 8.1) The snippet above creates the following\nAs you can see we are going to colour code the Job Status according to the three available results Successful, Failed and Unknown. We are also going to colour code the date column to see when the job was last run, this will enable you to easily identify if the last time the job ran it was successful but last night it didn‚Äôt kick off for some reason.\nThe next step is a fairly standard loop through available servers by picking them from a SQLServers text file, a list of the server names (ServerName\\Instance if required) that you wish to check. You could also just create an array of server names or pick them from a table with Invoke-SQLCmd but which ever way you do it you need to be able to iterate through the array and then the .Jobs Collection in the JobServer Namespace as follows\nWhat the script then does is to use the following properties of the $Job object and write the Excel File according to the logic in the description\n1 2 3 4 $Job.Name $Job.IsEnabled $Job.LastRunOutcome $Job.LastRunDate To finish up save the workbook to a share available to all of the DBA Team and quit Excel. Notice that I use a double whammy to make sure Excel is really gone. First I quit the .com object and then I stop the process. I do this because I found that on my server quitting the .com object left the Excel process running and I ended up with dozens and dozens of them. If you have Excel open before you run this script either comment out the last line or save your work (You should save your work anyway regulary!)\nAs always I take no responsibility for your environment, that‚Äôs your Job! Don‚Äôt run this on Production unless you know what it is doing and are happy that you have first tested it somewhere safely away from any important systems. Make sure that you understand the correct time to run this job and have qualified the impact on the box it is running on. Here is a screen shot of the finished Excel Sheet\nAs you can see the Data Transfer Job needs investigation! The reason I add to yellow rows above and below each servers list of jobs is to help me identify any server that is not responding as that will be easily recognised as two lots of yellow with nothing between them I have considered improving this script by inputting the data into a database and running a report from that database but have not had the need to do so yet.\nHere is the script\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 ############################################################################################# # # NAME: Agent Job Status to Excel.ps1 # AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com # DATE:22/07/2013 # # COMMENTS: Iterates through the sqlservers.txt file to populate # Excel File with colour coded status # # WARNING - This will stop ALL Excel Processes. Read the Blog Post for more info # # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî # Get List of sql servers to check $sqlservers = Get-Content \u0026#39;\u0026#39;; # from a file or a SQL query or whatever # Create a .com object for Excel $xl = new-object -comobject excel.application $xl.Visible = $true # Set this to False when you run in production $wb = $xl.Workbooks.Add() $ws = $wb.Worksheets.Item(1) $date = Get-Date -format f $Filename = ( get-date ).ToString(\u0026#39;ddMMMyyyHHmm\u0026#39;) $cells = $ws.Cells # Create a description $cells.item(1, 3).font.bold = $True $cells.item(1, 3).font.size = 18 $cells.item(1, 3) = \u0026#34;Back Up Report $date\u0026#34; $cells.item(5, 9) = \u0026#34;Last Job Run Older than 1 Day\u0026#34; $cells.item(5, 8).Interior.ColorIndex = 43 $cells.item(4, 9) = \u0026#34;Last Job Run Older than 7 Days\u0026#34; $cells.item(4, 8).Interior.ColorIndex = 53 $cells.item(7, 9) = \u0026#34;Successful Job\u0026#34; $cells.item(7, 8).Interior.ColorIndex = 4 $cells.item(8, 9) = \u0026#34;Failed Job\u0026#34; $cells.item(8, 8).Interior.ColorIndex = 3 $cells.item(9, 9) = \u0026#34;Job Status Unknown\u0026#34; $cells.item(9, 8).Interior.ColorIndex = 15 #define some variables to control navigation $row = 3 $col = 2 #insert column headings $cells.item($row, $col) = \u0026#34;Server\u0026#34; $cells.item($row, $col).font.size = 16 $Cells.item($row, $col).Columnwidth = 10 $col++ $cells.item($row, $col) = \u0026#34;Job Name\u0026#34; $cells.item($row, $col).font.size = 16 $Cells.item($row, $col).Columnwidth = 40 $col++ $cells.item($row, $col) = \u0026#34;Enabled?\u0026#34; $cells.item($row, $col).font.size = 16 $Cells.item($row, $col).Columnwidth = 15 $col++ $cells.item($row, $col) = \u0026#34;Outcome\u0026#34; $cells.item($row, $col).font.size = 16 $Cells.item($row, $col).Columnwidth = 12 $col++ $cells.item($row, $col) = \u0026#34;Last Run Time\u0026#34; $cells.item($row, $col).font.size = 16 $Cells.item($row, $col).Columnwidth = 15 $col++ # Load SMO extension [System.Reflection.Assembly]::LoadWithPartialName(\u0026#34;Microsoft.SqlServer.Smo\u0026#34;) | Out-Null; # Loop through each sql server from sqlservers.txt foreach ($sqlserver in $sqlservers) { # Create an SMO Server object $srv = New-Object \u0026#34;Microsoft.SqlServer.Management.Smo.Server\u0026#34; $sqlserver; # For each jobs on the server foreach ($job in $srv.JobServer.Jobs) { $jobName = $job.Name; $jobEnabled = $job.IsEnabled; $jobLastRunOutcome = $job.LastRunOutcome; $Time = $job.LastRunDate ; # Set Fill Colour for Job Enabled if ($jobEnabled -eq \u0026#34;FALSE\u0026#34;) { $colourenabled = \u0026#34;2\u0026#34;} else {$colourenabled = \u0026#34;48\u0026#34; } # Set Fill Colour for Failed jobs if ($jobLastRunOutcome -eq \u0026#34;Failed\u0026#34;) { $colour = \u0026#34;3\u0026#34; # RED } # Set Fill Colour for Uknown jobs Elseif ($jobLastRunOutcome -eq \u0026#34;Unknown\u0026#34;) { $colour = \u0026#34;15\u0026#34;} #GREY else {$Colour = \u0026#34;4\u0026#34;} # Success is Green $row++ $col = 2 $cells.item($Row, $col) = $sqlserver $col++ $cells.item($Row, $col) = $jobName $col++ $cells.item($Row, $col) = $jobEnabled #Set colour of cells for Disabled Jobs to Grey $cells.item($Row, $col).Interior.ColorIndex = $colourEnabled if ($colourenabled -eq \u0026#34;48\u0026#34;) { $cells.item($Row , 1 ).Interior.ColorIndex = 48 $cells.item($Row , 2 ).Interior.ColorIndex = 48 $cells.item($Row , 3 ).Interior.ColorIndex = 48 $cells.item($Row , 4 ).Interior.ColorIndex = 48 $cells.item($Row , 5 ).Interior.ColorIndex = 48 $cells.item($Row , 6 ).Interior.ColorIndex = 48 $cells.item($Row , 7 ).Interior.ColorIndex = 48 } $col++ $cells.item($Row, $col) = \u0026#34;$jobLastRunOutcome\u0026#34; $cells.item($Row, $col).Interior.ColorIndex = $colour #Reset Disabled Jobs Fill Colour if ($colourenabled -eq \u0026#34;48\u0026#34;) {$cells.item($Row, $col).Interior.ColorIndex = 48} $col++ $cells.item($Row, $col) = $Time #Set teh Fill Colour for Time Cells If ($Time -lt ($(Get-Date).AddDays(-1))) { $cells.item($Row, $col).Interior.ColorIndex = 43} If ($Time -lt ($(Get-Date).AddDays(-7))) { $cells.item($Row, $col).Interior.ColorIndex = 53} } $row++ $row++ # Add two Yellow Rows $ws.rows.item($Row).Interior.ColorIndex = 6 $row++ $ws.rows.item($Row).Interior.ColorIndex = 6 $row++ } $wb.Saveas(\u0026#34;C:\\temp\\Test$filename.xlsx\u0026#34;) $xl.quit() Stop-Process -Name EXCEL If you have any questions please get in touch\n","date":"2014-03-31T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/03/033114_2017_howicheckhu6.png","permalink":"https://blog.robsewell.com/blog/how-i-check-hundreds-of-sql-agent-jobs-in-60-seconds-with-powershell/","title":"How I Check Hundreds of SQL Agent Jobs in 60 Seconds with Powershell"},{"content":"In the previous post I showed the script to create an Excel Workbook, colour coded showing the last used date for all of the databases on servers in my sqlservers.txt file. After gathering that information over several months, there is then a requirement for someone to make a decision as to which databases can be removed.\nObviously there will be some databases that are read-only or if not set specifically as read-only may only be used for reference without data being added. You should hopefully have knowledge of these databases and be able to take them off the list quickly.\nThere are other challenges for a DBA to overcome prior to any action. Many questions need to be answered such as\nWho owns the database? Who is the service owner responsible for the service/application in use by the database? Even though they may be the service owner who will ultimately sign off permission to remove the database are they aware of how important it is for their people? Or what times of the year it is important to them? You may find test and development databases that have not been used for months but will they be required next week? Is it important enough for them to take the time to give the permission?\nAnd plenty more‚Ä¶ Add some in the comments below.\nOur Primary responsibility is the data. We need to be able to ensure that the data is safe and can be made available quickly and easily. In this situation we need to have a valid backup and a quick and easy method of restoring it. I chose to solve this by creating a T-SQL script which will :-\nPerform a DBCC CHECKDB on the database Backup the database with CHECKSUM Perform a VERIFY ONLY restore of the database Drop the database Create an agent job to restore the database from that backup The reasoning for these steps is best explained by watching this video\nand yes I always perform the last step too J\nI could have used PowerShell to do this by examining The SMO for the Server and the JobServer but this time I decided to challenge myself by writing it in T-SQL as I am weaker in that area. The script below is the result of that work. It works for me. I expect that there are other ways of doing this and please feel free to point out any errors or suggestions. That is how I learn. Hopefully these posts will be of use to other DBAs like myself.\nAs always with anything you read on the internet. Validate and test. This script works for me on SQL Servers 2005, 2008,2008R2 and 2012 but if you are thinking of running it in your own Production Environment ‚Äì DON‚ÄôT.\nWell not until you have tested it somewhere safe first J\nThe first challenge I encountered was that I wanted to only have to change the name of the database to be able to run the script and perform all of these steps. That will also lead onto a stored procedure and then I can automate more of this process and schedule at times to suit the database servers as well. I accomplished this by using a temp table and populating it with the variables I will need as shown below\n-- Drop temp table if it exists\rIF OBJECT_ID('tempdb..#vars') IS NOT NULL\rDROP TABLE #vars\r-- Create table to hold global variable\rcreate table #vars (DBName nvarchar(50), PATH nvarchar(300),DataName nvarchar(50),LogName nvarchar (50),DataLoc nvarchar (256),LogLoc nvarchar (256))\rinsert into #vars (DBName) values ('DATABASENAME')\r-- Declare and set variables\rDECLARE @PATH nvarchar(300)\rSet @Path = (SELECT 'PATH TO RATIONALISATION FOLDER WITH TRAILING SLASH' + @DBName + '_LastGolden_' + + convert(varchar(50),GetDate(),112) + '.bak' )\rDECLARE @DataName nvarchar(50)\rSet @DataName = (SELECT f.name\rFROM sys.master_files F\rjoin sys.databases D\ron\u0026amp;amp;nbsp;d.database_id = f.database_id\rWHERE F.type = 0\rAND d.Name = @DBNAME)\r-- Print @DataName\rDECLARE @LogName nvarchar (50)\rSet @LogName = (SELECT f.name\rFROM sys.master_files F\rjoin sys.databases D\ron\u0026amp;amp;nbsp;d.database_id = f.database_id\rWHERE F.type = 1\rAND d.Name = @DBNAME)\r-- PRINT @LogName\rDeclare @DataLoc nvarchar (256)\rSet @DataLoc = (SELECT f.physical_name\rFROM sys.master_files F\rjoin sys.databases D\ron\u0026amp;amp;nbsp;d.database_id = f.database_id\rWHERE F.type = 0\rAND d.Name = @DBNAME)\r--Print @DataLoc\rDeclare @LogLoc nvarchar (256)\rSet @LogLoc = (SELECT f.physical_name\rFROM sys.master_files F\rjoin sys.databases D\ron\u0026amp;amp;nbsp;d.database_id = f.database_id\rWHERE F.type = 1\rAND d.Name = @DBNAME)\r--Print @LogLoc\rupdate #vars Set PATH = @PATH\rupdate #vars Set DataName = @DataName\rupdate #vars Set LogName = @LogName\rupdate #vars Set DataLoc = @DataLoc\rupdate #vars Set LogLoc = @LogLoc\r-- Select * from #vars\rI then use the variables throughout the script by selecting them from the temp table as follows\nDECLARE @DBName nvarchar(50)\rSet @DBName = (Select DBNAme from #vars)\u0026amp;lt;code\u0026amp;gt;\rAnd using the variables to create and execute the T-SQL for each of the steps above.\nIt is pointless to move onto the next step of the previous one has failed so I created some error handling as follows\nif @@error != 0 raiserror('Rationalisation Script failed at Verify Restore', 20, -1) with log\rGO\rI created the T-SQL for the agent job by first creating the restore script and adding it to a variable and then right-clicking on a previously created restore database job and using the script to new window command\nIt was then a case of adding single quotes and reading the code until it would successfully run\n/***\rRationalisation Script\rScript to Automatically Backup, Drop and create Agent Job to restore from that backup\rAUTHOR - Rob Sewell https://blog.robsewell.com\rDATE - 19/01/2014\rUSAGE - You need to Change the Database Name after \u0026quot; insert #vars values (' \u0026quot;\rYou also need to check that the folder after \u0026quot; Set @Path = (SELECT ' \u0026quot; is correct and exists\rand Find and replace both entries for THEBEARD\\Rob with the account that will be the owner of the job and the database owner\rOnce this has been run AND you have checked that it has successfully backed up the database and created the job and you have checked hte job works\rYou may delete the backups but keep the backup folder under UserDbs\r***/\r--Drop temp table if it exists\rIF OBJECT_ID('tempdb..#vars') IS NOT NULL\rDROP TABLE #vars\r--Create table to hold global variable\rcreate table #vars (DBName nvarchar(50), PATH nvarchar(300),DataName nvarchar(50),LogName nvarchar (50),DataLoc nvarchar (256),LogLoc nvarchar (256))\rinsert into #vars (DBName) values ('SQL2012Ser2012DB'\r)\r--Declare and set variables\rDECLARE @DBName nvarchar(50)\rSet @DBName = (Select DBNAme from #vars)\rDECLARE @PATH nvarchar(300)\rSet @Path = (SELECT 'PATH TO RATIONALISATION FOLDER' + @DBName + '_LastGolden_' + + convert(varchar(50),GetDate(),112) + '.bak' )\rDECLARE @DataName nvarchar(50)\rSet @DataName = (SELECT f.name\rFROM sys.master_files F\rjoin sys.databases D\ron\rd.database_id = f.database_id\rWHERE F.type = 0\rAND d.Name = @DBNAME)\r--Print @DataName\rDECLARE @LogName nvarchar (50)\rSet @LogName = (SELECT f.name\rFROM sys.master_files F\rjoin sys.databases D\ron\rd.database_id = f.database_id\rWHERE F.type = 1\rAND d.Name = @DBNAME)\r--PRINT @LogName\rDeclare @DataLoc nvarchar (256)\rSet @DataLoc = (SELECT f.physical_name\rFROM sys.master_files F\rjoin sys.databases D\ron\rd.database_id = f.database_id\rWHERE F.type = 0\rAND d.Name = @DBNAME)\r--Print @DataLoc\rDeclare @LogLoc nvarchar (256)\rSet @LogLoc = (SELECT f.physical_name\rFROM sys.master_files F\rjoin sys.databases D\ron\rd.database_id = f.database_id\rWHERE F.type = 1\rAND d.Name = @DBNAME)\r--Print @LogLoc\rupdate #vars Set PATH = @PATH\rupdate #vars Set DataName = @DataName\rupdate #vars Set LogName = @LogName\rupdate #vars Set DataLoc = @DataLoc\rupdate #vars Set LogLoc = @LogLoc\r-- Select * from #vars\r-- DBCC\rDECLARE @DBCCSQL nvarchar (4000)\rSET @DBCCSQL = '\rUSE [' + @DBName + ']\rDBCC CHECKDB WITH NO_INFOMSGS, ALL_ERRORMSGS\r'\r-- Print @DBCCSQL\rEXECUTE(@DBCCSQL)\r-- Break out if error raised We need to do some work if there are errors here\rif @@error != 0 raiserror('Rationalisation Script failed at DBCC', 20, -1) with log\rGO\r-- Declare and set variables\rDECLARE @DBName nvarchar(50)\rSet @DBName = (Select DBNAme from #vars)\rDECLARE @PATH nvarchar(300)\rSet @Path = (SELECT PATH from #vars)\rDeclare @BKUPName nvarchar(300)\rSet @BKUPName = (SELECT 'Last Golden Backup For ' + @DBName + '- Full Database Backup')\rDECLARE @BackupSQL nvarchar (4000)\rSET @BackupSQL = '\rBACKUP DATABASE [' + @DBName + '] TO DISK = N''' + @PATH + '''\rWITH INIT, NAME = N''' + @BKUPName + ''',\rCHECKSUM, STATS = 10\r'\r--- PRINT @BackupSQL\r-- Backup database to Golden backup location\rEXECUTE(@BackupSQL)\rGO\r-- Break Out if there are errors here - If there is no backup we don't want to continue\rif @@error != 0 raiserror('Rationalisation Script failed at Backup', 20, -1) with log\rGO\rDECLARE @PATH nvarchar(300)\rSet @Path = (SELECT PATH from #vars)\rRESTORE VERIFYONLY\rFROM DISK = @PATH;\rif @@error != 0 raiserror('Rationalisation Script failed at Verify Restore', 20, -1) with log\rGO\r-- Declare variables for dropping database\rDECLARE @DBName nvarchar(50)\rSet @DBName = (Select DBNAme from #vars)\rDECLARE @DROPSQL nvarchar (4000)\rSET @DROPSQL = '\rUSE [master]\rALTER DATABASE [' + @DBName + '] SET SINGLE_USER WITH ROLLBACK IMMEDIATE\rDROP DATABASE [' + @DBName + ']\r'\r-- PRINT @DROPSQL\r--Drop database\rEXECUTE(@DROPSQL)\rGO\rif @@error != 0 raiserror('Rationalisation Script failed at Drop Database', 20, -1) with log\rGO\r--Declare variables for creating Job\rDECLARE @DBName nvarchar(50)\rSet @DBName = (Select DBNAme from #vars)\rDECLARE @PATH nvarchar(300)\rSet @Path = (Select PATH from #vars)\rDECLARE @DataName nvarchar(50)\rSet @DataName = (Select DataName from #vars)\rDECLARE @LogName nvarchar (50)\rSet @LogName = (Select LogName from #vars)\rDeclare @DataLoc nvarchar (256)\rSet @DataLoc = (Select DataLoc from #vars)\rDeclare @LogLoc nvarchar (256)\rSet @LogLoc = (Select LogLoc from #vars)\rDECLARE @RestoreCommand nvarchar(4000)\rSet @RestoreCommand = '''RESTORE DATABASE [' + @DBName + ']\rFROM DISK = N''''' + @PATH + '''''\rWITH FILE = 1,\rMOVE N''''' + @DataName + ''''' TO N''''' + @DataLoc + ''''',\rMOVE N''''' + @LogName + ''''' TO N''''' + @LogLoc + ''''',\rNOUNLOAD, REPLACE, STATS = 10\r'''\r--print @RestoreCommand\r--Create Job creation tsql\rDECLARE @JOBSQL nvarchar (4000)\rSET @JOBSQL = 'USE [msdb]\rBEGIN TRANSACTION\rDECLARE @ReturnCode INT\rSELECT @ReturnCode = 0\r/****** Object: JobCategory [[Uncategorized (Local)]]] Script Date: 01/18/2014 14:12:04 ******/\rIF NOT EXISTS (SELECT name FROM msdb.dbo.syscategories WHERE name=N''[Uncategorized (Local)]'' AND category_class=1)\rBEGIN\rEXEC @ReturnCode = msdb.dbo.sp_add_category @class=N''JOB'', @type=N''LOCAL'', @name=N''[Uncategorized (Local)]''\rIF (@@ERROR \u0026amp;lt;\u0026amp;gt; 0 OR @ReturnCode \u0026amp;lt;\u0026amp;gt; 0) GOTO QuitWithRollback\rEND\rDECLARE @JOBNAME nvarchar(300)\rset @JOBNAME = ''Rationlised - - Restore ' + @DBName + ' from Last Golden Backup''\rDeclare @JobDesc nvarchar(300)\rSet @JobDesc = '' Rationalised Database Restore Script for ' + @DBName + '''\rDECLARE @jobId BINARY(16)\rEXEC @ReturnCode = msdb.dbo.sp_add_job @job_name= @JOBNAME,\r@enabled=1,\r@notify_level_eventlog=0,\r@notify_level_email=0,\r@notify_level_netsend=0,\r@notify_level_page=0,\r@delete_level=0,\r@description=@JobDesc,\r@category_name=N''[Uncategorized (Local)]'',\r@owner_login_name=N''THEBEARD\\Rob'', @job_id = @jobId OUTPUT\rIF (@@ERROR \u0026amp;lt;\u0026amp;gt; 0 OR @ReturnCode \u0026amp;lt;\u0026amp;gt; 0) GOTO QuitWithRollback\r/****** Object: Step [Restore Database] Script Date: 01/18/2014 14:12:04 ******/\rEXEC @ReturnCode = msdb.dbo.sp_add_jobstep @job_id=@jobId, @step_name=N''Restore Database'',\r@step_id=1,\r@cmdexec_success_code=0,\r@on_success_action=3,\r@on_success_step_id=0,\r@on_fail_action=2,\r@on_fail_step_id=0,\r@retry_attempts=0,\r@retry_interval=0,\r@os_run_priority=0, @subsystem=N''TSQL'',\r@command= ' + @RestoreCommand + ',\r@database_name=N''master'',\r@flags=4\r/****** Object: Step [Set Owner] Script Date: 01/19/2014 10:14:57 ******/\rEXEC @ReturnCode = msdb.dbo.sp_add_jobstep @job_id=@jobId, @step_name=N''Set Owner'',\r@step_id=2,\r@cmdexec_success_code=0,\r@on_success_action=1,\r@on_success_step_id=0,\r@on_fail_action=2,\r@on_fail_step_id=0,\r@retry_attempts=0,\r@retry_interval=0,\r@os_run_priority=0, @subsystem=N''TSQL'',\r@command=N''USE [' + @DBName + ']\rEXEC sp_changedbowner @loginame = N''''THEBEARD\\Rob'''', @map = false'',\r@database_name=N''master'',\r@flags=0\rIF (@@ERROR \u0026amp;lt;\u0026amp;gt; 0 OR @ReturnCode \u0026amp;lt;\u0026amp;gt; 0) GOTO QuitWithRollback\rEXEC @ReturnCode = msdb.dbo.sp_update_job @job_id = @jobId, @start_step_id = 1\rIF (@@ERROR \u0026amp;lt;\u0026amp;gt; 0 OR @ReturnCode \u0026amp;lt;\u0026amp;gt; 0) GOTO QuitWithRollback\rEXEC @ReturnCode = msdb.dbo.sp_add_jobserver @job_id = @jobId, @server_name = N''(local)''\rIF (@@ERROR \u0026amp;lt;\u0026amp;gt; 0 OR @ReturnCode \u0026amp;lt;\u0026amp;gt; 0) GOTO QuitWithRollback\rCOMMIT TRANSACTION\rGOTO EndSave\rQuitWithRollback:\rIF (@@TRANCOUNT \u0026amp;gt; 0) ROLLBACK TRANSACTION\rEndSave:\r'\r--PRINT @JOBSQL\r--Create Agent Job\rEXECUTE(@JOBSql)\rif @@error != 0 raiserror('Rationalisation Script failed at Create Job', 20, -1) with log\rGO\rDROP Table #vars\rThe process I have used is to change the database name in the script and run it and then run the Agent Job and check the database has been created. Then and only then can I drop the database and disable any jobs for the database. Yes that was the last step in the video J as Grant says ‚Äúa file is just a file, a backup is a restored database‚Äù\nUsing this script you can reduce the footprint and load on your servers by removing unneeded or unused databases whilst still guaranteeing that should there be a requirement for them you KNOW you can easily restore them. You will still need to take some additional steps like adding a stop to the Agent Job to recreate any users and any other jobs that the database needs but that is more specific to your environment and you will be best placed to achieve this\n","date":"2014-03-03T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/03/030314_2100_rationalisa1.png","permalink":"https://blog.robsewell.com/blog/rationalisation-of-database-with-powershell-and-t-sql-part-two/","title":"Rationalisation of Database with Powershell and T-SQL part two"},{"content":"I have recently been involved in a project to rationalise databases. It is easy in a large organisation for database numbers to rapidly increase and sometimes the DBA may not be aware of or be able to control the rise if they don‚Äôt have knowledge of all of the database servers on the estate.\nThere are lots of¬†benefits of rationalisation to the business. Reduced cpu usage = reduced heat released = lower air-con bill for the server room and¬†less storage used¬†=¬†quicker backups and less tapes used or better still less requirement for that expensive new SAN. You may be able to consolidate data and provide one version of the truth for the business as well. Removing servers can release licensing costs which could then be diverted elsewhere or pay for other improvements.\nWilliam Durkin b¬†t¬†presented to the SQL South West User Group about this and will be doing the session at SQL Saturday in Exeter in March 2014 Please check out his session for a more detailed view\nI needed to be able to identify databases that could possibly be deleted and realised that an easy way to achieve this would be to use a script to check for usage of the database.\nNo need to recreate the wheel so I went to Aaron Bertrands blog http://sqlblog.com/blogs/aaron_bertrand/archive/2008/05/06/when-was-my-database-table-last-accessed.aspx¬†and used his script. Instead of using an audit file I decided to use Powershell so that I could output the results to Excel and colour code them. This made it easier to check the results and also easier to show to Managers and Service Owners\n#################################################################################\r# NAME: lastdbusage.ps1\r# AUTHOR: Rob Sewell\r# https://blog.robsewell.com\r# DATE:19/10/2013\r#\r# COMMENTS: Fill Excel WorkBook with details fo last access times for each database\r#\r# NOTES : Does NOT work with SQL 2000 boxes\r$FileName = '' # Set a filename for the output\r# Get List of sql servers to check\r$sqlservers = Get-Content '' # serverlist, database query whatever\r# Set SQL Query\r$query = \u0026quot;WITH agg AS\r(\rSELECT\rmax(last_user_seek) last_user_seek,\rmax(last_user_scan) last_user_scan,\rmax(last_user_lookup) last_user_lookup,\rmax(last_user_update) last_user_update,\rsd.name dbname\rFROM\rsys.dm_db_index_usage_stats, master..sysdatabases sd\rWHERE\rsd.name not in('master','tempdb','model','msdb')\rAND\rdatabase_id = sd.dbid group by sd.name\r)\rSELECT\rdbname,\rlast_read = MAX(last_read),\rlast_write = MAX(last_write)\rFROM\r(\rSELECT dbname, last_user_seek, NULL FROM agg\rUNION ALL\rSELECT dbname, last_user_scan, NULL FROM agg\rUNION ALL\rSELECT dbname, last_user_lookup, NULL FROM agg\rUNION ALL\rSELECT dbname, NULL, last_user_update FROM agg\r) AS x (dbname, last_read, last_write)\rGROUP BY\rdbname\rORDER BY 1;\r\u0026quot;\r#Open Excel\r$xl = new-object -comobject excel.application\r$wb = $xl.Workbooks.Add()\r# Load SMO extension\r[System.Reflection.Assembly]::LoadWithPartialName(\u0026quot;Microsoft.SqlServer.Smo\u0026quot;) | Out-Null;\r# Loop through each sql server from sqlservers.txt\rforeach ($sqlserver in $sqlservers) {\r# Get the time SQL was restarted\r$svr = New-Object 'Microsoft.SQLServer.Management.Smo.Server' $SQLServer\r$db = $svr.Databases['TempDB']\r$CreateDate = $db.CreateDate\r#Run Query against SQL Server\r$Results = Invoke-Sqlcmd -ServerInstance $sqlServer -Query $query -Database master\r# Add a new sheet\r$ws = $wb.Worksheets.Add()\r$name = \u0026quot;$sqlserver\u0026quot;\r# Name the Sheet\r$ws.name = $Name\r$cells = $ws.Cells\r$xl.Visible = $true\r#define some variables to control navigation\r$row = 2\r$col = 2\r$cells.item($row, $col) = $SQLServer + ' Was Rebooted at ' + $CreateDate\r$cells.item($row, $col).font.size = 16\r$Cells.item($row, $col).Columnwidth = 10\r$row = 3\r$col = 2\r# Set some titles\r$cells.item($row, $col) = \u0026quot;Server\u0026quot;\r$cells.item($row, $col).font.size = 16\r$Cells.item($row, $col).Columnwidth = 10\r$col++\r$cells.item($row, $col) = \u0026quot;Database\u0026quot;\r$cells.item($row, $col).font.size = 16\r$Cells.item($row, $col).Columnwidth = 40\r$col++\r$cells.item($row, $col) = \u0026quot;Last Read\u0026quot;\r$cells.item($row, $col).font.size = 16\r$Cells.item($row, $col).Columnwidth = 20\r$col++\r$cells.item($row, $col) = \u0026quot;Last Write\u0026quot;\r$cells.item($row, $col).font.size = 16\r$Cells.item($row, $col).Columnwidth = 20\r$col++\rforeach ($result in $results) {\r# Check if value is NULL\r$DBNull = [System.DBNull]::Value\r$LastRead = $Result.last_read\r$LastWrite = $Result.last_write\r$row++\r$col = 2\r$cells.item($Row, $col) = $sqlserver\r$col++\r$cells.item($Row, $col) = $Result.dbname\r$col++\rif ($LastRead -eq $DBNull) {\r$LastRead = \u0026quot;Not Since Last Reboot\u0026quot;\r$colour = \u0026quot;46\u0026quot;\r$cells.item($Row, $col).Interior.ColorIndex = $colour\r$cells.item($Row, $col) = $LastRead\r}\relse {\r$cells.item($Row, $col) = $LastRead\r}\r$col++\rif ($LastWrite -eq $DBNull) {\r$LastWrite = \u0026quot;Not Since Last Reboot\u0026quot;\r$colour = \u0026quot;46\u0026quot;\r$cells.item($Row, $col).Interior.ColorIndex = $colour\r$cells.item($Row, $col) = $LastWrite\r}\relse {\r$cells.item($Row, $col) = $LastWrite\r}\r}\r}\r$xl.DisplayAlerts = $false\r$wb.Saveas($FileName)\r$xl.quit()\rStop-Process -Name *excel*\rWhat it does is place the query in a variable. Get the contents of the SQL Server text file holding all my known SQL Servers and runs the query against each of them storing the results in a variable. It then creates an Excel Workbook and a new sheet for each server and populates the sheet including a bit of colour formatting before saving it. The results look like this\nThe tricky bit was understanding how to match the NULL result from the query. This was done by assigning a variable to¬†[System.DBNull]::Value and using that.\nOf course these stats are reset when SQL Server restarts so I also included the SQL server restart time using the create date property of the TempDB. I gathered these stats for a few months before starting any rationalisation.\nMy next post will be about the next step in the process.\n","date":"2014-02-25T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/02/usage-excel.jpg","permalink":"https://blog.robsewell.com/blog/rationalisation-of-database-with-powershell-and-t-sql-part-one/","title":"Rationalisation of Database with Powershell and T-SQL part one"},{"content":"SQL Saturdays and other community events rely on sponsors and you know that you will often get entered into a raffle for a prize in exchange for your contact details and there will be freebies of various types from many vendors but there is more that you can get from visiting the sponsors.\nFREE COFFEE\nAt SQL Saturday Exeter on the 22nd March 2014¬†we are putting free coffee amongst the sponsors. Yes, it‚Äôs a ruse in some ways to put you in the same room as the sponsors whilst your mind is buzzing with all the new SQL learning you have been doing and you are feeling confident and inspired about SQL and what you can achieve.\nWe need the sponsors to put on the events and the sponsors need us to help put them in contact with purchasers of their wares. It is good for all community events if the sponsors can put SQL Saturday Exeter (or another community event} into their CRM as the point of first contact or the place a decision was made for a purchase as it will mean that when they analyse their data in readiness for next years budget community events will still be important to them and they will spend their money and we will continue to be able to benefit from superb free or very cheap training and learning, networking and down right good fun at next years events\nDIRECT CONTACT WITH THE PEOPLE WHO MAKE YOUR TOOLS\nFor example, I use Red Gate‚Äôs SQL Monitor and make use of the graphs to baseline, to see when there are variations to that baseline and to get alerted about long running queries, deadlocks and many other useful DBA information.\nWhilst at the Red Gate stand at SQL Saturday in Cambridge I got talking to Daniel Rothig who is one of the developers for SQL Monitor and I was able to ask him about using SQL Monitor.\nI wanted to know how best to use the base lining feature and how best to describe some of the detail I was seeing to none-technical people. He and Jonathan Allen was able to give me some examples and knowledge to improve my capabilities in this area. It was fantastic to be able to discuss the product with him and see where they are wanting to take it.\nTHE SPONSORS WIN TOO\nObviously the sponsors need customers and that is why they put a large amount of marketing activity into SQL Community Events. They want to put their products in front of the people who will be using them and make sales.\nBut there is a further benefit too Daniel asked me to show him how I used the tool and what I would improve if I could.¬†I explained that I was having trouble getting the Regex correct for writing exceptions for the alerts for long running queries and I said that I wished there was button I could press to automatically ignore that query that sometimes. He said he would take that back to the team. Excellent, I was able to get a way to improve a good tool to make me work smarter and my experience better\nI don‚Äôt know if my idea will make it to Production but I hope so. Daniel also said it was useful to see the way users of their software navigated the application and used the features and that that knowledge would help future development\nDaniel said\n‚ÄúI‚Äôm sure we can make a sale or two on a SQL Saturday ‚Äì but then, why am I there, and not a sales team? We‚Äôve found it‚Äôs more valuable to meet people in the community, learn about their jobs and problems, and search for a gleam in their eyes when we show them our solutions. We take home those first impressions, and the feedback from long-time users, to make our software more focused, relevant, and useful.\nAnd the conversations are always great fun ‚Äì so come and say hi!‚Äù\nIt‚Äôs a win all ways round\nWE‚ÄôLL PUT BISCUITS WITH THE COFFEE TOO AT SQL SAT EXETER!!\nFind out more about SQL Saturday Exeter at http://sqlsouthwest.co.uk/\n","date":"2014-02-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/why-you-should-visit-the-sponsors-at-%23sqlsatexeter-and-other-community-events/","title":"Why You Should Visit the Sponsors at #SQLSatExeter and Other Community Events"},{"content":"A quick and simple post today as I have been very busy. I needed to list the users with permissions on mirroring endpoints today so I wrote this script and figured it was worth sharing.\nIt‚Äôs a simple script which takes a server name from a Read-Host prompt. Displays the available endpoints and asks which one you want and shows you the permissions\n$Server = Read-Host \u0026quot;Please Enter the Server\u0026quot;\r$Endpoints = $srv.Endpoints |select Name -ExpandProperty Name\r$EndpointName = Read-Host \u0026quot;Please Enter the Endpoint Name `n Available Names are `n $Endpoints\u0026quot;\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\r$Endpoint = $srv.Endpoints[$EndpointName]\r$Endpoint.enumObjectPermissions()\rand heres a screenshot of the results\nIf you want to do it with T-SQL\nselect s.name as grantee,\re.name as endpoint,\rp.permission_name as permission,\rp.state_desc as state_desc\rfrom sys.server_permissions p\rjoin sys.server_principals s on s.principal_id = p.grantee_principal_id\rjoin sys.endpoints e on p.major_id = e.endpoint_id\rwhere p.type='CO'\r","date":"2014-02-09T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2014/02/ps1.jpg","permalink":"https://blog.robsewell.com/blog/viewing-sql-endpoint-permissions-with-powershell/","title":"Viewing SQL Endpoint Permissions with PowerShell"},{"content":"A quick blog today. I was reading this blog post about How to read the SQL Error Log and I thought I would try some of the examples. I started my Azure VM using the steps in my previous post\nI ran\nGet-AzureVM -ServiceName TheBestBeard -Name Fade2black\rand then\nGet-AzureVM -ServiceName TheBestBeard -Name Fade2black|Get-AzureEndpoint |Format-Table -AutoSize\rand bingo I had my SQL Port to put in SSMS and can go and play some more with SQL\n","date":"2013-12-02T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-powershell-to-get-azure-endpoint-ports/","title":"Using PowerShell to get Azure Endpoint Ports"},{"content":"This post could also have been titled confusion with foreach or For-EachObject\nThe scenario ‚Äì Having created a blank database a number of users and permissions for an external consultant to create a test database for an application I got a phone call.\nPlease can you drop all the tables from the database as we need to re-run the installer with some different parameters\nSure, I thought. No problem. I will use PowerShell. A simple script is all I need\nThat ought to do it. Loop through the tables and drop each one. But when I ran it I got this error\nWhat I did (which I should have done first up but time pressures hadn‚Äôt allowed) was drop the database and write a script to recreate it and all the users and permissions required using my Create Windows User Function and Add User to Database Role Function but it got me thinking.\nSo I went home and fired up my Azure VMs and had a play and found two ways of resolving it. But first lets understand what is happening here. I read this post which explains it quite well for his script.\nWe are going through a list collection and deleting any instance of our event receiver, in the ‚ÄúForeach loop‚Äù. But once we delete an item we are modifying the current list collection. The ‚ÄúForeach‚Äù loop looks to see what the current value is, before it moves on to the next item. But since we deleted the current item, we get the ‚ÄúCollection was modified; enumeration operation may not execute‚Äù error.\nNow that understand what is going on, we can now look at a solution to correct the error.\nThe simplest way to avoid modifying the collection would be with a ‚ÄúFor Loop‚Äù.¬†With a ‚ÄúFor Loop‚Äù, no modifications are made that will interrupt the looping process.\nSo when PowerShell has dropped the table it returns to the tables collection to find the current table before moving on to the next table but as we have deleted the table it falls over.\nSo lets fix it.\nFirst lets create a test database with PowerShell. A piece of code that is useful to keep for scenarios like this. If you are creating a database for something other than a quick demo or a test then go and explore the other properties of the database object that you will surely want to configure. But for this demo the following is fine, it will use default options. The same applies for the tables script below.\nNow lets create some tables.\nAnd check they have been created\nNow following the advice from above we can do the following\nFirst we count the number of tables and set it to a variable and then create a for loop. Note if you put $i ‚Äìle $tables.Count then the script will only delete 4 tables! In the script block we are setting the $table variable to the first in the collection and then drops it. List the table names again to check or run $tables.Count and you will see that all the tables have been deleted.\nThis was the other solution I found. It makes use of the scripter method to script the Drop commands for the tables add them to a Query string and pass that to Invoke-SQLCmd to run it.\n","date":"2013-11-30T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2013/11/image7.png","permalink":"https://blog.robsewell.com/blog/dropping-all-tables-from-a-sql-database-with-powershell/","title":"Dropping All Tables From A SQL Database with PowerShell"},{"content":"The last post about Launching Azure VMs with PowerShell made someone ask me to explain how I start my Azure VMs normally so here goes.\nWhen I decide to write a blog post or develop and test a script or run through demos from a presentation or blog post I fire up my Azure Virtual machines with PowerShell. This is how I do it\nOpen PowerShell and check that I am connected to my default subscription by running Get-AzureSubscription\nNote ‚Äì You must have installed Windows Azure PowerShell and installed the PublishSettingsFile or used Add-AzureAccount for your subscription following the steps here\nhttp://www.windowsazure.com/en-us/manage/install-and-configure-windows-powershell/\nThen I run the following three Cmdlets\nGet-AzureVM shows me the VMs associated with that subscription.\nI then pipe to Start-AzureVM as I want to start both machines. If I only wanted one I would check that\nGet-AzureVM -name Fade2Black -ServiceName TheBestBeard\rreturned the correct machine and then pipe that to Start-AzureVM\nOnce the VMs have started I use Get-AzureRemoteDesktopFile giving a local path for the rdp file and specifying ‚ÄìLaunch to run the RDP session\nand away we go üôÇ\nOnce I have finished simply run\nand my machines are stopped and no longer running my credit down.\n","date":"2013-11-27T00:00:00Z","permalink":"https://blog.robsewell.com/blog/starting-my-azure-sql-server-vms-with-powershell/","title":"Starting My Azure SQL Server VMs with PowerShell"},{"content":"Last week I ran a PowerShell lab at SQL Relay in Cardiff. There are still a few places available for SQL Relay week 2. Take a look here for more details and follow the twitter hashtag #SQLRelay for up to date information\nThe link for my slides and demos from the second part are here https://t.co/Fik2odyUMA\nWhilst we were discussing Show-LastDatabaseBackup Kev Chant @KevChant asked where it was getting the information from and I answered that PowerShell was running SQL commands under the hood against the server and if you ran profiler that is what you would see. We didn‚Äôt have time to do that in Cardiff but I thought I would do it today to show what happens\nA reminder of what Show-LastDatabaseBackup function does\nIf we start a trace with Profiler and run this function we get these results in PowerShell\nIn Profiler we see that it is running the following T-SQL for\nexec sp_executesql N' SELECT dtb.name AS [Name] FROM master.sys.databases AS dtb WHERE (dtb.name=@_msparam_0)',N'@_msparam_0 nvarchar(4000)',@_msparam_0=N'RageAgainstTheMachine'\rand then for\nexec sp_executesql N' create table #tempbackup (database_name nvarchar(128), [type] char(1), backup_finish_date datetime) insert into #tempbackup select database_name, [type], max(backup_finish_date) from msdb..backupset where [type] = ''D'' or [type] = ''L'' or [type]=''I'' group by database_name, [type] SELECT (select backup_finish_date from #tempbackup where type = @_msparam_0 and db_id(database_name) = dtb.database_id) AS [LastBackupDate] FROM master.sys.databases AS dtb WHERE (dtb.name=@_msparam_1) drop table #tempbackup ',N'@_msparam_0 nvarchar(4000),@_msparam_1 nvarchar(4000)',@_msparam_0=N'D',@_msparam_1=N'RageAgainstTheMachine'\rFor\nexec sp_executesql N' create table #tempbackup (database_name nvarchar(128), [type] char(1), backup_finish_date datetime) insert into #tempbackup select database_name, [type], max(backup_finish_date) from msdb..backupset where [type] = ''D'' or [type] = ''L'' or [type]=''I'' group by database_name, [type] SELECT (select backup_finish_date from #tempbackup where type = @_msparam_0 and db_id(database_name) = dtb.database_id) AS [LastDifferentialBackupDate] FROM master.sys.databases AS dtb WHERE (dtb.name=@_msparam_1) \u0026amp;lt;mailto:dtb.name=@_msparam_1)\u0026amp;gt; drop table #tempbackup ',N'@_msparam_0 nvarchar(4000),@_msparam_1 nvarchar(4000)',@_msparam_0=N'I',@_msparam_1=N'RageAgainstTheMachine'\rAnd for\nexec sp_executesql N' create table #tempbackup (database_name nvarchar(128), [type] char(1), backup_finish_date datetime) insert into #tempbackup select database_name, [type], max(backup_finish_date) from msdb..backupset where [type] = ''D'' or [type] = ''L'' or [type]=''I'' group by database_name, [type] SELECT (select backup_finish_date from #tempbackup where type = @_msparam_0 and db_id(database_name) = dtb.database_id) AS [LastLogBackupDate] FROM master.sys.databases AS dtb WHERE (dtb.name=@_msparam_1) \u0026amp;lt;mailto:dtb.name=@_msparam_1)\u0026amp;gt; drop table #tempbackup ',N'@_msparam_0 nvarchar(4000),@_msparam_1 nvarchar(4000)',@_msparam_0=N'L',@_msparam_1=N'RageAgainstTheMachine'\rSo the answer to your question Kev is\nYes it does get the information from the msdb database\n","date":"2013-11-23T00:00:00Z","permalink":"https://blog.robsewell.com/blog/what-runs-on-the-sql-server-when-you-run-a-powershell-scriptquestion-from-%23sqlrelay/","title":"What Runs on the SQL Server when you run a PowerShell script?‚ÄìQuestion from #SQLRelay"},{"content":"\nTodays post is my first for the TSQL2sDay series. For those not familiar this is rotating blog party that was started by Adam Machanic (@AdamMachanic blog) back in 2009. If you want to catch up on all the fun to date check out this nice archive (link) put together by Steve Jones (@way0utwest blog). Thank you Steve!!!\nThis one is hosted by Jorge Segarra @SQLChicken:¬†who said This month‚Äôs topic is all about the cloud. What‚Äôs your take on it? Have you used it? If so, let‚Äôs hear your experiences. Haven‚Äôt used it? Let‚Äôs hear why or why not? Do you like/dislike recent changes made to cloud services? It‚Äôs clear skies for writing! So let‚Äôs hear it folks, where do you stand with the cloud?\nMy wife would tell you that my head is always in the cloud and she‚Äôs right (she usually is) just not like that picture! I would love to float gracefully above the land and gaze upon the view but its the landing that bothers me and will always stop me from trying it\nCredit http://owenrichardson.com/\nShe‚Äôs right, pedantically and literally too, because this year I have spent a lot of time with my head and my fingers and my thinking in Virtual Machines using Windows Azure. That is where I have learnt a lot of my SQL and Powershell this year. After SQL Saturday Exeter and SQL Bits in Nottingham this year I have needed a place to practice and learn, an environment to try things and break things and mend them again and experiment.\nI learn just as well by doing things as I do reading about them. Stuart Moore @napalmgram¬†has a great post called Learning to Play with SQL Server and whist I haven‚Äôt been as rough with my Azure SQL instances as he suggests I have been able to practice at will without worry and thanks to my MSDN subscription without cost. I have taken examples from blog posts and demos from User Group Sessions and run them on my Windows Azure VMs\nEvery single blog post I have written this year that has examples has been written in Azure and screen shots from Azure. Whilst some of my Powershell scripts in the PowerShell Box of Tricks series had already been written to solve one particular problem or another at MyWork, every single one was refined and demo‚Äôd and all the screen shots were from Azure and several were developed on Azure too\nMy first ever session to the SQL South West user group was about Spinning up and Shutting Down VMS in Azure was about Azure and was an interesting experience in Murphys Law which meant I ended up having to deliver it¬†on Azure.\nThe second time I have talked was about the PowerShell Box of Tricks series to the Cardiff User Group. Having learnt my lesson from the first time I had bought a mini HDMI to VGA converter and I had tested it using a couple of monitors at home and it worked wonderfully. However, when I got to Cardiff my little Asus convertible didn‚Äôt provide enough grunt to power the funky presentation screen. Luckily thanks to Stuart Moore @napalmgram who was also there doing his excellent PowerShell Back Up and Restore Session who let me use his Mac I was able to deliver the session using Office Web App to run the PowerPoint from my SkyDrive whilst all the demos were on ‚Ä¶‚Ä¶‚Ä¶Yup you guessed it Windows Azure !!!\nSo I feel qualified to answer Jorge‚Äôs questions and take part in T-SQL Tuesday this time round.\nI like Azure. I like the ease I can spin up and down machines or any PaaS services at will. I love that I can do it with PowerShell because I really enjoy using PowerShell in my day to day work and at home too. Living as I do in a beautifully convenient bungalow in the country, I still enjoy the frustration of watching that spinning ring as my videos buffer on our 1.8Mbs at best internet connection. Whilst that does have an impact on using Azure it is a damn sight better than waiting many days trying to download one single file. Something like an ISO file for the latest SQL Server CTP for example.\nThere is no way I would have got a look at SQL Server 2014 if it wasn‚Äôt for Azure. I was able to spin up a SQL Server 2014 machine in only a few minutes and log in and have a play and then delete it. I have done the same with Server 2012 and 2012 R2. It has enabled me to try setting up Availability Groups and other technologies not yet implemented at MyWork\nI wouldn‚Äôt have been able to do any of that on my machines at home as I don‚Äôt have anything capable of running Hyper-V whilst this 8 year old desktop still keeps hanging on despite the odd noises. (Negotiations are currently in place to replace it with something shiny and new. Just need that lottery win now !!)\nI have also transferred my Cricket Averages database to WASD and am talking with a friend of mine about developing an app that will use the mobile service as well.\nThe rate of change is much quicker in the cloud, things change and change quickly. As quickly as I had written my post about Spinning up and Shutting Down VMS in Azure Microsoft changed the rules and didn‚Äôt charge for machines that were turned off. New services appear all the time. New services move quickly through from Preview to release and as Grant Fritchey noticed this week new views have been added to to Windows Azure SQL Database under the covers. I think this is something we are just going to have to live with. The scale of the cloud means it is much easier to test improvements at large scale and that means they can be released quicker.¬†It makes it more challenging to keep up I admit but it‚Äôs a constant drip of new things rather than a big bang all at once.\nAzure has brought me to where I am today and I think it will continue to be part of my future. If I remember to submit my PowerShell session for SQL Saturday Exeter (Submit yours here) and it gets chosen then you will be able to see me there (if you register here) using Azure to give back to the SQL Community\n","date":"2013-11-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/tsql2sday-why-my-head-is-always-in-the-cloud/","title":"#TSQL2sDay Why My Head is Always in The Cloud"},{"content":"¬†Disclaimer ‚Äì I am on the committee organising the next SQL Saturday Exeter. To be kept up to date about SQL Saturday #269 in the South West, follow @SQLSatExeter and#SQLSatExeter on twitter and see details at the bottom. This post is about my experience at this years event.\nIn March this year the SQL South West User Group hosted SQL Saturday #194 in Exeter. I was a new member to the User Group having finally been able to join them for the first time in January. At that meeting Chris Testa O‚ÄôNeill presented a session and was very passionate about the SQL Community and the benefit of the SQL Saturdays and other events. I am always keen to learn new things and find ways of developing my skills. As I haven‚Äôt won the lottery I also look out for good deals as well!!\nSQL SATURDAY PRE-CONS ARE EXCEPTIONAL VALUE It was relatively easy to persuade my bosses to pay for my pre-con. For ¬£150 I was able to spend a whole day in a room with about a dozen people being trained in SQL Server Security by Denny Cherry @mrdenny. The conversation went along the lines of\n‚ÄúI want to go to this training session being delivered by this guy. Link to MVP page. It‚Äôs ¬£150 and is in Exeter so no other costs required‚Äù\nMy boss ‚Äì ‚ÄúOK‚Äù\nOf course there was a little more fun and games to be had with the payment but it was easy for me to get training sorted and ¬£150 is not going to break the training budget.\nLooking back through my notes from the session today I realise quite how much I have taken from it into my role at work. I can‚Äôt really comment which and what though that wouldn‚Äôt be good security!!\nI remember an enjoyable day with plenty of technical learning, a lot of questions and answers and plenty of laughs as well. But more than that was the opportunity to mix with other professionals and talk with them. During the breaks and at lunch there were plenty of opportunities to chew the fat, learn how others do things, make new friends and put faces to twitter handles. (NOTE : I do look pretty much like my twitter profile picture so if you see me at SQL Community events I expect you to come up and say hi, that\u0026rsquo;s part of the benefit of attending these events, having a good natter)\nTake a look at the end of this post for details of 2014 Pre-Cons\nSQL SATURDAY ‚Äì CAN‚ÄôT GET CHEAPER THAN FREE SQL Saturdays are FREE\nSQL Saturdays offer sessions from internationally renowned and local SQL speakers on subjects relevant to you and your job, your future career, your development plan or just to challenge yourself by learning about something outside of your comfort zone. For Nothing. Add in the networking opportunities, the prizes from the sponsors, (if you were at Exeter this year the beer and the pasty) and if you added it up its a sizeable investment in yourself, your career and your development (did I mention a free beer and pasty?)\nNOT BAD FOR FREE!!\nTo enable that, SQL Saturday organisers have to go out and talk sponsors into putting their hands into their pockets. They will only do that if it is worthwhile to them. You can make it easier for the organisers by going and spending time with the sponsors during the breaks, chatting with them and giving them your details. Also, if you choose to use one of their products please tell the sponsors you spoke to them at a SQL Saturday. They are (usually) data professionals who will record that and use that to make future decisions which will we hope include sponsoring SQL Saturdays.\nThis year on the Saturday I went to the following sessions\nA temporary fix for a short term problem by Ian Meade Advanced SQL Server 2012 HA and DR Architectures by Christian Bolton Busting common T-SQL myths by Dave Morrison Power View and the Cube by R√©gis Baccaro Natural Born Killers, performance issues to avoid by Richard Douglas Tracking server performance without slowing it down by Jonathan Allen which I also Room Monitored Increasing Business and IT collaboration by Chris Testa-O\u0026rsquo;Neill\nIt was a really good day. I learnt so much from all those knowledgeable and talented people. It really kicked me on in my development at work. I was able to take from each of those sessions and use that knowledge to do my job better and I made new friends and new contacts. Just going back to my notes today has reminded me of something that I need to look into for work. Some of the conversations I have had at events this year have been fascinating - learning how other people do the same thing you do in a completely different but equally valid way,¬†problem-solving with a different set and type of minds than the ones at MyWork, laughing at the same things and moaning about similar frustrations. All have been both entertaining and rewarding and I think are worth mentioning as things I enjoyed about going to SQL Community events this year and play a part in the reason I shall continue to go to them (Just hope my boss doesn\u0026rsquo;t read this and think he won‚Äôt have to pay as I will go anyway!)\nIt‚Äôs busy and hectic, the sessions come along thick and fast and there are lots of people around to talk to. I wish I had made use of the SQL Saturday mobile phone app and I definitely recommend researching ahead of time and planning your day out.\nThis years sessions have not been decided yet but I have seen some of the submissions and there are some fabulous sessions there. You could also submit a session yourself. Choosing the sessions will be tough, but we want to offer the opportunity to speak to as many people as possible both new and experienced speakers.\nYou can submit your sessions at this link http://www.sqlsaturday.com/269/callforspeakers.aspx\nROUND-UP SQL SATURDAY EXETER WHY WOULDN‚ÄôT YOU COME For a newbie, as I was last time, SQL Saturday Exeter was a revelation.\nAn opportunity to learn without spending thousands of my own or MyWorks money to sit in a lecture room and listen to a trainer.\nA chance to develop my understanding in a friendly environment amongst my peers where I could ask questions.\nA place to meet new people and build relationships who have helped me with situations at work throughout the year. I reckon I\u0026rsquo;m in credit already\nThis year I have attended SQL Bits and SQL Saturday Cambridge and this month I shall be at SQL Relay in Cardiff and in Bristol. That all started with SQL Saturday 194 in Exeter 2013\nWHAT ABOUT NEXT YEARS SQL SATURDAY EXETER? Next years SQL Saturday in Exeter, SQL Saturday #269, will be held at the same place - Jury‚Äôs Inn Hotel Exeter on March 21/22nd 2014.\nWe had such amazing submissions for our pre-cons that we have had to find more rooms to be able to fit them all in.. You can see for yourself the quality of the sessions and speakers for SQL Saturday Exeter 2014 at the following link\nhttp://sqlsouthwest.co.uk/sql-saturday-269-precon-training-day-details/\nWhat do you think? I want to split myself into 8 and go to every one!\nWHAT SHOULD YOU DO NOW? I suggest that you should book Saturday 22nd March 2014 out in your calendar right this minute. Done that? Good.\nNow go to this link\nhttp://www.sqlsaturday.com/269/\nand register for FREE to attend and let us know @SQLSatExeter\nNext make yourself a coffee (Other beverages are available) and head to the pre-con page\nhttp://sqlsouthwest.co.uk/sql-saturday-269-precon-training-day-details/\nThis bit is up to you, the choice is hard. I can‚Äôt tell you which one of our eight fabulous sessions you want to go to. It‚Äôs not for me to say which amazing speaker you want to spend a day with for a bargain price but if you need further info please get in touch and we will try and help. Unfortunately our human cloning experiment is not stable enough to allow you to go to more than one!\nThen, let me know you have done so and come and say hi when you are here.\n","date":"2013-11-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sql-saturday-exeterndashwhatrsquos-the-point-my-experience-of-2013-sqlsatexeter/","title":"SQL Saturday Exeter\u0026ndash;What\u0026rsquo;s the Point? My Experience of 2013 SQLSatExeter"},{"content":"So this morning I decided I was going to run through this blog post on understanding query plans http://sqlmag.com/t-sql/understanding-query-plans. I logged into my Azure Portal to check my balance and clicked start on the machine and then immediately clicked connect.\nD‚Äôoh\nOf course the RDP session wouldn‚Äôt open as the machine was not up so I went and made a coffee. Whilst doing that I thought of a way of doing it with PowerShell\nA little Do Until loop on the PowerState Property üôÇ\nOf course if I was doing it all though PowerShell I would have done this\n","date":"2013-10-26T00:00:00Z","permalink":"https://blog.robsewell.com/blog/launching-azure-vm-after-starting-with-powershell/","title":"Launching Azure VM After Starting With PowerShell"},{"content":"Yesterdays Post Show-WindowsUpdatesLocal does enable you to search for an installed update as follows\nShow-WindowsUpdatesLocal|Where-Object {$_.HotFixID -eq ‚ÄòKB2855336‚Äô} |Select Date, HotfixID, Result,Title|Format-Table ‚ÄìAutoSize\rI thought I would be able to do it quicker especially if I was searching a server with a lot of updates so I thought I would create a function to answer the¬†question Is this update installed on that server\nIt is very similar to Show-WindowsUpdatesLocal but does not include the Title or Description on the grounds that if you are searching for it you should know those!!\nIt also only adds the output to the collection if the KB is in the HotFixID property as shown below\nIf we use Measure-Command to compare the two we can see\nFrom 3.89 seconds on my poor overworked machine to 1.79 seconds üôÇ\nYou can find the code here\n############################################################# ########\r#\r# NAME: Search-WindowsUpdatesLocal.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:22/09/2013\r#\r# COMMENTS: Load function to show search for windows updates by KB locally\r#\r# USAGE: Search-WindowsUpdatesLocal KB2792100|Format-Table -AutoSize -Wrap\r# Function Search-WindowsUpdatesLocal ([String] $Search) {\r$Search = $Search + \u0026quot;\\d*\u0026quot; $Searcher = New-Object -comobject Microsoft.Update. Searcher\r$History = $Searcher.GetTotalHistoryCount()\r$Updates = $Searcher.QueryHistory(1, $History)\r# Define a new array to gather output\r$OutputCollection = @()\rForeach ($update in $Updates) {\r$Result = $null\rSwitch ($update.ResultCode) {\r0 { $Result = 'NotStarted'}\r1 { $Result = 'InProgress' }\r2 { $Result = 'Succeeded' }\r3 { $Result = 'SucceededWithErrors' }\r4 { $Result = 'Failed' }\r5 { $Result = 'Aborted' }\rdefault { $Result = $_ }\r}\r$string = $update.title\r$SearchAnswer = $string | Select-String -Pattern $Search | Select-Object { $_.Matches } $output = New-Object -TypeName PSobject\r$output | add-member NoteProperty ‚ÄúDate‚Äù -value $Update.Date\r$output | add-member NoteProperty ‚ÄúHotFixID‚Äù -value $SearchAnswer.‚Äò $_.Matches ‚Äò.Value\r$output | Add-Member NoteProperty \u0026quot;Result\u0026quot; -Value $Result\rif ($output.HotFixID) {\r$OutputCollection += $output\r}\r}\r$OutputCollection\r}\r","date":"2013-09-30T00:00:00Z","permalink":"https://blog.robsewell.com/blog/searching-for-installed-windows-update-with-powershell/","title":"Searching for Installed Windows Update With PowerShell"},{"content":"I wanted to be able to quickly show the Windows Updates on a server. This came about during a discussion about auditing.\nOf course, there is no point in re-inventing the wheel so I had a quick Google and¬†found a couple of posts on from¬†Hey Scripting Guy blog and one from Tim Minter. Neither quite did what I wanted so I modified them as follows.\nWe start by creating a Update object and find the total number of updates and setting them to a variable $History which we pass to the QueryHistory Method. This enables us to show all the updates\nPassing this to Get-Member shows\nwhich doesn‚Äôt show the KB so I read a bit more and found Tom Arbuthnot‚Äôs Blog Post\nthis transforms the ResultCode Property to something meaningful and places the KB in its own column.\nI have created a function called Show-WindowsUpdatesLocal It‚Äôs Local because doing it for a remote server takes a different approach but I will show that another day.\nThis means you can call the function and use the results however you like\nShow-WindowsUpdatesLocal\rShow-WindowsUpdatesLocal| Select Date, HotfixID, Result|Format-Table -AutoSize\rShow-WindowsUpdatesLocal|Where-Object {$_.Result -eq ‚ÄòFailed‚Äô} |Select Date, HotfixID, Result,Title|Format-Table -AutoSize\rOutput to file\nShow-WindowsUpdatesLocal|Format-Table -AutoSize|Out-File c:\\temp\\updates.txt\rOutput to CSV\nShow-WindowsUpdatesLocal|Export-Csv c:\\temp\\updates.csv\rYou can get the code here\n############################################################# #########\r#\r# NAME: Show-WindowsUpdatesLocal.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:22/09/2013\r#\r# COMMENTS: Load function to show all windows updates locally\r#\r# USAGE: Show-WindowsUpdatesLocal\r# Show-WindowsUpdatesLocal| Select Date, HotfixID, Result|Format-Table -AutoSize\r# Show-WindowsUpdatesLocal|Where-Object {$_.Result -eq 'Failed'} |Select Date, HotfixID, Result,Title| Format-Table -AutoSize\r# Show-WindowsUpdatesLocal|Format-Table -AutoSize| Out-File c:\\temp\\updates.txt\r# Show-WindowsUpdatesLocal|Export-Csv c:\\temp\\updates.csv\r# Function Show-WindowsUpdatesLocal {\r$Searcher = New-Object -ComObject Microsoft.Update. Searcher\r$History = $Searcher.GetTotalHistoryCount()\r$Updates = $Searcher.QueryHistory(1, $History)\r# Define a new array to gather output\r$OutputCollection = @() Foreach ($update in $Updates) {\r$Result = $null\rSwitch ($update.ResultCode) {\r0 { $Result = 'NotStarted'}\r1 { $Result = 'InProgress' }\r2 { $Result = 'Succeeded' }\r3 { $Result = 'SucceededWithErrors' }\r4 { $Result = 'Failed' }\r5 { $Result = 'Aborted' }\rdefault { $Result = $_ }\r}\r$string = $update.title\r$Regex = ‚ÄúKB\\d*‚Äù\r$KB = $string | Select-String -Pattern $regex | Select-Object { $_.Matches }\r$output = New-Object -TypeName PSobject\r$output | add-member NoteProperty ‚ÄúDate‚Äù -value $Update.Date\r$output | add-member NoteProperty ‚ÄúHotFixID‚Äù -value $KB.‚Äò $_.Matches ‚Äò.Value\r$output | Add-Member NoteProperty \u0026quot;Result\u0026quot; -Value $Result\r$output | add-member NoteProperty ‚ÄúTitle‚Äù -value $string\r$output | add-member NoteProperty ‚ÄúDescription‚Äù -value $update.Description\r$OutputCollection += $output\r}\r$OutputCollection\r}\r","date":"2013-09-29T00:00:00Z","permalink":"https://blog.robsewell.com/blog/show-windows-updates-locally-with-powershell/","title":"Show Windows Updates Locally With PowerShell"},{"content":"Whilst writing my PowerShell Box of Tricks GUI I realised that I had hard-coded the path to the sqlservers.txt file in several functions and I wanted one place where I could set this. At the top of the GUI script I added a variable and in the ReadMe explained this needed to be set but I needed to change it in all of the functions where it was referenced.\nThe Hey Scripting Guy Blog came to the rescue\nOnly four entries so i did them manually but You can also use PowerShell to replace the entries.\n","date":"2013-09-25T00:00:00Z","permalink":"https://blog.robsewell.com/blog/finding-text-in-all-files-in-a-folder-with-powershell/","title":"Finding Text In All Files In A Folder With PowerShell"},{"content":"When I started as a DBA at MyWork I faced a challenge. Many hundreds of databases, dozens of servers and no idea what was on where. It was remembering this situation when new team members were appointed that lead me to write the Find-Database script and I had written a simple GUI using Read-Host to enable the newbies to see the functions I had created\nWhilst writing this series of posts I decided that I would create a new GUI\nI wanted the choice to be made and then the form to close so I had to use a separate function for calling all the functions referenced in the form. This function takes an input $x and depending on the value runs a particular code block. Inside the code block I ask some questions using Read-Host to set the variables, load the function and run it as shown below for Show-DriveSizes\nThen I set about creating the GUI. First we load the Forms Assembly, create a new Form object and add a title\nThen using the details found here I I converted the image to ASCI and use it as the background image and set the size of the Form\nI choose a default font for the form. Note there are many many properties that you can set for all of these objects so use your best learning aid and find the ones you need.\nI then create three labels. I will show one. I think the code is self-explanatory and you will be able to see what is going on. Don‚Äôt forget to the last line though! That adds it to the form, if you miss it you can spend a few minutes scratching your head wondering why it hasn‚Äôt appeared!!!\nWe need a Text Box for the User to put their choice in. Again the code is fairly easy to understand\nThe next bit of code enables the user to use Enter and Escape keys to Go or to Quit. Notice that both call the Close() method to close the Form and return to the PowerShell console\nAdd a button for OK and one for quit\nand finally Activate the Form, Show it and run the function to call the correct function\nThe Return-Answer function simply calls the Return-Function function. I am not sure if that is the best way of doing it but it works in the way i wanted it to\n","date":"2013-09-24T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2013/09/image86.png","permalink":"https://blog.robsewell.com/blog/the-powershell-box-of-tricks-gui/","title":"The PowerShell Box Of Tricks GUI"},{"content":"There is a newer up to date version of this post here using the dbatools module and the sqlserver module\nBut if you want to continue with this way read on!!\nHaving created Windows Users or SQL Users using the last two days posts, today we shall add them to a role on a database.\nAs I discussed previously I believe that to follow good practice I try to ensure that database permissions are granted by role membership and each role is created with the minimum amount of permissions required for successful execution of the task involved.\nSo with each database having the correct roles created and the users created we just need to add the user to the database and to the role. This is easily done with PowerShell.\nThe Add-UserToRole function takes four parameters Server,Database,User and Role and does a series of error checks.\nWith these functions you can easily create a number of Users and add them to database roles quickly and easily and repeatedly.\nIf the test team come to you and require 10 Test Users and 3 Test Administrators adding to the test database. I create 2 notepad files\nand use them with the Add-SQLAccountToSQLRole and Add-UserToRole functions to create the users\nHere are the results in PowerShell\nand in SSMS\nThe Code is here\n############################################################# ################################\r#\r# NAME: Add-UserToRole.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:11/09/2013\r#\r# COMMENTS: Load function to add user or group to a role on a database\r#\r# USAGE: Add-UserToRole fade2black Aerosmith Test db_owner\r# Function Add-UserToRole ([string] $server, [String] $Database , [string]$User, [string]$Role)\r{\r$Svr = New-Object ('Microsoft.SqlServer.Management.Smo. Server') $server\r#Check Database Name entered correctly\r$db = $svr.Databases[$Database]\rif($db -eq $null)\r{\rWrite-Output \u0026quot; $Database is not a valid database on $Server\u0026quot;\rWrite-Output \u0026quot; Databases on $Server are :\u0026quot;\r$svr.Databases|select name\rbreak\r}\r#Check Role exists on Database\r$Rol = $db.Roles[$Role]\rif($Rol -eq $null)\r{\rWrite-Output \u0026quot; $Role is not a valid Role on $Database on $Server \u0026quot;\rWrite-Output \u0026quot; Roles on $Database are:\u0026quot;\r$db.roles|select name\rbreak\r}\rif(!($svr.Logins.Contains($User)))\r{\rWrite-Output \u0026quot;$User not a login on $server create it first\u0026quot;\rbreak\r}\rif (!($db.Users.Contains($User)))\r{\r# Add user to database\r$usr = New-Object ('Microsoft.SqlServer.Management. Smo.User') ($db, $User)\r$usr.Login = $User\r$usr.Create()\r#Add User to the Role\r$Rol = $db.Roles[$Role]\r$Rol.AddMember($User)\rWrite-Output \u0026quot;$User was not a login on $Database on $server\u0026quot;\rWrite-Output \u0026quot;$User added to $Database on $Server and $Role Role\u0026quot;\r}\relse\r{\r#Add User to the Role\r$Rol = $db.Roles[$Role]\r$Rol.AddMember($User)\rWrite-Output \u0026quot;$User added to $Role Role in $Database on $Server \u0026quot;\r}\r}\r","date":"2013-09-23T00:00:00Z","permalink":"https://blog.robsewell.com/blog/add-user-to-sql-server-database-role-with-powershell-and-quickly-creating-test-users/","title":"Add User to SQL Server Database Role with PowerShell and Quickly Creating Test Users"},{"content":"Another post in the PowerShell Box of Tricks series.\nIn yesterdays post Creating a Windows User and Adding to SQL Role we created a Windows User, today it‚Äôs a SQL User. Again it is nice and simple and allows you to pipe input from other sources enabling you to easily and quickly repeat any process that needs SQL Users.\nIt is pretty similar as you would expect. We create a Login Object, set the Logintype to¬†SqlLogin add the Password and create it with the Create Method. It is then added to the Role Specified\nThe same error checking is performed as the Windows Login function. If the login already exists on the server it will just add it to the role and if the role has been mistyped it will let you know. It does this by checking if the role object is Null for the Roles and the Contains Method for the Logins\nThe function is called as follows.\nAdd-SQLAccountToSQLRole FADE2BLACK Test Password01 dbcreator\rThe code can be found here\n############################################################# ###########\r#\r# NAME: Add-SQLAccountToSQLRole.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:11/09/2013\r#\r# COMMENTS: Load function to create a sql user and add them to a server role\r#\r# USAGE: Add-SQLAccountToSQLRole FADE2BLACK Test Password01 dbcreator\r# Add-SQLAccountToSQLRole FADE2BLACK Test Password01 public\rFunction Add-SQLAccountToSQLRole ([String]$Server, [String] $User, [String]$Password, [String]$Role) {\r$Svr = New-Object ('Microsoft.SqlServer.Management.Smo. Server') $server\r# Check if Role entered Correctly\r$SVRRole = $svr.Roles[$Role]\rif ($SVRRole -eq $null) {\rWrite-Host \u0026quot; $Role is not a valid Role on $Server\u0026quot;\r}\relse {\r#Check if User already exists\rif ($svr.Logins.Contains($User)) {\r$SqlUser = New-Object -TypeName Microsoft. SqlServer.Management.Smo.Login $Server, $User\r$LoginName = $SQLUser.Name\rif ($Role -notcontains \u0026quot;public\u0026quot;) { $SVRRole.AddMember($LoginName)\r}\r}\relse {\r$SqlUser = New-Object -TypeName Microsoft. SqlServer.Management.Smo.Login $Server, $User\r$SqlUser.LoginType = 'SqlLogin'\r$SqlUser.PasswordExpirationEnabled = $false\r$SqlUser.Create($Password)\r$LoginName = $SQLUser.Name\rif ($Role -notcontains \u0026quot;public\u0026quot;) {\r$SVRRole.AddMember($LoginName)\r}\r}\r}\r}\r","date":"2013-09-21T00:00:00Z","permalink":"https://blog.robsewell.com/blog/creating-sql-user-and-adding-to-server-role-with-powershell/","title":"Creating SQL User and adding to Server Role with PowerShell"},{"content":" The function does some simple error checking. If the login already exists on the server it will just add it to the role and if the role has been mistyped it will let you know. It does this by checking if the Role object is Null for the Roles and the Contains Method for the Logins\nAdd-WindowsAccountToSQLRole FADE2BLACK ‚ÄòFADE2BLACK\\Test‚Äô public\r###########################################################\r#\r# NAME: Add-WindowsAccountToSQLRole.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:11/09/2013\r#\r# COMMENTS: Load function to create a windows user and add them to a server role\r#\r# USAGE: Add-WindowsAccountToSQLRole FADE2BLACK 'FADE2BLACK\\Test' dbcreator\r# Add-WindowsAccountToSQLRole FADE2BLACK 'FADE2BLACK\\Test' public\rFunction Add-WindowsAccountToSQLRole ([String]$Server, [String] $User, [String]$Role) {\r$Svr = New-Object ('Microsoft.SqlServer.Management.Smo. Server') $server\r# Check if Role entered Correctly\r$SVRRole = $svr.Roles[$Role]\rif ($SVRRole -eq $null) {\rWrite-Output \u0026quot; $Role is not a valid Role on $Server\u0026quot;\r}\relse {\r#Check if User already exists\rif ($svr.Logins.Contains($User)) {\r$SqlUser = New-Object -TypeName Microsoft. SqlServer.Management.Smo.Login $Server, $User\r$LoginName = $SQLUser.Name\rif ($Role -notcontains \u0026quot;public\u0026quot;) {\r$svrole = $svr.Roles | where {$_.Name -eq $Role}\r$svrole.AddMember(\u0026quot;$LoginName\u0026quot;)\r}\r}\relse {\r$SqlUser = New-Object -TypeName Microsoft. SqlServer.Management.Smo.Login $Server, $User\r$SqlUser.LoginType = 'WindowsUser'\r$SqlUser.Create()\r$LoginName = $SQLUser.Name\rif ($Role -notcontains \u0026quot;public\u0026quot;) {\r$svrole = $svr.Roles | where {$_.Name -eq $Role}\r$svrole.AddMember(\u0026quot;$LoginName\u0026quot;)\r}\r}\r}\r}\r","date":"2013-09-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/creating-a-windows-user-and-adding-to-a-sql-server-role-with-powershell/","title":"Creating a Windows User and adding to a SQL Server Role with PowerShell"},{"content":" I‚Äôll start by saying this is a bit of a cheat. PowerShell has a perfectly good cmdlet called Get-EventLog¬†and plenty of ways to use it\n#####################################################################\r#\r# NAME: Show-EventLog.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function for Showing the windows event logs on a server\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r# Define a server an event log the number of events and display\r# pipe to this and then to out-gridview to only show Errors - where {$_. entryType -match \u0026quot;Error\u0026quot;}\rFunction Show-EventLog ($Server, $log, $Latest) {\rGet-EventLog -computername $server -log $log -newest $latest | Out-GridView\r}\r","date":"2013-09-18T00:00:00Z","permalink":"https://blog.robsewell.com/blog/displaying-the-windows-event-log-with-powershell/","title":"Displaying the Windows Event Log with PowerShell"},{"content":" Create a Server Object and notice that there is a Method named EnumProcesses by piping it to Get-Member and then look at the Properties and Methods of EnumProcesses\n#######################################################################\r#\r# NAME: Show-SQLProcesses.ps1\r# AUTHOR: Rob Sewell http://sqldbawithabeard.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function for Showing Processes on a SQL Server\r####################################\rFunction Show-SQLProcesses ($SQLServer)\r{\r$server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\r$Server.EnumProcesses()|Select Spid,BlockingSpid, Login, Host,Status,Program, Command,Database,Cpu,MemUsage |Format-Table -wrap -auto\r$OUTPUT= [System.Windows.Forms.MessageBox]::Show(\u0026quot;Do you want to Kill a process?\u0026quot; , \u0026quot;Question\u0026quot; , 4) if ($OUTPUT -eq \u0026quot;YES\u0026quot; ) {\r$spid = Read-Host \u0026quot;Which SPID?\u0026quot;\r$Server.KillProcess($Spid)\r} else { }\r}\r","date":"2013-09-17T00:00:00Z","permalink":"https://blog.robsewell.com/blog/showing-and-killing-sql-server-processes-with-powershell/","title":"Showing and Killing SQL Server Processes with PowerShell"},{"content":" The Show-LastServerBackup function iterates through each database on the server and takes each of the three properties mentioned in yesterdays post. However this time I created an empty hash table and added each result to it as follows\nI created the hash table with @() and then assign each property to a variable inside the loop and add it to a temporary PSObject with some custom NoteProperties to fit the data\n############################################################################# ################\r#\r# NAME: Show-LastServerBackup.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function for Showing Last Backup of each database on a server\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Show-LastServerBackup ($SQLServer) {\r$server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\r$Results = @();\rforeach ($db in $server.Databases) {\r$DBName = $db.name\r$LastFull = $db.lastbackupdate\rif ($lastfull -eq '01 January 0001 00:00:00')\r{$LastFull = 'NEVER'}\r$LastDiff = $db.LastDifferentialBackupDate if ($lastdiff -eq '01 January 0001 00:00:00')\r{$Lastdiff = 'NEVER'}\r$lastLog = $db.LastLogBackupDate if ($lastlog -eq '01 January 0001 00:00:00')\r{$Lastlog = 'NEVER'}\r$TempResults = New-Object PSObject;\r$TempResults | Add-Member -MemberType NoteProperty -Name \u0026quot;Server\u0026quot; -Value $Server;\r$TempResults | Add-Member -MemberType NoteProperty -Name \u0026quot;Database\u0026quot; -Value $DBName;\r$TempResults | Add-Member -MemberType NoteProperty -Name \u0026quot;Last Full Backup\u0026quot; -Value $LastFull;\r$TempResults | Add-Member -MemberType NoteProperty -Name \u0026quot;Last Diff Backup\u0026quot; -Value $LastDiff;\r$TempResults | Add-Member -MemberType NoteProperty -Name \u0026quot;Last Log Backup\u0026quot; -Value $LastLog;\r$Results += $TempResults;\r}\r$Results\r}\r","date":"2013-09-15T00:00:00Z","permalink":"https://blog.robsewell.com/blog/show-the-last-backups-on-a-server-with-powershell/","title":"Show The Last Backups On A Server with PowerShell"},{"content":" ############################################################################# ################\r#\r# NAME: Show-LastServerBackup.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function for Showing Last Backup of each database on a server\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Show-LastDatabaseBackup ($SQLServer, $sqldatabase) {\r$server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\r$db = $server.Databases[$sqldatabase]\rWrite-Output \u0026quot;Last Full Backup\u0026quot;\r$LastFull = $db.lastbackupdate\rif ($lastfull -eq '01 January 0001 00:00:00')\r{$LastFull = 'NEVER'}\rWrite-Output $LastFull\rWrite-Output \u0026quot;Last Diff Backup\u0026quot;\r$LastDiff = $db.LastDifferentialBackupDate if ($lastdiff -eq '01 January 0001 00:00:00')\r{$Lastdiff = 'NEVER'}\rWrite-Output $Lastdiff\rWrite-Output \u0026quot;Last Log Backup\u0026quot; $lastLog = $db. LastLogBackupDate if ($lastlog -eq '01 January 0001 00:00:00')\r{$Lastlog = 'NEVER'}\rWrite-Output $lastlog\r}\r","date":"2013-09-14T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-for-a-database-backup-with-powershell/","title":"Checking For A Database Backup with PowerShell"},{"content":" ############################################################################# ################\r#\r# NAME: Search-SQLErrorLog.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:22/07/2013\r#\r# COMMENTS: Load function for Searching SQL Error Log and exporting and displaying to CSV\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Search-SQLErrorLog ([string] $SearchTerm , [string] $SQLServer) {\r$FileName = 'c:\\TEMP\\SQLLogSearch.csv'\r$Search = '*' + $SearchTerm + '*'\r$server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\r$server.ReadErrorLog(5)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |Export-Csv $FileName\r$server.ReadErrorLog(4)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |ConvertTo-Csv |Out-File $FileName -append\r$server.ReadErrorLog(3)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |ConvertTo-Csv |Out-File $FileName -append\r$server.ReadErrorLog(2)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |ConvertTo-Csv |Out-File $FileName -append\r$server.ReadErrorLog(1)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |ConvertTo-Csv |Out-File $FileName -append\r$server.ReadErrorLog(0)| Where-Object {$_.Text -like $Search} | Select LogDate, ProcessInfo, Text |ConvertTo-Csv |Out-File $FileName -append\rInvoke-Item $filename\r}\r","date":"2013-09-13T00:00:00Z","permalink":"https://blog.robsewell.com/blog/searching-the-sql-error-log-with-powershell/","title":"Searching the SQL Error Log with PowerShell"},{"content":"Another post in the PowerShell Box of Tricks series. Here is another script which I use to save me time and effort during my daily workload enabling me to spend more time on more important (to me) things!\nTodays question which I often get asked is What databases are on that server?\nThis is often a follow up to a question that requires the Find-Database script. It is often asked by support teams investigating issues. It can also be asked by developers checking the impact of other services on their DEV/UAT environments, by change managers investigating impact of changes, by service managers investigating the impact of downtime, when capacity planning for a new service and numerous other situations.\nA simple quick and easy question made simpler with this function which can also be called when creating documentation\nShow-DatabasesOnServer SERVERNAME and use the results as you need This only shows you the name but if you need more information about your databases then have a look and see what you require.\nUse `Get-Member` to see what is there. I ran the following code to count the number of Properties available for Databases (Using PowerShell V3 on SQL Server 2012 SP1 11.0.3350.0 )\n154 Properties that you can examine and that is just for databases:-)\nPicking out a few properties you could do something like this\nIf you want aliases for your column headings you will need to add a bit of code to the select.\nFor Example, maybe you want to Database Name as a heading and the Size in Gb (Its in Mb in the example above) You would need to create a hash table with a Label element and an Expression element. The Label is the column heading and the Expression can just be the data or a calculation on data.\nSo select Name becomes\nselect @{label=\u0026quot;Database Name\u0026quot;;Expression={$_.Name}}\rSelect @{label=\u0026quot;Size GB\u0026quot;;Expression={\u0026quot;{0:N3}\u0026quot; -f ($_.Size/1024)}}\r$srv.databases|select @{label=\u0026quot;Server\u0026quot;;Expression={$_.Parent.name}},` @{label=\u0026quot;Database Name\u0026quot;;Expression={$_.Name}}, Owner, Collation, CompatibilityLevel,` RecoveryModel, @{label=\u0026quot;Size GB\u0026quot;;Expression={\u0026quot;{0:N3}\u0026quot; -f ($_.Size/1024)}}|` Format-Table -Wrap ‚ÄìAutoSize\rand the results\nand here is the full code\n\u0026lt;#PSScriptInfo\r.VERSION 1.0\r.GUID 48bf0316-66c3-4253-9154-6fc5b28e482a\r.AUTHOR Rob Sewell\r.DESCRIPTION Returns Database Name and Size in MB for databases on a SQL server\r.COMPANYNAME .COPYRIGHT .TAGS SQL, Database, Databases, Size\r.LICENSEURI .PROJECTURI .ICONURI .EXTERNALMODULEDEPENDENCIES .REQUIREDSCRIPTS .EXTERNALSCRIPTDEPENDENCIES .RELEASENOTES\r#\u0026gt;\r\u0026lt;#\r.Synopsis\rReturns the databases on a SQL Server and their size\r.DESCRIPTION\rReturns Database Name and Size in MB for databases on a SQL server\r.EXAMPLE\rShow-DatabasesOnServer\rThis will return the user database names and sizes on the local machine default instance\r.EXAMPLE\rShow-DatabasesOnServer -Servers SERVER1\rThis will return the database names and sizes on SERVER1\r.EXAMPLE\rShow-DatabasesOnServer -Servers SERVER1 -IncludeSystemDatabases\rThis will return all of the database names and sizes on SERVER1 including system databases\r.EXAMPLE\rShow-DatabasesOnServer -Servers 'SERVER1','SERVER2\\INSTANCE'\rThis will return the user database names and sizes on SERVER1 and SERVER2\\INSTANCE\r.EXAMPLE\r$Servers = 'SERVER1','SERVER2','SERVER3'\rShow-DatabasesOnServer -Servers $servers|out-file c:\\temp\\dbsize.txt\rThis will get the user database names and sizes on SERVER1, SERVER2 and SERVER3 and export to a text file c:\\temp\\dbsize.txt\r.NOTES\rAUTHOR : Rob Sewell https://blog.robsewell.com\rInitial Release 22/07/2013\rUpdated with switch for system databases added assembly loading and error handling 20/12/2015\rSome tidying up and ping check 01/06/2016\r#\u0026gt;\rFunction Show-DatabasesOnServer {\r[CmdletBinding()]\rparam (\r# Server Name or array of Server Names - Defaults to $ENV:COMPUTERNAME\r[Parameter(Mandatory = $false, ValueFromPipeline = $true,\rValueFromPipelineByPropertyName = $true, Position = 0)]\r$Servers = $Env:COMPUTERNAME,\r# Switch to include System Databases\r[Parameter(Mandatory = $false)]\r[switch]$IncludeSystemDatabases\r)\r[void][reflection.assembly]::LoadWithPartialName( \u0026quot;Microsoft.SqlServer. Smo\u0026quot; );\rforeach ($Server in $Servers) {\rif ($Server.Contains('\\')) {\r$ServerName = $Server.Split('\\')[0]\r$Instance = $Server.Split('\\')[1]\r}\relse {\r$Servername = $Server\r} ## Check for connectivity\rif ((Test-Connection $ServerName -count 1 -Quiet) -eq $false) {\rWrite-Error \u0026quot;Could not connect to $ServerName - Server did not respond to ping\u0026quot;\r$_.Exception\rcontinue\r}\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $Server\rif ($IncludeSystemDatabases) {\rtry {\r$Return = $srv.databases| Select Name, Size\r}\rcatch {\rWrite-Error \u0026quot;Failed to get database information from $Server\u0026quot;\r$_.Exception\rcontinue\r}\r}\relse {\rtry {\r$Return = $srv.databases.Where{$_.IsSystemObject -eq $false} | Select Name, Size\r}\rcatch {\rWrite-Error \u0026quot;Failed to get database information from $Server\u0026quot;\r$_.Exception\rcontinue\r}\r}\rWrite-Output \u0026quot;`n The Databases on $Server and their Size in MB `n\u0026quot;\r$Return\r}\r}\r","date":"2013-09-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/list-databases-and-properties-on-sql-server-with-powershell/","title":"List Databases (and Properties) on SQL Server with PowerShell"},{"content":" Show-LatestSQLErrorLog fade2black|Out-File -FilePath c:\\temp\\log.txt\rInvoke-Item c:\\temp\\log.txt\r############################################################################# ################\r#\r# NAME: Show-Last24HoursSQLErrorLog.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:22/07/2013\r#\r# COMMENTS: Load function for reading last days current SQL Error Log for Server\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Show-Last24HoursSQLErrorLog ([string]$Server) { $srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server $logDate = (get-date).AddDays(-1)\r$Results = $srv.ReadErrorLog(0) |Where-Object {$_.LogDate -gt $logDate}| format-table -Wrap -AutoSize $Results }\r","date":"2013-09-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/reading-todays-sql-error-log-with-powershell/","title":"Reading Todays SQL Error Log With PowerShell"},{"content":"As you may have noticed, I love PowerShell!\nI have developed a series of functions over time which save me time and effort whilst still enabling me to provide a good service to my customers. I keep them all in a functions folder and call them whenever. I call it my PowerShell Box of Tricks\nI am going to write a short post about each one over the next few weeks as I write my presentation on the same subject which I will be presenting to SQL User Groups.\nTodays post is not about a question but about a routine task DBAs do. Dropping Logins\nWhilst best practice says add users to active directory groups, add the group to roles and give the roles the correct permissions there are many situations where this is not done and DBAs are required to manually remove logins. This can be a time consuming task but one that is essential. There was a time at MyWork when this was achieved via a script that identified which servers had a users login and the task was to connect to each server in SSMS and remove the user from each database and then drop the server login. As you can imagine it was not done diligently. Prior to an audit I was tasked with ensuring that users that had left MyWork did not have logins to any databases. It was this that lead to the Checking for SQL Logins script and to this one\nIt starts exactly the same as the Checking for SQL Logins script by grabbing the list of SQL Servers from the text file and creating an array of user names including all the domains as I work in a multi-domain environment\nThen iterate through each database ignoring those that may need special actions due to the application and call the drop method\nRepeat the process for the servers and send or save the report as required. Simple and easy and has undoubtedly saved me many hours compared to the previous way of doing things üôÇ\nIMPORTANT NOTE This script will not delete logins if they have granted permissions to other users. I always recommend running the Checking for SQL Logins script after running this script to ensure all logins have been dropped\nThis script can be found\n############################################################################# ################\r#\r# NAME: Drop-SQLUsers.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function to Display a list of server, database and login for SQL servers listed # in sqlservers.txt file and then drop the users\r#\r# I always recommend running the Checking for SQL Logins script after running this script to ensure all logins have been dropped\r#\r# Does NOT drop Users who have granted permissions\r#BE CAREFUL\rFunction Drop-SQLUsers ($Usr) {\r[System.Reflection.Assembly]::LoadWithPartialName('Microsoft.SqlServer. SMO') | out-null\r# Suppress Error messages - They will be displayed at the end\r$ErrorActionPreference = \u0026quot;SilentlyContinue\u0026quot;\r# cls\r# Pull a list of servers from a local text file\r$servers = Get-Content 'c:\\temp\\sqlservers.txt'\r# Create an array for the username and each domain slash username\r$logins = @(\u0026quot;DOMAIN1\\$usr\u0026quot;, \u0026quot;DOMAIN2\\$usr\u0026quot;, \u0026quot;DOMAIN3\\$usr\u0026quot; , \u0026quot;$usr\u0026quot;)\rWrite-Output \u0026quot;#################################\u0026quot;\rWrite-Output \u0026quot;Dropping Logins for $Logins\u0026quot; #loop through each server and each database and Write-Output \u0026quot;#########################################\u0026quot;\rWrite-Output \u0026quot;`n Database Logins`n\u0026quot; foreach ($server in $servers) { if (Test-Connection $Server -Count 1 -Quiet) {\t$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\r#drop database users\rforeach ($database in $srv.Databases) {\rif ($database -notlike \u0026quot;*dontwant*\u0026quot;) {\rforeach ($login in $logins) {\rif ($database.Users.Contains($login)) {\r$database.Users[$login].Drop();\rWrite-Output \u0026quot; $server , $database , $login - Database Login has been dropped\u0026quot; }\r}\r}\r}\r}\r}\rWrite-Output \u0026quot;`n#########################################\u0026quot;\rWrite-Output \u0026quot;`n Servers Logins`n\u0026quot; foreach ($server in $servers) { if (Test-Connection $Server -Count 1 -Quiet) {\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\r#drop server logins\rforeach ($login in $logins) {\rif ($srv.Logins.Contains($login)) { $srv.Logins[$login].Drop(); Write-Output \u0026quot; $server , $login Login has been dropped\u0026quot; }\r}\r}\r}\rWrite-Output \u0026quot;`n#########################################\u0026quot;\rWrite-Output \u0026quot;Dropping Database and Server Logins for $usr - Completed \u0026quot; Write-Output \u0026quot;If there are no logins displayed above then no logins were found or dropped!\u0026quot; Write-Output \u0026quot;###########################################\u0026quot; }\r","date":"2013-09-10T00:00:00Z","permalink":"https://blog.robsewell.com/blog/dropping-sql-users-with-powershell/","title":"Dropping SQL Users with PowerShell"},{"content":" Login domain\\user‚Äô has granted one or more permissions. Revoke the permission before dropping the login (Microsoft SQL Server, Error: 15173)\n$svrs = ## list of servers Get-Content from text fiel etc\rforeach ($svr in $svrs) {\r$server = New-Object Microsoft.SQLServer.Management.Smo.Server $svrs\rforeach ($endpoint in $server.Endpoints['Mirroring']) {\rif ($endpoint.Owner = 'Domain\\User') {\r$endpoint.Owner = 'Domain\\NEWUser'\r$endpoint.Alter()\r} }\r}\r","date":"2013-09-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/alter-sql-mirroring-endpoint-owner-with-powershell/","title":"Alter SQL Mirroring Endpoint Owner with Powershell"},{"content":"This morning I have been setting up my Azure Servers in preparation for my presentation to the Cardiff SQL User Group this month.\nI used my scripts from My Post on Spinning Up Azure SQL Boxes to create two servers and then I wanted to create some databases\nI decided it was time to write a Create-Database function using a number of scripts that I have used to create individual databases.\nErrors Whilst finalising the function I didn‚Äôt quite get it right sometimes and was faced with an error.\nNot the most useful of errors to troubleshoot. The issue could be anywhere in the script\nYou can view the last errors PowerShell has shown using $Errors. This gives you the last 500 errors but you can see the last error by using $Error[0] if you pipe it to Format-List you can get a more detailed error message so I added a try catch to the function which gave me an error message I could resolve.\nMuch better. The problem was\nCannot create file ‚ÄòC:\\Program Files\\Microsoft SQL Server\\MSSQL11.MSSQLSERVER\\MSSQL\\DATA\\.LDF‚Äô because it already exists.\nMistyping a variable has caused this. Creating an empty file name variable which then threw the error the second(and third,fourth fifth) times I ran the script but this error pointed me to it.\nCreating Database There are a vast number of variables you can set when creating a database. I decided to set File Sizes, File Growth Sizes, Max File Sizes and Recovery Model. I only set Server and Database Name as mandatory parameters and gave the other parameters default values\nWe take the parameters for file sizes in MB and set them to KB\nThen set the default file locations. Create a database object, a Primary file group object and add the file group object to the database object\nAdd a User File Group for User objects\nCreate a database file on the primary file group using the variables set earlier\nDo the same for the user file and then create a Log File\nSet the Recovery Model and create the database and then set the user file group as the default\nFinally catch the errors\nIt can then be called as follows\nCreate-Database SERVERNAME DATABASENAME\ror by setting all the parameters\nCreate-Database -Server Fade2black -DBName DatabaseTest -SysFileSize 10 -UserFileSize 15 -LogFileSize 20 -UserFileGrowth 7 -UserFileMaxSize 150 -LogFileGrowth 8 -LogFileMaxSize 250 -DBRecModel FULL\rThis means that I can easily and quickly set up several databases of different types and sizes\nThe script can be found here\n############################################################################# ################\r#\r# NAME: Create-Database.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:08/09/2013\r#\r# COMMENTS: Load function for creating a database\r# Only Server and DB Name are mandatory the rest will be set to small defaults\r#\r# USAGE: Create-Database -Server Fade2black -DBName Test35 -SysFileSize 10 -UserFileSize 15 -LogFileSize 20\r# -UserFileGrowth 7 -UserFileMaxSize 150 -LogFileGrowth 8 -LogFileMaxSize 250 -DBRecModel FULL\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Create-Database {\rParam(\r[Parameter(Mandatory = $true)]\r[String]$Server ,\r[Parameter(Mandatory = $true)]\r[String]$DBName,\r[Parameter(Mandatory = $false)]\r[int]$SysFileSize = 5,\r[Parameter(Mandatory = $false)]\r[int]$UserFileSize = 25,\r[Parameter(Mandatory = $false)]\r[int]$LogFileSize = 25,\r[Parameter(Mandatory = $false)]\r[int]$UserFileGrowth = 5,\r[Parameter(Mandatory = $false)]\r[int]$UserFileMaxSize = 100,\r[Parameter(Mandatory = $false)]\r[int]$LogFileGrowth = 5,\r[Parameter(Mandatory = $false)]\r$LogFileMaxSize = 100,\r[Parameter(Mandatory = $false)]\r[String]$DBRecModel = 'FULL'\r)\rtry {\r# Set server object\r$srv = New-Object ('Microsoft.SqlServer.Management.SMO.Server') $server\r$DB = $srv.Databases[$DBName]\r# Define the variables\r# Set the file sizes (sizes are in KB, so multiply here to MB)\r$SysFileSize = [double]($SysFileSize * 1024.0)\r$UserFileSize = [double] ($UserFileSize * 1024.0)\r$LogFileSize = [double] ($LogFileSize * 1024.0)\r$UserFileGrowth = [double] ($UserFileGrowth * 1024.0)\r$UserFileMaxSize = [double] ($UserFileMaxSize * 1024.0)\r$LogFileGrowth = [double] ($LogFileGrowth * 1024.0)\r$LogFileMaxSize = [double] ($LogFileMaxSize * 1024.0)\rWrite-Output \u0026quot;Creating database: $DBName\u0026quot;\r# Set the Default File Locations\r$DefaultDataLoc = $srv.Settings.DefaultFile\r$DefaultLogLoc = $srv.Settings.DefaultLog\r# If these are not set, then use the location of the master db mdf/ ldf\rif ($DefaultDataLoc.Length -EQ 0) {$DefaultDataLoc = $srv. Information.MasterDBPath}\rif ($DefaultLogLoc.Length -EQ 0) {$DefaultLogLoc = $srv.Information. MasterDBLogPath}\r# new database object\r$DB = New-Object ('Microsoft.SqlServer.Management.SMO.Database') ($srv, $DBName)\r# new filegroup object\r$PrimaryFG = New-Object ('Microsoft.SqlServer.Management.SMO. FileGroup') ($DB, 'PRIMARY')\r# Add the filegroup object to the database object\r$DB.FileGroups.Add($PrimaryFG )\r# Best practice is to separate the system objects from the user objects.\r# So create a seperate User File Group\r$UserFG = New-Object ('Microsoft.SqlServer.Management.SMO. FileGroup') ($DB, 'UserFG')\r$DB.FileGroups.Add($UserFG)\r# Create the database files\r# First, create a data file on the primary filegroup.\r$SystemFileName = $DBName + \u0026quot;_System\u0026quot;\r$SysFile = New-Object ('Microsoft.SqlServer.Management.SMO. DataFile') ($PrimaryFG , $SystemFileName)\r$PrimaryFG.Files.Add($SysFile)\r$SysFile.FileName = $DefaultDataLoc + $SystemFileName + \u0026quot;.MDF\u0026quot;\r$SysFile.Size = $SysFileSize\r$SysFile.GrowthType = \u0026quot;None\u0026quot;\r$SysFile.IsPrimaryFile = 'True'\r# Now create the data file for the user objects\r$UserFileName = $DBName + \u0026quot;_User\u0026quot;\r$UserFile = New-Object ('Microsoft.SqlServer.Management.SMO. Datafile') ($UserFG, $UserFileName)\r$UserFG.Files.Add($UserFile)\r$UserFile.FileName = $DefaultDataLoc + $UserFileName + \u0026quot;.NDF\u0026quot;\r$UserFile.Size = $UserFileSize\r$UserFile.GrowthType = \u0026quot;KB\u0026quot;\r$UserFile.Growth = $UserFileGrowth\r$UserFile.MaxSize = $UserFileMaxSize\r# Create a log file for this database\r$LogFileName = $DBName + \u0026quot;_Log\u0026quot;\r$LogFile = New-Object ('Microsoft.SqlServer.Management.SMO.LogFile') ($DB, $LogFileName)\r$DB.LogFiles.Add($LogFile)\r$LogFile.FileName = $DefaultLogLoc + $LogFileName + \u0026quot;.LDF\u0026quot;\r$LogFile.Size = $LogFileSize\r$LogFile.GrowthType = \u0026quot;KB\u0026quot;\r$LogFile.Growth = $LogFileGrowth\r$LogFile.MaxSize = $LogFileMaxSize\r#Set the Recovery Model\r$DB.RecoveryModel = $DBRecModel\r#Create the database\r$DB.Create()\r#Make the user filegroup the default\r$UserFG = $DB.FileGroups['UserFG']\r$UserFG.IsDefault = $true\r$UserFG.Alter()\r$DB.Alter()\rWrite-Output \u0026quot; $DBName Created\u0026quot;\rWrite-Output \u0026quot;System File\u0026quot;\r$SysFile| Select Name, FileName, Size, MaxSize, GrowthType| Format-List\rWrite-Output \u0026quot;User File\u0026quot;\r$UserFile| Select Name, FileName, Size, MaxSize, GrowthType, Growth| Format-List\rWrite-Output \u0026quot;LogFile\u0026quot;\r$LogFile| Select Name, FileName, Size, MaxSize, GrowthType, Growth| Format-List\rWrite-Output \u0026quot;Recovery Model\u0026quot;\r$DB.RecoveryModel\r}\rCatch {\r$error[0] | fl * -force\r}\r}\r","date":"2013-09-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/creating-sql-server-database-with-powershell/","title":"Creating SQL Server Database with PowerShell"},{"content":" I create an empty hash table and then populate it with the results\nSet a results variable to the names from the hash table and count the number of records\nand call it like this\nNote that the search uses the contains method so no need for wildcards\nResults come out like this\n############################################################################# ################\r#\r# NAME: Find-Database.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:22/07/2013\r#\r# COMMENTS: Load function for finding a database\r# USAGE: Find-Database DBName\r##################################\rFunction Find-Database ([string]$Search) {\r[System.Reflection.Assembly]::LoadWithPartialName('Microsoft.SqlServer. SMO') | out-null\r# Pull a list of servers from a local text file\r$servers = Get-Content 'sqlservers.txt'\r#Create an empty Hash Table\r$ht = @{}\r$b = 0\r#Convert Search to Lower Case\r$DatabaseNameSearch = $search.ToLower() Write-Output \u0026quot;#################################\u0026quot;\rWrite-Output \u0026quot;Searching for $DatabaseNameSearch \u0026quot; Write-Output \u0026quot;#################################\u0026quot; #loop through each server and check database name against input\rforeach ($server in $servers) {\rif (Test-Connection $Server -Count 1 -Quiet) {\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\rforeach ($database in $srv.Databases) {\r$databaseName = $database.Name.ToLower()\rif ($databaseName.Contains($DatabaseNameSearch)) {\r$DatabaseNameResult = $database.name\r$Key = \u0026quot;$Server -- $DatabaseNameResult\u0026quot;\r$ht.add($Key , $b)\r$b = $b + 1\r}\r} }\r}\r$Results = $ht.GetEnumerator() | Sort-Object Name|Select Name\r$Resultscount = $ht.Count\rif ($Resultscount -gt 0) {\rWrite-Output \u0026quot;############### I Found It!! #################\u0026quot;\rforeach ($R in $Results) {\rWrite-Output $R.Name }\r}\rElse {\rWrite-Output \u0026quot;############ I am really sorry. I cannot find\u0026quot; $DatabaseNameSearch \u0026quot;Anywhere ##################### \u0026quot;\r} }\r","date":"2013-09-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/using-powershell-to-find-a-database-amongst-hundreds/","title":"Using PowerShell to find a database amongst hundreds"},{"content":" and here are the results from my Azure VM. (See My previous posts on how to create your own Azure VMs with PowerShell)\n#############################################################################\r#\r# NAME: Show-DriveSizes.ps1\r# AUTHOR: Rob Sewell http://sqldbawiththebeard.com\r# DATE:22/07/2013\r#\r# COMMENTS: Load function for displaying drivesizes\r# USAGE: Show-DriveSizes server1\r###########################################\rFunction Show-DriveSizes ([string]$Server) {\r$Date = Get-Date\rWrite-Host -foregroundcolor DarkBlue -backgroundcolor yellow \u0026quot;$Server - - $Date\u0026quot;\r#interogate wmi service and return disk information\r$disks = Get-WmiObject -Class Win32_logicaldisk -Filter \u0026quot;Drivetype=3\u0026quot; -ComputerName $Server\r$diskData = $disks | Select DeviceID, VolumeName , # select size in Gbs as int and label it SizeGb\r@{Name = \u0026quot;SizeGB\u0026quot;; Expression = {$_.size / 1GB -as [int]}},\r# select freespace in Gbs and label it FreeGb and two deciaml places\r@{Name = \u0026quot;FreeGB\u0026quot;; Expression = {\u0026quot;{0:N2}\u0026quot; -f ($_.Freespace / 1GB)}},\r# select freespace as percentage two deciaml places and label it PercentFree @{Name = \u0026quot;PercentFree\u0026quot;; Expression = {\u0026quot;{0:P2}\u0026quot; -f ($_.Freespace / $_. Size)}}\r$diskdata } ","date":"2013-09-06T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-drive-sizes-with-powershell/","title":"Checking Drive Sizes with PowerShell"},{"content":" $server | Get-Member\r$Server.JobServer|gm\r$Server.JobServer.Operators | gm\r############################################################################# ################\r#\r# NAME: Show-SQLServerOperators.ps1\r# AUTHOR: Rob Sewell https://blog.robsewell.com\r# DATE:03/09/2013\r#\r# COMMENTS: Load function for Enumerating Operators and Notifications\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Show-SQLServerOperators ($SQLServer) {\rWrite-Output \u0026quot;############### $SQLServer ##########################\u0026quot;\rWrite-Output \u0026quot;#####################################################`n\u0026quot; $server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\rforeach ($Operator in $server.JobServer.Operators) {\r$Operator = New-Object (\u0026quot;$SMO.Agent.Operator\u0026quot;) ($server.JobServer, $Operator)\r$OpName = $Operator.Name\rWrite-Output \u0026quot;Operator $OpName\u0026quot;\rWrite-Output \u0026quot;`n###### Job Notifications ######\u0026quot;\r$Operator.EnumJobNotifications()| Select JobName | Format-Table\rWrite-Output \u0026quot;#####################################################`n\u0026quot; Write-Output \u0026quot;`n###### Alert Notifications #######\u0026quot;\r$Operator.EnumNotifications() | Select AlertName | Format-Table\rWrite-Output \u0026quot;#####################################################`n\u0026quot; }\r} ","date":"2013-09-05T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sql-server-operators-and-notifications-with-powershell-strange-enumerate-issue-fixed-by-@napalmgram/","title":"SQL Server Operators and Notifications with Powershell ‚Äì Strange Enumerate issue fixed by @napalmgram"},{"content":" ############################################################################# ################\r#\r# NAME: Show-SQLServerPermissions.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function for Enumerating Server and Database Role permissions or object permissions\r#\r# USAGE Show-SQLServerPermissions Server1\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\rFunction Show-SQLServerPermissions ($SQLServer) {\r$server = new-object \u0026quot;Microsoft.SqlServer.Management.Smo.Server\u0026quot; $SQLServer\r$selected = \u0026quot;\u0026quot; $selected = Read-Host \u0026quot;Enter Selection 1.) Role Membership or 2.) Object Permissions\u0026quot;\rSwitch ($Selected) {\r1 {\rWrite-Host \u0026quot;#### Server Role Membership on $Server ############################################## `n`n\u0026quot;\rforeach ($Role in $Server.Roles) {\rif ($Role.EnumServerRoleMembers().count -ne 0) {\rWrite-Host \u0026quot;############### Server Role Membership for $role on $Server #########################`n\u0026quot; $Role.EnumServerRoleMembers()\r}\r}\rWrite-Host \u0026quot;################################################################ ######################\u0026quot; Write-Host \u0026quot;################################################################ ######################`n `n `n\u0026quot; foreach ($Database in $Server.Databases) {\rWrite-Host \u0026quot;`n#### $Database Permissions on $Server ###############################################`n\u0026quot; foreach ($role in $Database.Roles) {\rif ($Role.EnumMembers().count -ne 0) {\rWrite-Host \u0026quot;########### Database Role Permissions for $Database $Role on $Server ################`n\u0026quot;\r$Role.EnumMembers()\r}\r}\r}\r} 2 {\rWrite-Host \u0026quot;################## Object Permissions on $Server ################################`n\u0026quot;\rforeach ($Database in $Server.Databases) {\rWrite-Host \u0026quot;`n#### Object Permissions on $Database on $Server #################################`n\u0026quot;\rforeach ($user in $database.Users) {\rforeach ($databasePermission in $database. EnumDatabasePermissions($user.Name)) {\rWrite-Host $databasePermission.PermissionState $databasePermission.PermissionType \u0026quot;TO\u0026quot; $databasePermission.Grantee\r}\rforeach ($objectPermission in $database. EnumObjectPermissions($user.Name)) {\rWrite-Host $objectPermission.PermissionState $objectPermission.PermissionType \u0026quot;ON\u0026quot; $objectPermission.ObjectName \u0026quot;TO\u0026quot; $objectPermission. Grantee\r}\r}\r}\r}\r}\r}\r","date":"2013-09-04T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sql-login-object-permissions-via-powershell/","title":"SQL login object permissions via PowerShell"},{"content":" ############################################################################# ################\r#\r# NAME: Show-SQLUserPermissions.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:06/08/2013\r#\r# COMMENTS: Load function to Display the permissions a user has across the estate\r# NOTE - Will not show permissions granted through AD Group Membership\r# # USAGE Show-SQLUserPermissions DBAwithaBeard\rFunction Show-SQLUserPermissions ($user)\r{\r[System.Reflection.Assembly]::LoadWithPartialName('Microsoft.SqlServer.SMO') | out-null\r# Suppress Error messages - They will be displayed at the end\r$ErrorActionPreference = \u0026quot;SilentlyContinue\u0026quot;\r#cls\r$Query = @\u0026quot;\rSELECT IL.ServerName\rFROM [dbo].[InstanceList] IL\rWHERE NotContactable = 0\rAND Inactive = 0\rAND DatabaseEngine = 'Microsoft SQL Server'\r\u0026quot;@\rTry\r{\r$Results = (Invoke-Sqlcmd -ServerInstance HMDBS02 -Database DBADatabase -Query $query -ErrorAction Stop).ServerName\r}\rcatch\r{\rWrite-Error \u0026quot;Unable to Connect to the DBADatabase - Please Check\u0026quot;\r}\r# Create an array for the username and each domain slash username\r$logins = @(\u0026quot;DOMAIN1\\$user\u0026quot;,\u0026quot;DOMAIN3\\$user\u0026quot;, \u0026quot;DOMAIN4\\$user\u0026quot; ,\u0026quot;$user\u0026quot; )\rWrite-Output \u0026quot;#################################\u0026quot; Write-Output \u0026quot;Logins for `n $logins displayed below\u0026quot; Write-Output \u0026quot;################################# `n\u0026quot; #loop through each server and each database and display usernames, servers and databases\rWrite-Output \u0026quot; Server Logins\u0026quot;\rforeach($server in $Results)\r{\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\rforeach($login in $logins)\r{\rif($srv.Logins.Contains($login))\r{\rWrite-Output \u0026quot;`n $server , $login \u0026quot; foreach ($Role in $Srv.Roles)\r{\r$RoleMembers = $Role. EnumServerRoleMembers()\rif($RoleMembers -contains $login)\r{\rWrite-Output \u0026quot; $login is a member of $Role on $Server\u0026quot;\r}\r}\r}\relse\r{\r}\r}\r}\rWrite-Output \u0026quot;`n#########################################\u0026quot;\rWrite-Output \u0026quot;`n Database Logins\u0026quot; foreach($server in $servers)\r{\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server\rforeach($database in $srv.Databases)\r{\rforeach($login in $logins)\r{\rif($database.Users.Contains($login))\r{\rWrite-Output \u0026quot;`n $server , $database , $login \u0026quot; foreach($role in $Database.Roles)\r{\r$RoleMembers = $Role.EnumMembers()\rif($RoleMembers -contains $login)\r{\rWrite-Output \u0026quot; $login is a member of $Role Role on $Database on $Server\u0026quot;\r}\r}\r}\relse\r{\rcontinue\r} }\r}\r}\rWrite-Output \u0026quot;`n#########################################\u0026quot;\rWrite-Output \u0026quot;Finished - If there are no logins displayed above then no logins were found!\u0026quot; Write-Output \u0026quot;#########################################\u0026quot; }\r","date":"2013-09-02T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-sql-server-user-role-membership-with-powershell/","title":"Checking SQL Server User Role Membership with PowerShell"},{"content":"As some of you may know, I love PowerShell!\nI use it all the time in my daily job as a SQL DBA and at home whilst learning as well.\nNot only do I use PowerShell for automating tasks such as Daily Backup Checks, Drive Space Checks, Service Running Checks, File Space Checks, Failed Agent Job Checks, SQL Error Log Checks, DBCC Checks and more but also for those questions which come up daily and interfere with concentrating on a complex or time consuming task.\nI have developed a series of functions over time which save me time and effort whilst still enabling me to provide a good service to my customers. I keep them all in a functions folder and call them whenever I need them. I also have a very simple GUI which I have set up for my colleagues to enable them to easily answer simple questions quickly and easily which I will blog about later. I call it my PowerShell Box of Tricks\nI am going to write a short post about each one over the next few weeks as I write my presentation on the same subject which I will be presenting to SQL User Groups.\nTodays question which I often get asked is Which database does this account have access to?\nThis question can come from Support Desks when they are investigating a users issue, Developers when they are testing an application as well as audit activities. It is usually followed by what permissions do they have which is covered by my next blog post.\nI start by getting the list of servers from my text file and creating an array of logins for each domain as I work in a multi domain environment\nThen loop through each server and if the login exists write it out to the window.\nI then repeat this but loop through each database as well\nA little bit of formatting is added and then a quick easy report that can easily be copied to an email as required.\nTo call it simply load the function\nand get the results\nThe code is below\n\u0026lt;#\r.Synopsis\rA workflow to display users server and database logins across a SQL estate\r.DESCRIPTION\rDisplay a list of server login and database user and login for SQL servers listed in sqlservers.txt file from a range of domains\rAUTHOR: Rob Sewell https://blog.robsewell.com\rLAST UPDATE: DATE:07/01/2015\r.EXAMPLE\rShow-SQLUserLogins DBAwithaBeard\rShows the SQL Server logins and database users matching DOMAIN1\\DBAWithaBeard,DOMAIN2\\DBAWithaBeard, DBAWithaBeard\r#\u0026gt;\rWorkflow Show-UserLogins\r{\rparam ([string]$usr)\r$servers = Get-Content '\\sql\\Powershell Scripts\\sqlservers.txt'\r$ErrorActionPreference = \u0026quot;SilentlyContinue\u0026quot;\r# Create an array for the username and each domain slash username\r$logins = @(\u0026quot;DOMAIN1\\$usr\u0026quot;,\u0026quot;DOMAIN2\\$usr\u0026quot;, \u0026quot;DOMAIN3\\$usr\u0026quot; ,\u0026quot;$usr\u0026quot; )\rWrite-Output \u0026quot;#################################\u0026quot; Write-Output \u0026quot;SQL Servers, Databases and Logins for `n$logins displayed below \u0026quot; Write-Output \u0026quot;################################# `n\u0026quot; #loop through each server and each database and display usernames, servers and databases\rWrite-Output \u0026quot; Server Logins`n\u0026quot;\rforeach -parallel ($server in $servers)\r{\rinlinescript\r{\r[System.Reflection.Assembly]::LoadWithPartialName('Microsoft.SqlServer.SMO') | out-null\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $Using:server\rif(!$srv.Version)\r{\rWrite-Output \u0026quot;$Using:server is not contactable - Please Check Manually\u0026quot;\r}\relse\r{ foreach ($login in $Using:logins)\r{ if($srv.Logins.Contains($login))\r{\rWrite-Output \u0026quot; $Using:server -- $login \u0026quot; } else\r{\rcontinue\r}\r} } }\r}\rWrite-Output \u0026quot;`n###########################\u0026quot;\rWrite-Output \u0026quot;`n Database Logins`n\u0026quot;\rforeach -parallel ($server in $servers)\r{\rinlinescript\r{\r$srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $Using:server\rif(!$srv.Version)\r{\rWrite-Output \u0026quot;$Using:server is not contactable - Please Check Manually\u0026quot;\r}\relse\r{ foreach($database in $srv.Databases|Where-Object{$_.IsAccessible -eq $True})\r{\rforeach($login in $Using:logins)\r{\rif($database.Users.Contains($login))\r{\rWrite-Output \u0026quot; $Using:server -- $database -- $login \u0026quot; }\relse\r{\r} }\r}\r}\r}\r} Write-Output \u0026quot;`n#########################################\u0026quot;\rWrite-Output \u0026quot;Finished - If there are no logins displayed above then no logins were found!\u0026quot; Write-Output \u0026quot;#########################################\u0026quot; }\r","date":"2013-08-31T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-for-sql-server-logins-with-powershell/","title":"Checking for SQL Server logins with PowerShell"},{"content":"With over 700 databases to look after at MyWork automation is high on my list of priorities. I have two PowerShell scripts which run regularly checking SQL Error logs. One checks for the output from DBCC CHECKDB and one for errors. They then email the results to the DBA team.\nThis week we noticed that a new database was creating a lot of entries. It appeared to be starting up every few minutes. A bit of investigation by my colleague revealed that this database had been created on SQL Express and migrated to SQL Server.\nSQL Express sets AUTO_CLOSE to on by default and this is what was creating the entries.\nWhat does the AUTO_CLOSE setting do?\nAccording to BoL Link\nDescription Default value When set to ON, the database is shut down cleanly and its resources are freed after the last user exits. The database automatically reopens when a user tries to use the database again. True for all databases when using SQL Server 2000 Desktop Engine or SQL Server Express, and False for all other editions, regardless of operating system. When set to OFF, the database remains open after the last user exits. That explains what was happening, the database was shutting down as the session finished and then starting back up again when the next one started. Repeatedly. Filling up the log files with entries, resetting the DMVs and using resources unnecessarily.\nTo find databases with this setting on query the master.sys.databases for the is_auto_close_on column Link or check the properties page in SSMS\nYou can change the setting there or with T-SQL\nOf course I like to do it with PowerShell!!\nTo find the databases with AUTO_CLOSE setting on\nTo change the setting with PowerShell\n$svrs = ## list of servers Get-Content from text fiel etc\rforeach ($svr in $svrs) {\r$server = New-Object Microsoft.SQLServer.Management.Smo.Server $svrs\rforeach ($db in $server.Databases) {\rif ($db.AutoClose = $true) {\rWrite-Output \u0026quot;$Server - $($DB.Name) AutoClose ON\u0026quot;\r} }\r}\r$Svr = 'SERVERNAME'\r$DB = 'DatabaseName'\r$server = New-Object Microsoft.SQLServer.Management.Smo.Server $svrs\r$db.AutoClose = $false\r$db.Alter()\r","date":"2013-08-31T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2013/08/image_thumb.png","permalink":"https://blog.robsewell.com/blog/sql-express-migration-auto-close-setting/","title":"SQL Express Migration Auto Close Setting"},{"content":"I am impressed with the output from¬†sp_BlitzIndex‚Ñ¢ and today I tried to save it to an excel file so that I could pass it on to the developer of the service. When I opened it in Excel and imported it from the csv file it didn‚Äôt keep the T-SQL in one column due the commas which bothered me so I decided to use Powershell to output the format to Excel as follows\n#############################################################################################\r#\r# NAME: SPBlitzIndexToCSV.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:22/06/2013\r#\r# COMMENTS: This script will take the output from spBlitzIndex‚Ñ¢ and\r# export it to csv without splitting the tsql commands\r# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r$Server = Read-Host \u0026quot;Please enter Server\u0026quot;\r$Database = Read-Host \u0026quot;Enter Database Name to run spBlitzIndex against\u0026quot;\r$filePath = \u0026quot;C:\\temp\\BlitzIndexResults\u0026quot;\r$Date = Get-Date -format ddMMYYYY\r$FileName = \u0026quot;Blitzindex_\u0026quot; + $Database + \u0026quot;_\u0026quot; + $Date + \u0026quot;.csv\u0026quot;$Query = \u0026quot;EXEC dbo.sp_BlitzIndex @database_name='$Database';\u0026quot;\r$Blitz = Invoke-SQLCMD -server $Server -database master -query $Query$Blitz|Export-Csv $FilePath\r$FileName\rPlease don‚Äôt ever trust anything you read on the internet and certainly don‚Äôt implement it on production servers without first both understanding what it will do and testing it thoroughly. This solution worked for me in my environment I hope it is of use to you in yours but I know nothing about your environment and you know little about mine\n","date":"2013-06-23T00:00:00Z","permalink":"https://blog.robsewell.com/blog/sp_blitzindex-ouput-to-excel-with-powershell/","title":"sp_BlitzIndex‚Ñ¢ ouput to Excel with Powershell"},{"content":"So at our SQL SouthWest User Group session last week we had sessions from Jonathan @fatherjack and Annette @Mrsfatherjack on SSRS and SSIS respectively. During Annettes SSIS session a question was asked about reading email attachments and then loading them into a database. No-one had an answer using SSIS but I said it could be done with Powershell . So I have written the following script.\nWhat it does is open an Outlook com object, search for an email with a certain subject and save it in the temp folder and then import it into a SQL database. This needs to be done on a machine with Outlook and Excel installed. It is possible to process the email using EWS in an Exchange environment and other people have written scripts to do so.\nIt uses two functions Out-Datatable from http://gallery.technet.microsoft.com/scriptcenter/4208a159-a52e-4b99-83d4-8048468d29dd\nand Write-Datatable from\nhttp://gallery.technet.microsoft.com/scriptcenter/2fdeaf8d-b164-411c-9483-99413d6053ae\nThe first takes the output from parsing the Excel File and converts it into a datatable object which can then be piped to the second which uses the BulkCopy method. Alternatively if you require it you could add each row of the excel file to an array and then use Invoke-SQLCmd to insert the data row by row.\nwhile($row1 -le\r$lastusedrange) {\r$Col1 = $ws.Cells.Item($row1,1).Value2\r$Col2 = $ws.Cells.Item($row1,2).Value2 $Col3 = $ws.Cells.Item($row1,3).Value2\r$query = \u0026quot;INSERT INTO Database.Schema.Table\r(Column1\r,Column2\r,Column3 )\rVALUES\r('$Col1'\r,'$Col2'\r,'$Col3')\rGO\r\u0026quot;\r$dt = invoke-sqlcmd -query $query -ServerInstance $Server -database $database\r## For Testing Write-Host $query\r#############################################################################################\r#\r# NAME: ExcelEmailAttachmentToDatabase.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:15/06/2013\r#\r# COMMENTS: This script will read your email using outlook com object and save Excel Attachment # and import it into a database\r# REQUIRES: It uses two functions Out-Datatable from # http://gallery.technet.microsoft.com/scriptcenter/4208a159-a52e-4b99-83d4-8048468d29dd # and Write-Datatable from\r# http://gallery.technet.microsoft.com/scriptcenter/2fdeaf8d-b164-411c-9483-99413d6053ae #\r# ------------------------------------------------------------------------\r# Create Outlook Object\rAdd-type-assembly \u0026quot;Microsoft.Office.Interop.Outlook\u0026quot;|out-null\r$olFolders = \u0026quot;Microsoft.Office.Interop.Outlook.olDefaultFolders\u0026quot; -as [type]\r$outlook = new-object -comobject outlook.application\r$namespace = $outlook.GetNameSpace(\u0026quot;MAPI\u0026quot;)\r# Set Folder to Inbox\r$folder = $namespace.getDefaultFolder($olFolders::olFolderInBox)\r# CHeck Email For Subject and set to variable $Email = $folder.items | Where-Object Subject -Contains $Subject\r$Attachments = $Email.Attachments\r$filepath = $env:TEMP\r$filename = \u0026quot;TestFilename.xlsx\u0026quot;\r$Subject = \u0026quot;This is a Test\u0026quot;\r$server = 'test server'\r$Database = 'Test Database'\r$Table = 'tbl_DataloadTest'\rforeach ($Attachment in $Attachments) {\r$attachName = $Attachment.filename\rIf\r($attachName.Contains(\u0026quot;xlsx\u0026quot;)) {\r$Attachment.saveasfile((Join-Path $filepath $filename)) } }\r# Create an Excel Object\r$xl = New-Object -comobject Excel.Application\r\u0026amp;lt;# ##For testing $xl.visible = $true\r#\u0026amp;gt;\r# Open the File\r$wb = $xl.WorkBooks.Open(\u0026quot;$filepath\\$filename\u0026quot;)\r$ws = $wb.Worksheets.Item(1)\r# If your data does not start at A1 you may need\r$column1 = 1\r$row1 = 2\r$lastusedrange = $ws.UsedRange.Rows.Count\r$dt = @()\rwhile ($row1 -le $lastusedrange) {\r$Col1 = $ws.Cells.Item($row1, 1).Value2\r$Col2 = $ws.Cells.Item($row1, 2).Value2\r$Col3 = $ws.Cells.Item($row1, 3).Value2\r$newrow = ($Col1, $col2, $col3)\r$dt += $newrow\r# Move to next row\r$row1 = $row1 + 1\r}\r$xl.Quit()\r[System.Runtime.Interopservices.Marshal]::ReleaseComObject($xl)\r$Input = $dt|Out-DataTable\rWrite-DataTable -ServerInstance $server -Database $Database -TableName $Table -Data $Input\rVisit your own User Group ‚Äì You can find them herehttp://www.sqlpass.org/\nIf you are in the South West UK then come and join our group. Free training and conversation with like minded people once a month and pizza too what could be better!!\n","date":"2013-06-20T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershell-can-read-email-insert-excel-file-attachment-into-a-sql-database/","title":"Powershell can read email \u0026 insert excel file attachment into a SQL Database"},{"content":"This has been an interesting journey. The Adventure Works database is frequently used in blogs and reference books and I wanted to install it in my Windows Azure Learning Lab and I also wanted to automate the process.\nThe easiest way is to download the Windows Azure MDF file from¬†http://msftdbprodsamples.codeplex.com/ jump through all the security warnings in Internet Explorer and save the file and then create the database as follows\nCREATE DATABASE AdventureWorks2012\rON (FILENAME = 'PATH TO \\AdventureWorks2012_Data.mdf')\rFOR ATTACH_REBUILD_LOG ;\rThat is the way I will do it from now on! After reading this page I tried to download the file with Powershell but it would not as I could not provide a direct link to the file. Maybe someone can help me with that. So I thought I would use my SkyDrive to hold the MDF file and map a drive on the server.\nto do this you need to add the Desktop Experience feature to the server. This can be done as follows\nImport-Module ServerManager\rAdd-WindowsFeature Desktop-Experience -restart\rThis will take a few minutes to install, reboot and then configure the updates before you can log back in. While it is doing this log into your SkyDrive and navigate to a folder and copy the URL to notepad\nIt will look something like this\nhttps://skydrive.live.com/?lc=2137#cid=XXXXXXXXXXXXXXXX\u0026amp;id=CYYYYYYYYYYYYYYYY\nCopy the GUID after the cid=\nand write this command\nnet use T: \\\\d.docs.live.net@SSL\\XXXXXXXXXXXXXXXX /user:$user $password\rI keep this in a script and pass the user and password in via Read-Host\nHowever, if you try to copy the item from the folder you will get an error\nThe file size exceeds the limit allowed and cannot be saved\nSo you will need to alter a registry key as follows\nSet-ItemProperty -Path HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\WebClient\\Parameters -Name FileSizeLimitInBytes -Value 4294967295\rand then restart the WebClient service Then run the net use command to map the drive and copy the file with Copy-Item\nBut my script to auto install the Adventure Works database via Powershell once you have completed all the pre-requisites is\n$user = Read-Host \u0026quot;user\u0026quot;\r$password = Read-Host \u0026quot;Password\u0026quot;\rnet use T: \\\\d.docs.live.net@SSL\\XXXXXXXXXXXXXXX /user:$user $password\rNew-Item c:\\AW -ItemType directory\rCopy-Item T:\\Documents\\Azure\\AdventureWorks2012_Data.mdf C:\\AW\rInvoke-Sqlcmd -ServerInstance YourServerName -Database master -Query \u0026quot;CREATE DATABASE AdventureWorks2012\rON (FILENAME = 'C:\\AW\\AdventureWorks2012_Data.mdf')\rFOR ATTACH_REBUILD_LOG ;\u0026quot;\rTo be honest I don‚Äôt think I will use this method as my copy failed twice before it succeeded so I will just download the file and create the database!!\nPlease don‚Äôt ever trust anything you read on the internet and certainly don‚Äôt implement it on production servers without first both understanding what it will do and testing it thoroughly. This solution worked for me in my environment I hope it is of use to you in yours but I know nothing about your environment and you know little about mine\n","date":"2013-05-19T00:00:00Z","permalink":"https://blog.robsewell.com/blog/add-adventure-works-database-to-windows-azure-vm/","title":"Add Adventure Works Database to Windows Azure VM"},{"content":"It was patching time this week at MyWork so I thought I would share some Powershell scripts I use to speed up the process.\nI keep these in their own folder and cd to it. Then I can just type the first few letters and tab and Powershell completes it. Nice and easy and time saving\nThe first thing I do is to stop the SQL services with the StopSQLServices.ps1\nGet the server name with Read-Host then I like to see the before and after using\nget-service -ComputerName $server|Where-Object { $_.Name -like '*SQL*' }\rThis uses the Get-service CMDlet to find the services with SQL in the name and display them. Then we pass the running services to an array and use the stop method with a while to check if the services are stopped before displaying the services again. Note this will stop all services with SQL in the name so if for example you are using Redgates SQL Monitor it will stop those services too. If that could be an issue then you may need to alter the where clause. As always test test test before implementing in any live environment.\nOnce the services are stopped I RDP using the RDP script which again uses Read-host to get a server and then opens up a RDP with a simple Invoke-Command. This means I can stay in Powershell.\nThen I patch the server and reboot using the ping script to set up a continuous ping.\nIf you want to install Windows Updates via Powershell you can use the details here. I like to jump on the box to keep an eye on it.\nTo check the event log The EventLog.ps1 script is very simple\nGet-EventLog -computername $server -log $log -newest $latest | Out-GridView\rEnter the server name and then application or system and it will display the results using out-gridview which will allow you to filter the results as required. I have another version of this script with a message search as well.\nYou can simply add where {$_.entryType -match ‚ÄúError‚Äù} if you only want the errors or Warning for the warnings. I like to look at it all.\nCheck the SQL error log with this script which uses the SMO method\n$Server = Read-Host \u0026quot;Please Enter the Server\u0026quot; $srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server $Results = $srv.ReadErrorLog(0) | format-table -Wrap -AutoSize $Results\rI love these four lines they make it so easy for me to look at the SQL error log whenever I need to. If you want you can pipe to Out-GridView or even to notepad. If I want to check one of the previous error logs I change ReadErrorLog(0) to ReadErrorLog(1) or 2 or 3 etc. I have a daily script which emails me any SQL error log errors and DBCC errors every day so I am aware of any issues before\nThen the AutoServices.ps1 to show the state of the auto start services. Strangely you cannot get the Start Type from Get-Service so I use Get-WMIObject. If any have failed to start then I use Get-Service to get the service¬†and pipe to Start-Service\nThis is what works for me I hope it is of use to you\nPlease don‚Äôt ever trust anything you read on the internet and certainly don‚Äôt implement it on production servers without first both understanding what it will do and testing it thoroughly. This solution worked for me in my environment I hope it is of use to you in yours but I know nothing about your environment and you know little about mine\n\u0026lt;# .NOTES Name: StopSQLServices.ps1 Author: Rob Sewell https://blog.robsewell.com\rRequires: Version History: Added New Header 23 August 2014\r.SYNOPSIS .DESCRIPTION .PARAMETER .PARAMETER .PARAMETER .EXAMPLE #\u0026gt; #############################################################################################\r#\r# NAME: StopSQLServices.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com @fade2blackuk\r# DATE:15/05/2013\r#\r# COMMENTS: This script will stop all SQL Services on a server\r# ------------------------------------------------------------------------\r$Server = Read-Host \u0026quot;Please Enter the Server - This WILL stop all SQL services\u0026quot;\rWrite-Host \u0026quot;########### Services on $Server BEFORE ##############\u0026quot; -ForegroundColor Green -BackgroundColor DarkYellow\rget-service -ComputerName $server|Where-Object { $_.Name -like '*SQL*' }Write-Host \u0026quot;########### Services on $Server BEFORE ##############\u0026quot; -ForegroundColor Green -BackgroundColor DarkYellow\r## $Services = Get-WmiObject Win32_Service -ComputerName $server| Where-Object { $_.Name -like '*SQL*'-and $_.State-eq 'Running' }\r$Services = Get-Service -ComputerName $server|Where-Object { $_.Name -like '*SQL*' -and $_.Status -eq 'Running' }\rforeach ($Service in $Services) {\r$ServiceName = $Service.displayname\r(get-service -ComputerName $Server -Name $ServiceName).Stop()\rwhile ((Get-Service -ComputerName $server -Name $ServiceName).status -ne 'Stopped')\r{\u0026lt;#do nothing#\u0026gt;}\r}\rWrite-Host \u0026quot;########### Services on $Server After ##############\u0026quot; -ForegroundColor Green -BackgroundColor DarkYellow\rGet-Service -ComputerName $server|Where-Object { $_.Name -like '*SQL*' }\rWrite-Host \u0026quot;########### Services on $Server After ##############\u0026quot; -ForegroundColor Green -BackgroundColor DarkYellow\r#############################################################################################\r#\r# NAME: RDP.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com @fade2blackuk\r# DATE:15/05/2013\r#\r# COMMENTS: This script to open a RDP\r# ------------------------------------------------------------------------\r$server = Read-Host \u0026quot;Server Name?\u0026quot;\rInvoke-Expression \u0026quot;mstsc /v:$server\u0026quot;\r#############################################################################################\r#\r# NAME: Ping.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com @fade2blackuk\r# DATE:15/05/2013\r#\r# COMMENTS: This script to set up a continous ping # Use CTRL + C to stop it\r# ------------------------------------------------------------------------\r$server = Read-Host \u0026quot;Server Name?\u0026quot;\rInvoke-Expression \u0026quot;ping -t $server\u0026quot;\r#############################################################################################\r#\r# NAME: SQLErrorLog.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com @fade2blackuk\r# DATE:15/05/2013\r#\r# COMMENTS: This script will display the SQL Error Log for a remote server\r# ------------------------------------------------------------------------\r$Server = Read-Host \u0026quot;Please Enter the Server\u0026quot; $srv = New-Object ('Microsoft.SqlServer.Management.Smo.Server') $server $srv.ReadErrorLog(0) | Out-GridView #############################################################################################\r#\r# NAME: Autoservices.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com @fade2blackuk\r# DATE:15/05/2013\r#\r# COMMENTS: # Script to show the services running that are set to Automatic startup - # good for checking after reboot\r# ------------------------------------------------------------------------\r$Server = Read-Host \u0026quot;Which Server?\u0026quot;\rGet-WmiObject Win32_Service -ComputerName $Server | Where-Object { $_.StartMode -like 'Auto' }| Select-Object __SERVER, Name, StartMode, State | Format-Table -auto\rWrite-Host \u0026quot;SQL Services\u0026quot;\rGet-WmiObject Win32_Service -ComputerName $Server | Where-Object { $_.DisplayName -like '*SQL*' }| Select-Object __SERVER, Name, StartMode, State | Format-Table -auto\r","date":"2013-05-19T00:00:00Z","permalink":"https://blog.robsewell.com/blog/checking-sql-error-logs-event-logs-and-stopping-services-with-powershell/","title":"Checking SQL Error Logs, Event Logs and Stopping Services with Powershell"},{"content":"Edit 2022 -\nFind your User Groups here Azure Data Community\nThe timing was good enough that I could offer to do a talk based on my previous post on Windows Azure for my SQL User Group SQL SouthWest when Jonathan and Annette.( @FatherJack and @MrsFatherJack) put out a call for volunteers (Edit Sept 2020 - Brave enough to say now that Jonathan just told me I was doing it !! In the nicest possible way).\nI did my best with the 7 P‚Äôs. I ran through it at lunchtime, I made sure I had power and a HDMI lead after checking with Jonathan, I got a glass of water. I knew the first line I was going to say\nHowever, I neglected to check that I would have HDMI in at the location so everything that was on my laptop was useless! My laptop did very odd things to the USB stick when I tried to transfer to Jonathans laptop and he didn‚Äôt have Powershell V3 installed so whilst Neil Hambly @Neil_Hambly from Confio was speaking I was busy ignoring a very interesting talk on Waits to install and configure Powershell Azure on my Azure VM. Sorry Neil.\nBut in the end it more or less worked and we are lucky to have such a patient and supportive user group who helped me along the way as well. Thank you folks\nThings I took away from the evening\nDouble check you have all the connections Practice and Practice some more Think about the times when something is running and what you will say when there is nothing to see Presenting completely inside a RDP session adds unnecessary complication The Demo Gods WILL hit you and the curse of the red text will fall upon you during the presentation. Accept it and move on. Have an opening line Remember to breath (especially when the demo falls over) Enjoy it! It didn‚Äôt go perfectly but people gave me some good feedback and I am pleased to say that I have pointed people towards something new that will help them and passed over my knowledge and that to me is what the SQL Community is all about. I have a load of other ideas for things I can talk about and blog about so it is going to be a very busy time for me as I work my way through them and do all the other exciting things coming my way in the SQL world.\nVisit your own User Group ‚Äì You can find them here Azure Data Community\nIf you are in the South West UK then come and join our group. Free training and conversation with like minded people once a month and pizza too what could be better!!\n","date":"2013-05-17T00:00:00Z","permalink":"https://blog.robsewell.com/blog/lessons-learnt-from-my-first-talk-at-sql-southwest/","title":"Lessons Learnt from my first talk at SQL SouthWest"},{"content":"So at SQL Bits I went to Chris Testa-O‚ÄôNeill‚Äôs session on certification. This has inspired me to start working on passing the MCSE exams. My PC at home doesn‚Äôt have much grunt and my tablet wont run SQL. I considered some new hardware but I knew I would have a hard time getting authorisation from the Home Financial Director (Mrs F2B) despite my all the amazing justification and benefits I could provide!!\nSo I looked at Windows Azure as a means of having some servers to play with. After watching this video and then this video I took the plunge and dived in.\nAfter setting up my account I read a few blogs about Powershell and Windows Azure.\nhttp://adminian.com/2013/04/16/how-to-setup-windows-azure-powershell/\nNote ‚Äì Here I only spin up extra small instances and don‚Äôt configure SQL as per Microsoft‚Äôs recommendations. I am only using these VMs for learning and talking at my user group your needs may be different\nFirst you‚Äôll need Microsoft Web Platform Installer. Then download and install Windows Azure PowerShell,\nImport-Module c:\\Program Files\\Microsoft SDKs\\Windows Azure\\PowerShell\\Azure\\Azure.psd1\rThis gives you all the Windows Azure Powershell Cmdlets.\nGet-AzurePublishSettingsFile which will give you a download for a file.¬†PowerShell will use this to control your Windows Azure so although you need it now, keep it safe and probably out of your usual directories so it doesn‚Äôt get compromised.\nImport-AzurePublishSettingsFile and the file path to import it into Powershell.\nGet-AzureSubscription to see the results and note the subscription name.\nNow we create a storage account\nNew-AzureStorageAccount -StorageAccountName chooseaname -label 'a label' -Description 'The Storage Account for the Lab Spin Up and Down' -Location 'West Europe'\rGet-AzureLocation will show you the available locations if you want a different one.I then set the storage account to be default for my subscription\nSet-AzureSubscription -SubscriptionName 'Subscription Name from Earlier' -CurrentStorageAccount 'theoneyouchose'\rI spent a couple of days sorting out the following scripts. They set up three SQL Servers, configure them to allow SSMS, Powershell and RDP connections and also remove them all. The reasoning behind this is that you will be charged for servers even when they are turned off\nFirst we set some variables\n$image = 'fb83b3509582419d99629ce476bcb5c8__Microsoft-SQL-Server-2012SP1-Standard-CY13SU04-SQL11-SP1-CU3-11.0.3350.0-B'\r$SQL1 = 'SQL1'\r$SQL2 = 'SQL2'\r$SQL3 = 'SQL3'\r$size = 'ExtraSmall'\r$AdminUser = 'ChoosePCAdminName'\r$password = 'SUPERCOMpl1c@teDPassword'\r$Service = 'theservicenameyouchoose'\r$Location = 'West Europe'\rTo choose an image run Get-AzureVMImage|select name and pick the one for you. I chose a size of extra small as it is cheaper. As I won‚Äôt be pushing the servers very hard I don‚Äôt need any extra grunt. Set up a service the first time and use the location switch but then to use the same service again remove the location switch otherwise you will get an error stating DNS name already in use which is a little confusing until you know.\n$vm = New-AzureVMConfig -Name $SQL1 -InstanceSize $size -ImageName $image |\rAdd-AzureProvisioningConfig -AdminUsername $AdminUser -Password $password -Windows |\rAdd-AzureEndpoint -Name \u0026quot;SQL\u0026quot; -Protocol \u0026quot;tcp\u0026quot; -PublicPort 57502 -LocalPort 1433|\rAdd-AzureEndpoint -Name PS-HTTPS -Protocol TCP -LocalPort 5986 -PublicPort 5986\\\rThis creates a VM object and adds two endpoints for the server, one for Powershell and one for SSMS. When you provision more than one server you will need to make sure you use a different Public Port for each server otherwise you will get an error. You will need to note which server has which port when you need to connect with SSMS.\nOnce you have your VM object just pass it to New-AzureVM as shown\nNew-AzureVM -ServiceName $Service -VMs $vm\rProviding you have no errors you can then just wait until you see this.\nIt will take a few minutes. Long enough to get a cuppa. Even then you won‚Äôt be able to connect straightaway as Azure will be provisioning the server still.\nThe next bit of the script downloads the RDP shortcut to a folder on the desktop and assigns a variable for the clean up script. I use this because the next time you spin up a server it may not use exactly the same port for RDP.\n$SQL1RDP = \u0026quot;$ENV:userprofile\\Desktop\\Azure\\RDP\\$SQL1.rdp\u0026quot;\rGet-AzureRemoteDesktopFile -ServiceName $Service -name $SQL1 -LocalPath $SQL1RDP\rInvoke-Expression $SQL1RDP\rThe Invoke-Expression will open up a RDP connection but unless you have gone to get a cuppa I would check in your management portal before trying to connect as the server may still be provisioning. In fact,I would go to your Windows Azure Management Portal and check your virtual machine tab where you will see your VMs being provisioned\nNow you have three servers but to be able to connect to them from your desktop and practice managing them you still need to do a bit of work. RDP to each server run the following script in Powershell.\n# Configure PowerShell Execution Policy to Run all Scripts ‚Äì It‚Äôs a one time Progress\rSet-ExecutionPolicy ‚ÄìExecutionPolicy Unrestricted\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=in action=allow enable=yes profile=any\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=out action=allow program=any enable=yes profile=any\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Administration\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;File and Printer Sharing\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Service Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Performance Logs and Alerts\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Event Log Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Scheduled Tasks Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Volume Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Desktop\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Windows Firewall Remote Management\u0026quot; new enable =yes\rnetsh advfirewall firewall set rule group=\u0026quot;windows management instrumentation (wmi)\u0026quot; new enable =yes\\\rI use netsh advfirewall as I find it easy and I understand it. I know you can do it with Set-NetFirewallProfile but that‚Äôs the beauty of Powershell you can still use all your old cmd knowledge as well. This will allow you to remote manage the servers. You can do it from your laptop with the creation of some more endpoints but I just use one server as a management server for my learning.\nThe last part of the script changes SQL to Mixed authentication mode and creates a SQL user with sysadmin and restarts the SQL service on each server and that‚Äôs it. Its ready to go.\nOpen up SSMS on your desktop and connect to YourServiceName.Cloudapp.Net, PortNumber (57500-5702 in this example) To remove all of the implementation run the code that is commented out in steps. First it assigns a variable to each VHD, then it removes the VM. You should then wait a while before removing the VHDs as it takes a few minutes to remove the lease and finally remove the RDP shortcuts as next time they will be different.\n\u0026lt;#\r.NOTES\rName: CreateLab.ps1\rAuthor: Rob Sewell https://blog.robsewell.com\rRequires: Get the Windows Azures CmdLets then run this\rVersion History:\rAdded New Header 23 August 2014\r.SYNOPSIS\rThis script will create 3 Windows Azure SQL Servers and open up RDP connections\rready for use. There is also the scripts to remove the Windows Azure Objects to save on\rusage costs\r.DESCRIPTION\r.PARAMETER\r.PARAMETER\r.PARAMETER\r.EXAMPLE\r#\u0026gt;\r# Get the Subscription File and Import it\rGet-AzurePublishSettingsFile\rImport-AzurePublishSettingsFile FilepathtoPublishSettingsFile\r\u0026lt;# Run this once to set up a Storage Account\rNew-AzureStorageAccount -StorageAccountName storageaccountname -location 'West Europe' -Label 'Storage Account for My Lab'\r#\u0026gt;\rGet-AzureSubscription #Note the name\r#Set the storage account to the subscription\rSet-AzureSubscription -SubscriptionName SubscriptionName -CurrentStorageAccount storageaccountname\r#Some variables\r# Use Get-AzureVMimage to find the one you want ie Get-AzureVMImage | where { ($_.ImageName -like \u0026quot;*SQL*\u0026quot;) }|select ImageName\r$image = 'fb83b3509582419d99629ce476bcb5c8__Microsoft-SQL-Server-2012SP1-Standard-CY13SU04-SQL11-SP1-CU3-11.0.3350.0-B'\r$SQL1 = 'SQL1'\r$SQL2 = 'SQL2'\r$SQL3 = 'SQL3'\r$size = 'ExtraSmall'\r$AdminUser = 'ChoosePCAdminName'\r$password = 'SUPERCOMpl1c@teDPassword'\r$Service = 'theservicenameyouchoose'\r$Location = 'West Europe'\r\u0026lt;# Run this the first time to create a Service\rNew-AzureService -ServiceName $Service -Location $Location -Label 'SQLDBA with a Beard Service'\r#\u0026gt;\r#Configure the VMs\r$vm = New-AzureVMConfig -Name $SQL1 -InstanceSize $size -ImageName $image |\rAdd-AzureProvisioningConfig -AdminUsername $AdminUser -Password $password -Windows|\rAdd-AzureEndpoint -Name \u0026quot;SQL\u0026quot; -Protocol \u0026quot;tcp\u0026quot; -PublicPort 57500 -LocalPort 1433\r$vm2 = New-AzureVMConfig -Name $SQL2 -InstanceSize $size -ImageName $image |\rAdd-AzureProvisioningConfig -AdminUsername $AdminUser -Password $password -Windows |\rAdd-AzureEndpoint -Name \u0026quot;SQL\u0026quot; -Protocol \u0026quot;tcp\u0026quot; -PublicPort 57501 -LocalPort 1433\r$vm3 = New-AzureVMConfig -Name $SQL3 -InstanceSize $size -ImageName $image |\rAdd-AzureProvisioningConfig -AdminUsername $AdminUser -Password $password -Windows |\rAdd-AzureEndpoint -Name \u0026quot;SQL\u0026quot; -Protocol \u0026quot;tcp\u0026quot; -PublicPort 57502 -LocalPort 1433|\rAdd-AzureEndpoint -Name PS-HTTPS -Protocol TCP -LocalPort 5986 -PublicPort 5986\r#Provision the VMs\rNew-AzureVM -ServiceName $Service -VMs $vm, $vm2,$vm3\r# Get the RDP Files\r$SQL1RDP = \u0026quot;$ENV:userprofile\\Desktop\\Azure\\RDP\\$SQL1.rdp\u0026quot;\r$SQL2RDP = \u0026quot;$ENV:userprofile\\Desktop\\Azure\\RDP\\$SQL2.rdp\u0026quot;\r$SQL3RDP = \u0026quot;$ENV:userprofile\\Desktop\\Azure\\RDP\\$SQL3.rdp\u0026quot;\rGet-AzureRemoteDesktopFile -ServiceName $Service -name $SQL1 -LocalPath $SQL1RDP\rGet-AzureRemoteDesktopFile -ServiceName $Service -name $SQL2 -LocalPath $SQL2RDP\rGet-AzureRemoteDesktopFile -ServiceName $Service -name $SQL3 -LocalPath $SQL3RDP\r# Open the RDP Fies - Check the machine is up in your Management Portal\rInvoke-Expression $SQL1RDP\rInvoke-Expression $SQL2RDP\rInvoke-Expression $SQL3RDP\r# Now run the SetupVM script for each server\r\u0026lt;#\rThis is the clean up script to remove the servers and services\rRun this first\r$SQL1Disk = Get-AzureDisk|where {$_.attachedto.rolename -eq $SQL1}\r$SQL2Disk = Get-AzureDisk|where {$_.attachedto.rolename -eq $SQL2}\r$SQL3Disk = Get-AzureDisk|where {$_.attachedto.rolename -eq $SQL3}\r#Then This\rRemove-AzureVM -Name $SQL1 -ServiceName $Service\rRemove-AzureVM -Name $SQL2 -ServiceName $Service\rRemove-AzureVM -Name $SQL3 -ServiceName $Service\rThen wait a while and run this\r$SQL1Disk|Remove-AzureDisk -DeleteVHD\r$SQL2Disk|Remove-AzureDisk -DeleteVHD\r$SQL3Disk|Remove-AzureDisk -DeleteVHD\r#Remove-AzureService $Service\rGet-ChildItem \u0026quot;$ENV:userprofile\\Desktop\\Azure\\RDP\\*.rdp\u0026quot;|Remove-Item\r#\u0026gt;\r\u0026lt;#\rThis is the clean up script for variables\rRemove-Variable [a..z]* -Scope Global\rRemove-Variable [1..9]* -Scope Global\r#\u0026gt;\r.NOTES\rName: SetUpVMSQL1.ps1\rAuthor: Rob Sewell https://blog.robsewell.com\rRequires:\rVersion History:\rAdded New Header 23 August 2014\r.SYNOPSIS\rThis script will set up the SQL1 VM ready for use and enable SQL Authentication\rAdd a user called SQLAdmin with a password of P@ssw0rd\rRestart SQL Service.Run on SQL1\r.DESCRIPTION\r.PARAMETER\r.PARAMETER\r.PARAMETER\r.EXAMPLE\r#\u0026gt;\r# Configure PowerShell Execution Policy to Run all Scripts ÔøΩ ItÔøΩs a one time Progress\rSet-ExecutionPolicy ÔøΩExecutionPolicy Unrestricted\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=in action=allow enable=yes profile=any\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=out action=allow program=any enable=yes profile=any\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Administration\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;File and Printer Sharing\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Service Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Performance Logs and Alerts\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Event Log Management\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Scheduled Tasks Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Volume Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Desktop\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Windows Firewall Remote Management\u0026quot; new enable =yes\rnetsh advfirewall firewall set rule group=\u0026quot;windows management instrumentation (wmi)\u0026quot; new enable =yes\r# To Load SQL Server Management Objects into PowerShell\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOExtendedÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SqlWmiManagementÔøΩ) | out-null\rSQLPS\r$Name = 'SQL1'\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rEXEC xp_instance_regwrite N'HKEY_LOCAL_MACHINE', N'Software\\Microsoft\\MSSQLServer\\MSSQLServer', N'LoginMode', REG_DWORD, 2\rGO\r\u0026quot;\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rCREATE LOGIN [SQLAdmin] WITH PASSWORD=N'P@ssw0rd', DEFAULT_DATABASE=[master]\rGO\rALTER SERVER ROLE [sysadmin] ADD MEMBER [SQLAdmin]\rGO\r\u0026quot;\rget-Service -ComputerName $Name -Name MSSQLSERVER|Restart-Service -force\\\r\u0026lt;#\r.NOTES\rName: SetUpVMSQL2.ps1\rAuthor: Rob Sewell https://blog.robsewell.com\rRequires:\rVersion History:\rAdded New Header 23 August 2014\r.SYNOPSIS\r.DESCRIPTION\r.PARAMETER\r.PARAMETER\r.PARAMETER\r.EXAMPLE\r#\u0026gt;\r#############################################################################################\r#\r# NAME: SetupVMSQL2.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:10/05/2013\r#\r#\r# COMMENTS: This script will set up the SQL1 VM ready for use and enable SQL Authentication\r# Add a user called SQLAdmin with a password of P@ssw0rd\r# Restart SQL Service\r# ------------------------------------------------------------------------\r# Run on SQL2\r# Configure PowerShell Execution Policy to Run all Scripts ÔøΩ ItÔøΩs a one time Progress\rSet-ExecutionPolicy ÔøΩExecutionPolicy Unrestricted\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=in action=allow enable=yes profile=any\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=out action=allow program=any enable=yes profile=any\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Administration\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;File and Printer Sharing\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Service Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Performance Logs and Alerts\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Event Log Management\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Scheduled Tasks Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Volume Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Desktop\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Windows Firewall Remote Management\u0026quot; new enable =yes\rnetsh advfirewall firewall set rule group=\u0026quot;windows management instrumentation (wmi)\u0026quot; new enable =yes\r# To Load SQL Server Management Objects into PowerShell\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOExtendedÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SqlWmiManagementÔøΩ) | out-null\rSQLPS\r$Name = 'SQL2'\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rEXEC xp_instance_regwrite N'HKEY_LOCAL_MACHINE', N'Software\\Microsoft\\MSSQLServer\\MSSQLServer', N'LoginMode', REG_DWORD, 2\rGO\r\u0026quot;\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rCREATE LOGIN [SQLAdmin] WITH PASSWORD=N'P@ssw0rd', DEFAULT_DATABASE=[master]\rGO\rALTER SERVER ROLE [sysadmin] ADD MEMBER [SQLAdmin]\rGO\r\u0026quot;\rget-Service -ComputerName $Name -Name MSSQLSERVER|Restart-Service -force\\\r\u0026lt;#\r.NOTES\rName: SetUpVMSQL3.ps1\rAuthor: Rob Sewell https://blog.robsewell.com\rRequires:\rVersion History:\rAdded New Header 23 August 2014\r.SYNOPSIS\r.DESCRIPTION\r.PARAMETER\r.PARAMETER\r.PARAMETER\r.EXAMPLE\r#\u0026gt;\r#############################################################################################\r#\r# NAME: SetupVMSQL3.ps1\r# AUTHOR: Rob Sewell http://newsqldbawiththebeard.wordpress.com\r# DATE:10/05/2013\r#\r#\r# COMMENTS: This script will set up the SQL3 VM ready for use and enable SQL Authentication\r# Add a user called SQLAdmin with a password of P@ssw0rd\r# and enable PS Remoting\r# Restart SQL Service\r# ------------------------------------------------------------------------\r# Run on SQL3\r# Configure PowerShell Execution Policy to Run all Scripts ÔøΩ ItÔøΩs a one time Progress\rSet-ExecutionPolicy ÔøΩExecutionPolicy Unrestricted\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=in action=allow enable=yes profile=any\rnetsh advfirewall firewall add rule name=SQL-SSMS dir=out action=allow program=any enable=yes profile=any\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Administration\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;File and Printer Sharing\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Service Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Performance Logs and Alerts\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Event Log Management\u0026quot; new enable=yes\rNetsh advfirewall firewall set rule group=\u0026quot;Remote Scheduled Tasks Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Volume Management\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Remote Desktop\u0026quot; new enable=yes\rnetsh advfirewall firewall set rule group=\u0026quot;Windows Firewall Remote Management\u0026quot; new enable =yes\rnetsh advfirewall firewall set rule group=\u0026quot;windows management instrumentation (wmi)\u0026quot; new enable =yes\r#Extra one for PS Remoting\rnetsh advfirewall firewall add rule name=\u0026quot;Port 5986\u0026quot; dir=in action=allow protocol=TCP localport=5986\r# To Load SQL Server Management Objects into PowerShell\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SMOExtendedÔøΩ) | out-null\r[System.Reflection.Assembly]::LoadWithPartialName(ÔøΩMicrosoft.SqlServer.SqlWmiManagementÔøΩ) | out-null\rSQLPS\r$Name = 'SQL3'\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rEXEC xp_instance_regwrite N'HKEY_LOCAL_MACHINE', N'Software\\Microsoft\\MSSQLServer\\MSSQLServer', N'LoginMode', REG_DWORD, 2\rGO\r\u0026quot;\rInvoke-Sqlcmd -ServerInstance $Name -Database master -Query \u0026quot;USE [master]\rGO\rCREATE LOGIN [SQLAdmin] WITH PASSWORD=N'P@ssw0rd', DEFAULT_DATABASE=[master]\rGO\rALTER SERVER ROLE [sysadmin] ADD MEMBER [SQLAdmin]\rGO\r\u0026quot;\rget-Service -ComputerName $Name -Name MSSQLSERVER|Restart-Service -force\rEnable-PSRemoting -force\rPlease don‚Äôt ever trust anything you read on the internet and certainly don‚Äôt implement it on production servers without first both understanding what it will do and testing it thoroughly. This solution worked for me in my environment I hope it is of use to you in yours but I know nothing about your environment and you know little about mine\n","date":"2013-05-14T00:00:00Z","permalink":"https://blog.robsewell.com/blog/spinning-up-and-shutting-down-windows-azure-lab-with-powershell/","title":"Spinning up and shutting down Windows Azure Lab with Powershell"},{"content":"You know how it is. Question questions questions. As a DBA you are the fount of all knowledge. You are the protector of the data after all so obviously you know every little thing that is needed to be known.\nFrequently, I am asked\nHow many processors does that server have?\nHow much RAM is on that server? What type?\nWhat OS? Which Patches were installed\nor more SQL based questions about configuration\nWhich SQL Product? Which version? Which Service Pack?\nWhat are the linked servers on that server?\nOr you want to know which login have which roles on the server or the autogrowth settings or any number of other ‚Äòlittle things‚Äô\nAs the DBA as they are asking about my servers I should know and whilst I have a lot of info in my head, there‚Äôs not enough room for it all!! So I have to break from what I am doing and dive into Powershell or SSMS and get them the info that they need. When this happens I often thought I wish I could have this information to hand but I have never had time to organise it myself.\nWorse still, imagine your boss walks through the door and says we have to provide information for an audit. Can you give me full details of all the SQL Servers and their configurations both windows and SQL and I need it by the end of play tomorrow.\nIt happens.\nIts Personal Development Review time. I should have asked my boss to give me an objective of thoroughly documenting the SQL Server estate this year. I could have done it in a few hours. You only think of these things when its too late.\nHow can I do this? I hear you cry. Head over to https://sqlpowerdoc.codeplex.com and you will see.\nSQL PowerDoc was written by Kendal VanDyke who is a practiced IT professional with over a decade of experience in SQL Server development and administration. Kendal is currently a principal consultant with UpSearch SQL, where he helps companies keep their SQL Servers running in high gear. Kendal is also a Microsoft MVP for SQL Server and president of the PASS chapter MagicPASS in Orlando, FL. You can find his blog at http://www.kendalvandyke.com/ and on Twitter at @SQLDBA\nI found out about Power Doc a few weeks ago. It looked so cool, I tested it at home and then on my dev server and then on the whole estate. It is CPU heavy on the box it is running on if you have a load of servers. I don‚Äôt know how long it took to run as it ran overnight but the information you get back is staggering, enough to satisfy even the most inquisitive of auditors or questioners. You can pass it to your server team too and they will love it as it can do a Windows based inventory.\nWhat does it document? What DOESNT it document? If you head over to https://sqlpowerdoc.codeplex.com/wikipage?title=What%27s%20Documented the list you see doesn‚Äôt reflect the sheer amount of data you can get back. Run it against your own machine and you will see what I mean.\nAs well as documenting everything it also runs around 100 checks to diagnose potential issues and problems. You can see where autogrowth is set to percentage, databases that haven‚Äôt been backed up, auto shrink, Max Memory set too high, the list goes on. Even the links to the MSDN articles are in there for the things it finds.\nThe documentation is thorough and even if you haven‚Äôt use Powershell before everything you need is on the website to run PowerDoc.\nSo much thought and effort has been put into this it‚Äôs difficult to see how it could be improved.\nWhat I have done then is added the Excel file to our dba SharePoint team site and enabled the right people access to it. Equally they tell others and I get bothered slightly less.\n","date":"2013-05-09T00:00:00Z","permalink":"https://blog.robsewell.com/blog/documenting-sql-server-the-easy-way-with-power-doc/","title":"Documenting SQL Server the easy way with Power Doc"},{"content":" The quality of the speakers and sessions is exceptional http://sqlbits.com/information/Agenda.aspx\nThe Helpers are awesome Often Sessions fill up very quickly ‚Äì Get there early The GuideBook app is pretty good but with a map it would be better http://guidebook.com/\nYou can learn as much outside of the sessions as you can in them There are amazing prizes A LEGO R2D2 !!!\nBring your sense of humour The SQL community contains the most gracious and generous, willing to help people You can connect with your user group and get a mini SQL Bits every month Find your User Group Here http://sqlsouthwest.co.uk/national_ug.htm\nIf there is no user group in your area people will help you to start one RT @fatherjack Interesting chat about a potential new user group in the uk. Anyone around Newcastle area looking for some free training?\nEvery session is videoed and will be available online. For free. You will learn and have fun Too awesome for words! ‚Äú@justjonlevett: Lego Server! @fusionio #sqlbitspic.twitter.com/bhxPaTIq4K‚Äù\nMore blogs about SQL Bits XI and Photos SQL Bits Facebook https://www.facebook.com/SQLBits\nRoger Van Unen Gallery https://plus.google.com/photos/109984741094039234638/albums/5874913179986208577\nJR‚Äôs Gallery https://skydrive.live.com/?cid=7b73b60f4c7d77c9\u0026amp;id=7B73B60F4C7D77C9%212222\nSteve Jones Blog http://voiceofthedba.wordpress.com/2013/05/06/fun-at-sql-bits/\nChris Webbs Blog http://cwebbbi.wordpress.com/2013/05/05/sqlbits-xi-summary/\nFind more from the Facebook Page or #sqlbits\nFinally a BIG Thank you to all these people http://sqlbits.com/about/WhosWho.aspx\nand the fantastic helpers without whom SQL Bits would never happen\nTill Next year\n","date":"2013-05-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/12-things-i-learnt-at-sqlbits-xi/","title":"12 Things I learnt at SQLBits XI"},{"content":"Or, How SQLBits put me in touch with Laerte and solved a problem I have a scheduled Powershell job which I use to create an Excel file colour coded for backup checks. (I will blog about it another time) It works brilliantly on my desktop and saves the file to a UNC path and emails the team the location. It works brilliantly when run in Powershell on the server. When I schedule it to run though it doesn‚Äôt do so well. The job completes without errors but no file is saved.\nIf you examine the processes running at the time you can see the excel process is running¬†so I knew it was doing something but couldn‚Äôt work out why it was failing.\nIt was one of those jobs that gets put to the bottom of the list because the service worked ok I just needed to have it running on the server rather than a desktop for resilience, recovery and security purposes. Every now and then I would try and work out what was going on but new work and new problems would always arrive and it has been like that for 6 or maybe even 9 months.\nAs you know I attended SQLBits this weekend and I went into a session with Laerte Junior. Laerte is a SQL Server MVP and can be found at simple-talk as well as his own blog http://shellyourexperience.com/ or on twitter @LaerteSQLDBA Oh and He loves Star Wars üôÇ\nAfter a fascinating session I asked him if I could show him my problem. He very graciously said yes and after looking at the code and listening to me explain the problem he suggested this very simple solution which he said had taken him a great deal of searching to find. It‚Äôs a bug with COM objects and requires the creation of folders as shown below. I cam into work today, tried it and it worked. HOORAY another thing off my list and big thanks to Laerte\n#Region Bug_Jobs_ComObjects #(32Bit, always)\r# Create Folder #\rNew-Item ‚Äìname C:\\Windows\\System32\\config\\systemprofile\\Desktop¬†‚Äìitemtype directory\r# #(64Bit)\r# Create folder #\rNew-Item ‚Äìname C:\\Windows\\SysWOW64\\config\\systemprofile\\Desktop¬†‚Äìitemtype directory\r#EndRegion Bug_Jobs_ComObjects\rThis worked for me however I had already implemented another fix for a possible gotcha so I will tell you of that one too\nSometimes Powershell cannot save to UNC paths because of¬†IE enhanced security.\nEither log in as user and add server to intranet site zones or disable the warning in registry as follows\n[HKEY_CURRENT_USER\\Software\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\ZoneMap] \u0026quot;UNCAsIntranet\u0026quot;=dword:00000000\rPlease don‚Äôt ever trust anything you read on the internet and certainly don‚Äôt implement it on production servers without first both understanding what it will do and testing it thoroughly. This solution worked for me in my environment I hope it is of use to you in yours but I know nothing about your environment and you know little about mine\n","date":"2013-05-07T00:00:00Z","permalink":"https://blog.robsewell.com/blog/powershell-wont-save-when-running-as-a-scheduled-job/","title":"Powershell won‚Äôt save when running as a scheduled job"},{"content":"Changing Domain Names in a Column A quick little post for today. Not particularly SQL related but the points at the end are relevant.\nI had a task when moving a service to a new development area to change the domain name within columns in several tables from ‚ÄúDOMAIN1\\USER‚Äù to ‚ÄúDOMAIN2\\USER‚Äù\nIn SQL I was able to do this quite easily as follows\nUSE [DATABASENAME] GO\r-- Declare variables DECLARE @Live nvarchar(10) DECLARE @Dev nvarchar(10) -- Set the variable to the Domains Set @Live = 'Live Domain' Set @Dev = 'Dev Domain' --Update tables UPDATE [TABLENAME] SET [User] = REPLACE([User], @Live, @Dev) GO UPDATE [TABLENAME] SET [Group] = REPLACE([Group], @Live, @Dev) GO\rI also had to do the same for some Oracle databases too and this is where the fun started!\nI needed to create the update scripts for documentation for the Oracle databases.\nI wanted to create\nupdate schema.tablename set userid = replace ('DOMAIN1\\USER', 'DOMAIN1', 'DOMAIN2') WHERE USERID = 'DOMAIN1\\USER';\rfor each userid in the table.I had trouble with the script I found in our DBA area as it kept failing with\nORA-00911: invalid character\nat the \\\nas it wouldn‚Äôt add the ‚Äò ‚Äò around DOMAIN1\\USER\nNot being an Oracle DBA but wanting to solve the issue once and for all I tried a whole host of solutions trying to find the escape character. i asked the Oracle DBAs but they were unable to help Checking the IT Pros handbook (also known as Google!) made me more confused but in the end I solved it.\nselect 'update schema.table set userid = replace (''' || userid || ''', ''DOMAIN1'', ''DOMAIN2'') WHERE USERID = ''' || USERID || ''';' FROM schema.tablename;\rA whole host of ‚Äòs in there!!\nI put this in my blog as it is relevant to my situation and an experience I have had that I couldn‚Äôt easily solve. Maybe it will help another person searching for the same thing.\nIt raises some interesting points\nThe script provided ( I use that term loosely, it had the right name and was in the right place to use for this process) had obviously not been run as it didn‚Äôt work or someone had manually added the ‚Äòs. I wasn‚Äôt go to do that for the number of users required.\nIf it no good, if it doesn‚Äôt do what i expected or is still in development then mark it as so, so that everyone knows. In the name of the script, in the comments in the script or by keeping live tested scripts in one place. Which ever method you choose is fine as long as it is appropriate to your environment and everyone knows about it\nI probably say a dozen times a day to my new colleague\n‚ÄúIn case you/I get run over by a bus‚Äù\nIt is all very well being the one who knows everything but it is pointless if you aren‚Äôt there SPOF‚Äôs (Single Points of Failure) apply to people as well as hardware.\nEnable your service to be supported by preparing proper documentation.\nThis doesn‚Äôt have to be reams of paperwork. It can sometimes be as simple as placing things in a recognised place or a single comment in the script.\nI hold my hands up. I am guilty of this too. I have been so busy I haven‚Äôt done this as much as I should have over the last few months of last year. I have tried but not done as well as I should have. In my defence, I have spent plenty of time recently rectifying this, which is why this situation was so memorable.\nSome links I have read in the past related to this by¬†people who know more than me.\nDocumentation It Doesn‚Äôt Suck ‚Äì Brent Ozar\nYour Lack Of Documentation is Costing you More than you Think ‚Äì John Samson\nDo You Document Your SQL Server Instances? ‚Äì Brad McGhee\n","date":"2013-02-11T00:00:00Z","permalink":"https://blog.robsewell.com/blog/those-pesky-s/","title":"Those Pesky ‚Äòs"},{"content":"![medium_33194896]({{ \u0026ldquo;/assets/uploads/2013/02/medium_33194896_thumb.jpg\u0026rdquo; | relative_url }})\nThe hardest part is looking at the blank page and beginning to type. It‚Äôs much easier to go and play with the settings of the site, to look at plugins and other cool things. The only other blog I have written was http://wombatsdojogle.wordpress.com. This was a little easer as there was always ‚Äòsomething‚Äô that needed to be written about. Whether it was training or route planning or every day on the road I had material and an obvious thing to write.\nThis is a little harder for me so I will begin as follows\nDuring different careers working in secure units, working for a small family firm doing everything from delivery driving to office work and working for myself selling things via eBay and at car boot sales I have always been interested in computers. I was (still am) the fella who could fix and sort things out. Towards the end I was getting paid for it too. I helped small businesses and individuals, I set up systems, reinstalled operating systems, dealt with viruses. You know the sort of thing. When things dried up and circumstances changed meaning I could spend some time away from home I got a job at an arts university. In a small team my responsibilities ranged from password resets and printer installs to rolling out new PCs and laptops and helping to merge active directory domains. I loved it. The travel too and from work was a pain at times but the job was grand though the pay wasn‚Äôt!!\nI joined MyWork in a service desk role. Not your typical log and flog sort of place but a 24/7 team responsible for the first answering of the phone to a significant amount of second line fixing and routine IT tasks invaluable to the running of MyWork. A couple of years later after many suggestions of jobs I should apply for and plenty of encouragement from colleagues I applied for and got the position of Oracle DBA. That didn‚Äôt work out quite as expected and two months later I was asked to move to be a SQL DBA. That was 18 months ago and I am astonished by how much I have learnt so far and still slightly daunted by the sheer amount there still is to learn.\nThe reason I was asked to move is that responsibility for the SQL estate had moved to the team and the one SQL DBA was struggling with the sheer amount of work required. He had joined only a few months earlier and found that best practice and SQL had not been applied particularly well and with more than 700 databases to support he couldn‚Äôt keep up.\nHe and I began to make changes. Permissions for developers were removed ‚Äì no more sysadmins for developers on live systems. Backups were run 7 days a week and checked every day. Service accounts were set up to run the various SQL services per server. Documentation was begun. All the good things that should be done were started to be done.\nThere were arguments and outbursts. Developers took time to understand that we were doing things for the best of MyWork and not to annoy them or stop them working. We got things wrong for sure. We didn‚Äôt communicate well with colleagues in other teams at times but we had the backing of our line management.\nThen my colleague left to go to pastures new. I had been a SQL DBA for exactly 6 months and I was on my own. Then my line manager left so I had to look after the general maintenance of the Oracle estate as well. There are also some Ingres databases critical to MyWork and they were my responsibility as well.\nFor a few months I somehow managed to keep everything going without making any major booboos. It was a struggle. I was fighting my lack of knowledge, the sheer amount of work and running much too hard on caffeine and nicotine. At the end of last year some salvation arrived. First an Oracle DBA joined then a team lead (also an Oracle DBA) and another SQL DBA. Not the many years experienced SQL DBA I had hoped for who could advise me and teach me but I sure am glad he‚Äôs here.\nIn the last few weeks I am beginning to see the benefit of this. No longer on call all the time. Not as much fire fighting. Able to plan my day instead of walking in and dealing with whoever or whatever was shouting loudest. I was finally able to go to the local SQL User Group for the first time last month.\nhttp://sqlsouthwest.co.uk/\nand meet up with some fabulous people.\nI decided to start to write a blog about my experience. I hope it will show me how far I have come, how much I have learnt and the way I have done it. It may be of use to people and hopefully it will increase my interaction with the rest of the SQL community who are without doubt the most interactive and helpful group of people mainly without egos.\nI have an idea of my next post. It will be about resolving the challenge and time spent checking and resolving backups.\nThe idea for it started with reading a blog post by John Samson http://www.johnsansom.com who can be found on twitter @SQLBrit\nThe blog post is one of the most read on his blog and is titled\nThe Best Database Administrators Automate Everything Here is a quote from that blog entry\nAutomate Everything That‚Äôs right, I said everything. Just sit back and take the time to consider this point for a moment. Let it wander around your mind whilst you consider the processes and tasks that you could look to potentially automate. Now eliminate the word potentially from your vocabulary and evaluate how you could automate e-v-e-r-y-t-h-i-n-g that you do.\nEven if you believe that there is only a remote possibility that you will need to repeat a given task, just go ahead and automate it anyway! Chances are that when the need to repeat the process comes around again, you will either be under pressure to get it done, or even better have more important_Proactive Mode_ tasks/projects to be getting on with\nI have tried my best to follow this advice. I haven‚Äôt always succeeded. Many times I just didn‚Äôt have the time to spare to write the automation even though it would save me time later. Now with more assistance in my team I am starting to resolve that\nMy interest in PowerShell, which was piqued when I wanted to organise my photos and a colleague pointed me at a script to sort my photos into year and month, encouraged me to create my favourite automation process which I will describe next time.\nphoto credit: emdot via photopin cc\n","date":"2013-02-10T00:00:00Z","image":"https://blog.robsewell.com/assets/uploads/2013/02/medium_33194896_thumb.jpg","permalink":"https://blog.robsewell.com/blog/you-have-to-start-somewhere/","title":"You Have To Start Somewhere"},{"content":"Abbott: Strange as it may seem, they give ball players nowadays very peculiar names.\nCostello: Funny names?\nAbbott: Nicknames, nicknames. Now, on the St. Louis team we have Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;\nCostello: That\u0026rsquo;s what I want to find out. I want you to tell me the names of the fellows on the St. Louis team.\nAbbott: I\u0026rsquo;m telling you. Who\u0026rsquo;s on first, What\u0026rsquo;s on second, I Don\u0026rsquo;t Know is on third\u0026ndash;\nCostello: You know the fellows\u0026rsquo; names?\nAbbott: Yes.\nCostello: Well, then who\u0026rsquo;s playing first?\nAbbott: Yes.\nCostello: I mean the fellow\u0026rsquo;s name on first base.\nAbbott: Who.\nCostello: The fellow playin\u0026rsquo; first base.\nAbbott: Who.\nCostello: The guy on first base.\nAbbott: Who is on first.\nCostello: Well, what are you askin\u0026rsquo; me for?\nAbbott: I\u0026rsquo;m not asking you\u0026ndash;I\u0026rsquo;m telling you. Who is on first.\nCostello: I\u0026rsquo;m asking you\u0026ndash;who\u0026rsquo;s on first?\nAbbott: That\u0026rsquo;s the man\u0026rsquo;s name.\nCostello: That\u0026rsquo;s who\u0026rsquo;s name?\nAbbott: Yes.\nCostello: When you pay off the first baseman every month, who gets the money?\nAbbott: Every dollar of it. And why not, the man\u0026rsquo;s entitled to it.\nCostello: Who is?\nAbbott: Yes.\nCostello: So who gets it?\nAbbott: Why shouldn\u0026rsquo;t he? Sometimes his wife comes down and collects it.\nCostello: Who\u0026rsquo;s wife?\nAbbott: Yes. After all, the man earns it.\nCostello: Who does?\nAbbott: Absolutely.\nCostello: Well, all I\u0026rsquo;m trying to find out is what\u0026rsquo;s the guy\u0026rsquo;s name on first base?\nAbbott: Oh, no, no. What is on second base.\nCostello: I\u0026rsquo;m not asking you who\u0026rsquo;s on second.\nAbbott: Who\u0026rsquo;s on first!\nCostello: St. Louis has a good outfield?\nAbbott: Oh, absolutely.\nCostello: The left fielder\u0026rsquo;s name?\nAbbott: Why.\nCostello: I don\u0026rsquo;t know, I just thought I\u0026rsquo;d ask.\nAbbott: Well, I just thought I\u0026rsquo;d tell you.\nCostello: Then tell me who\u0026rsquo;s playing left field?\nAbbott: Who\u0026rsquo;s playing first.\nCostello: Stay out of the infield! The left fielder\u0026rsquo;s name?\nAbbott: Why.\nCostello: Because.\nAbbott: Oh, he\u0026rsquo;s center field.\nCostello: Wait a minute. You got a pitcher on this team?\nAbbott: Wouldn\u0026rsquo;t this be a fine team without a pitcher?\nCostello: Tell me the pitcher\u0026rsquo;s name.\nAbbott: Tomorrow.\nCostello: Now, when the guy at bat bunts the ball\u0026ndash;me being a good catcher\u0026ndash;I want to throw the guy out at first base, so I pick up the ball and throw it to who?\nAbbott: Now, that\u0026rsquo;s he first thing you\u0026rsquo;ve said right.\nCostello: I DON\u0026rsquo;T EVEN KNOW WHAT I\u0026rsquo;M TALKING ABOUT!\nAbbott: Don\u0026rsquo;t get excited. Take it easy.\nCostello: I throw the ball to first base, whoever it is grabs the ball, so the guy runs to second. Who picks up the ball and throws it to what. What throws it to I don\u0026rsquo;t know. I don\u0026rsquo;t know throws it back to tomorrow\u0026ndash;a triple play.\nAbbott: Yeah, it could be.\nCostello: Another guy gets up and it\u0026rsquo;s a long ball to center.\nAbbott: Because.\nCostello: Why? I don\u0026rsquo;t know. And I don\u0026rsquo;t care.\nAbbott: What was that?\nCostello: I said, I DON\u0026rsquo;T CARE!\nAbbott: Oh, that\u0026rsquo;s our shortstop!\n","date":"2010-01-08T00:00:00Z","permalink":"https://blog.robsewell.com/blog/post-chat/","title":"Post: Chat"}]